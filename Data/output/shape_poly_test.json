[
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_constraints_eq_threefry(self):\n\n    def f(x):\n        a, = x.shape\n        x_padded = jnp.concatenate([x, jnp.zeros((a % 2,), dtype=x.dtype)])\n        x_reshaped = x_padded.reshape((2, -1))\n        x_reshaped += 2\n        x_padded_1 = x_reshaped.reshape((-1,))\n        x_1 = x_padded_1[:a]\n        return x + x_1\n    check_shape_poly(self, f, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['a'], symbolic_constraints=['mod(a + mod(a, 2), -2) == 0', '-2*floordiv(a + mod(a, 2), -2) == a + mod(a, 2)'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(with_function=v) for v in [True, False]])\ndef test_grad_int(self, with_function=False):\n    x_shape = (2, 3, 4)\n    xi = np.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape)\n    yf = xi.astype(np.float32)\n    xi_yf = (xi, yf)\n    zb = np.array([True, False], dtype=np.bool_)\n\n    def f_jax(xi_yf, zb):\n        xi, yf = xi_yf\n        return (jnp.zeros(xi.shape, dtype=jnp.float32), (xi, zb, xi.astype(np.float32) * 2.0 * yf))\n    args = (xi_yf, zb)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=[('b1, b2, 4', 'b1, b2, 4'), 'b1'])\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    res_tf, g_tf = tf_test_util.ComputeTfValueAndGrad(f_tf, args)\n    self.assertAllClose(g_tf[0][0], np.zeros_like(xi))\n    self.assertAllClose(g_tf[0][1], (xi * 2).astype(yf.dtype))\n    self.assertAllClose(g_tf[1], np.zeros_like(zb))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f2():\n    x = 2\n    g2()\n    return x"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f2():\n    x = 2\n    g2()\n    return x"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    exp = export.export(jax.jit(f_jax))(jax.ShapeDtypeStruct(export.symbolic_shape('b'), x.dtype), y=jax.ShapeDtypeStruct(y.shape, y.dtype))\n    self.assertAllClose(f_jax(x, y=y), exp.call(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_constraints_ge_compile_time_check(self):\n\n    def f(x):\n        a = x.shape[0]\n        assert _bounds(a) == (2, 4)\n        return lax.dynamic_slice_in_dim(x, 1, 2, 0)\n    x_spec = jax.ShapeDtypeStruct(export.symbolic_shape('a', constraints=['a >= 2', 'a <= 4']), np.int32)\n    exp = export.export(jax.jit(f))(x_spec)\n    x_2 = np.arange(2, dtype=np.int32)\n    res_2 = exp.call(x_2)\n    self.assertAllClose(x_2[0:2], res_2)\n    x_4 = np.arange(4, dtype=np.int32)\n    res_4 = exp.call(x_4)\n    self.assertAllClose(x_4[1:3], res_4)\n    with self.assertRaisesRegex(ValueError, re.escape(\"Expected 'a - 2' to be greater or equal to 0, but found -1\")):\n        exp.call(np.arange(1, dtype=np.int32))\n    with self.assertRaisesRegex(ValueError, re.escape(\"Expected '- a + 4' to be greater or equal to 0, but found -1\")):\n        exp.call(np.arange(5, dtype=np.int32))",
    "assertions": [
      "assert _bounds(a) == (2, 4)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_constraints_eq_0_compile_time_check(self):\n\n    def f(x):\n        return x\n    x_spec = jax.ShapeDtypeStruct(export.symbolic_shape('a, b', constraints=['max(a, b) == b']), np.int32)\n    exp = export.export(jax.jit(f))(x_spec)\n    with self.assertRaisesRegex(ValueError, re.escape(\"Expected 'max(a, b) - b' to be equal to 0, but found 1\")):\n        exp.call(np.ones((3, 2), dtype=np.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_constraints_eq_1_compile_time_check(self):\n\n    def f(x):\n        return x\n    x_spec = jax.ShapeDtypeStruct(export.symbolic_shape('a, b', constraints=['a == b']), np.int32)\n    exp = export.export(jax.jit(f))(x_spec)\n    exp.call(np.ones((3, 3), dtype=np.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_constraints_eq_2_compile_time_check(self):\n\n    def f(x):\n        return x\n    x_spec = jax.ShapeDtypeStruct(export.symbolic_shape('a, b', constraints=['max(a, b) == 4', 'a == b']), np.int32)\n    exp = export.export(jax.jit(f))(x_spec)\n    with self.assertRaisesRegex(ValueError, re.escape(\"Expected 'max(a, b) - 4' to be equal to 0, but found -1\")):\n        exp.call(np.ones((3, 3), dtype=np.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(slc=slc) for slc in [slice(None, None, None), slice(2, 5)]])\ndef test_stateful(self, slc: slice):\n    w, = export.symbolic_shape('w', constraints=['w >= 3'])\n\n    def f(x_ref):\n        ones = jnp.ones_like(x_ref)[slc]\n        ref_primitives.ref_addupdate(x_ref, slc, ones)\n        x1 = ref_primitives.ref_get(x_ref, slc)\n        x2 = x1 + ones\n        ref_primitives.ref_set(x_ref, slc, x2)\n    exp = export.export(jax.jit(discharge.run_state(f)))(jax.ShapeDtypeStruct((w,), dtype=_f32))\n    x = np.ones((32,), dtype=_f32)\n    expected = np.copy(x)\n    expected[slc] = 3.0\n    self.assertAllClose(exp.call(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def rand(key):\n    nums = jax.vmap(lambda key: random.uniform(key, (1000,), dtype))(key)\n    return nums.flatten()"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        jax2tf.convert(add_all_jax, polymorphic_shapes=polymorphic_shapes)(*args)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_gradients_pytree(self):\n    \"\"\"Shape polymorphism with gradients and pytrees for inputs and outputs.\"\"\"\n\n    def f(x):\n        return dict(res=x['x'] * 2.0)\n    check_shape_poly(self, f, skip_jax_run=True, input_signature=[dict(x=tf.TensorSpec([None, 3, 4]))], polymorphic_shapes=[dict(x='b, 3, 4')])\n    f_tf = jax2tf.convert(f, polymorphic_shapes=[dict(x='b, 3, 4')])\n    x = dict(x=np.ones((2, 3, 4), dtype=np.float32))\n    xv = tf.Variable(x['x'], dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(dict(x=xv))\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, dict(grad=res_tf_grad))\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[0]['res']))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[1]['grad']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_grad_not_var_output(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    x = np.arange(12, dtype=np.float32).reshape((4, 3))\n    xv = tf.Variable(x)\n    f_tf = jax2tf.convert(f_jax, with_gradient=True, polymorphic_shapes=['b, ...'])\n    with tf.GradientTape() as tape:\n        res_tf = f_tf(xv)\n    grad_tf = tape.gradient(res_tf, xv)\n    self.assertAllClose(np.ones(x.shape, dtype=np.float32), grad_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(with_function=v) for v in [True, False]])\ndef test_grad_int(self, with_function=False):\n    x_shape = (2, 3, 4)\n    xi = np.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape)\n    yf = xi.astype(np.float32)\n    xi_yf = (xi, yf)\n    zb = np.array([True, False], dtype=np.bool_)\n\n    def f_jax(xi_yf, zb):\n        xi, yf = xi_yf\n        return (jnp.zeros(xi.shape, dtype=jnp.float32), (xi, zb, xi.astype(np.float32) * 2.0 * yf))\n    args = (xi_yf, zb)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=[('b1, b2, 4', 'b1, b2, 4'), 'b1'])\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    res_tf, g_tf = tf_test_util.ComputeTfValueAndGrad(f_tf, args)\n    self.assertAllClose(g_tf[0][0], np.zeros_like(xi))\n    self.assertAllClose(g_tf[0][1], (xi * 2).astype(yf.dtype))\n    self.assertAllClose(g_tf[1], np.zeros_like(zb))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_with_hash_collision_vmap(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (2, -1))\n    try:\n        orig_hash = getattr(shape_poly._DimExpr, '__hash__')\n\n        def collision_hash(obj):\n            return hash(5)\n        setattr(shape_poly._DimExpr, '__hash__', collision_hash)\n        xs = np.ones((3, 5, 6), dtype=np.float32)\n        f_toconvert = jax.vmap(pjit.pjit(f_jax))\n        res_1 = jax2tf.convert(f_toconvert)(xs)\n        res_2 = jax2tf.convert(f_toconvert, polymorphic_shapes='b1, b2, ...')(xs)\n        self.assertAllClose(res_1, res_2)\n    finally:\n        setattr(shape_poly._DimExpr, '__hash__', orig_hash)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_vmap_error(self):\n    x = y = np.ones((3, 5), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y: x + y), polymorphic_shapes=['b, ...', None])(x, y)\n    z = x\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y, z: x + y + z), polymorphic_shapes=['b, ...', 'c, ...', None])(x, y, z)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_dim_vars_definitely_equal(self):\n    a, b = shape_poly.symbolic_shape('a, b')\n    self.assertTrue(core.definitely_equal(a, a))\n    self.assertFalse(core.definitely_equal(a, 1))\n    self.assertFalse(core.definitely_equal(a, b))\n    self.assertTrue(core.definitely_equal_one_of_dim(a, [2, a]))\n    self.assertFalse(core.definitely_equal_one_of_dim(a, [2, b]))\n    self.assertFalse(core.definitely_equal_one_of_dim(a, []))\n    self.assertTrue(core.definitely_equal_one_of_dim(2, [a, 3, 2]))\n    self.assertFalse(core.definitely_equal_one_of_dim(1, [2, b]))\n    self.assertFalse(core.definitely_equal_one_of_dim(3, []))\n    self.assertTrue(core.definitely_equal(1, jnp.add(0, 1)))\n    self.assertFalse(core.definitely_equal(1, 'a'))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f2(o, x):\n    self.assertIsInstance(o, dict)\n    self.assertIs(o['a'], obj['a'])\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f2(o, x):\n    self.assertIsInstance(o, dict)\n    self.assertIs(o['a'], obj['a'])\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_min_max_type_check(self):\n    a, = shape_poly.symbolic_shape('a')\n    for i, f in enumerate([lambda x: core.max_dim(x, a), lambda x: core.max_dim(a, x), lambda x: core.min_dim(x, a), lambda x: core.min_dim(a, x)]):\n        with self.subTest(f'jit_{i}'):\n            with self.assertRaisesRegex(core.ConcretizationTypeError, ''):\n                jax.jit(f)(1)\n    arr = jnp.array([1], dtype=np.int32)\n    for i, f in enumerate([lambda: core.max_dim(arr, a), lambda: core.max_dim(a, arr), lambda: core.min_dim(arr, a), lambda: core.min_dim(a, arr)]):\n        with self.subTest(f'array_{i}'):\n            with self.assertRaisesRegex(TypeError, 'Only integer scalar arrays can be converted to a scalar index'):\n                f()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    for polymorphic_shapes in [None, '_, h', 'h, h']:\n        with self.subTest(shapes=polymorphic_shapes):\n            check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=polymorphic_shapes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'expr={name}', expr=expr) for name, expr in [('d + 2', lambda d: d + 2), ('2 - d', lambda d: 2 - d), ('d * 2', lambda d: d * 2), ('d * d', lambda d: d * d), ('(- d) * d', lambda d: -d * d), ('d * d - d', lambda d: d * d - d), ('d // 2', lambda d: d // 2), ('(d + 1) // 2', lambda d: (d + 1) // 2), ('d // -2', lambda d: d // -2), ('(d + 1) // -2', lambda d: (d + 1) // -2), ('(-d) // 2', lambda d: -d // 2), ('(-d - 1) // 2', lambda d: (-d - 1) // 2), ('(-d) // -2', lambda d: -d // -2), ('(-d - 1) // -2', lambda d: (-d - 1) // -2), ('d % 2', lambda d: d % 2), ('(d + 1) % 2', lambda d: (d + 1) % 2), ('d % -2', lambda d: d % -2), ('(d + 1) % -2', lambda d: (d + 1) % -2), ('(-d) % 2', lambda d: -d % 2), ('(-d - 1) % 2', lambda d: (-d - 1) % 2), ('(-d) % -2', lambda d: -d % -2), ('(-d - 1) % -2', lambda d: (-d - 1) % -2)]])\ndef test_non_trivial_dim_expr(self, expr=lambda d: d % -2):\n    check_shape_poly(self, lambda x: x[0] * 0 + expr(x.shape[0]), arg_descriptors=[RandArg((3,), np.int64)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name='cube', f=lambda x: x.shape[0] ** 3), dict(testcase_name='zero', f=lambda x: x.shape[0] ** 0), dict(testcase_name='rpow', f=lambda x: 2 ** x.shape[0]), dict(testcase_name='negative', f=lambda x: x.shape[0] ** (-2), expect_error=(ValueError, 'cannot be raised to negative powers')), dict(testcase_name='non_integer', f=lambda x: x.shape[0] ** 1.5, expect_error=(ValueError, 'cannot be raised to non-integer powers')), dict(testcase_name='sym_pow', f=lambda x: x.shape[0] ** x.shape[0])])\ndef test_pow(self, f, expect_error: tuple[Exception, str] | None=None):\n    check_shape_poly(self, f, arg_descriptors=[RandArg((3,), np.float32)], polymorphic_shapes=['b'], expect_error=expect_error)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None])\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_arg_avals_errors(self):\n    \"\"\"Test error reporting for shape polymorphism.\"\"\"\n\n    def conv_and_run(*, arg_shape: core.Shape, polymorphic_shape: str):\n        arg = np.arange(math.prod(arg_shape), dtype=np.float32).reshape(arg_shape)\n        check_shape_poly(self, lambda x: x, arg_descriptors=[arg], polymorphic_shapes=[polymorphic_shape])\n    with self.assertRaisesRegex(ValueError, re.escape('polymorphic shape spec should be')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=5.0)\n    with self.assertRaisesRegex(ValueError, re.escape('pytree structure error: different types')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=['a list'])\n    with self.assertRaisesRegex(ValueError, re.escape('pytree structure error: different types')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=('a tuple',))\n    with self.assertRaisesRegex(ValueError, \"Cannot solve for values of dimension variables {'b'}\"):\n        conv_and_run(arg_shape=(4, 36, 3), polymorphic_shape='b * b, b * d * d, d')\n    with self.assertRaisesRegex(ValueError, \"Division had remainder 2 when computing the value of 'b'\"):\n        conv_and_run(arg_shape=(5, 36), polymorphic_shape='3 * b, ...')\n    with self.assertRaisesRegex(ValueError, \"Expected value >= 1 for dimension variable 'b'\"):\n        conv_and_run(arg_shape=(10, 3), polymorphic_shape='3 * b + 10, ...')\n    with self.assertRaisesRegex(ValueError, \"Expected value >= 1 for dimension variable 'b'\"):\n        conv_and_run(arg_shape=(7, 3), polymorphic_shape='3 * b + 10, ...')\n    with self.assertRaisesRegex(ValueError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 3) and the specification 'a' (= 2)\")):\n        conv_and_run(arg_shape=(2, 3), polymorphic_shape='(a, a)')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_pytree(self):\n    \"\"\"Arguments and polymorphic_shapes are pytrees.\"\"\"\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n    check_shape_poly(self, add_all_jax, arg_descriptors=args, polymorphic_shapes=[(['v', 'v'], ['v']), dict(a='v', b='v')])\n    check_shape_poly(self, add_all_jax, arg_descriptors=args, polymorphic_shapes='v')\n    check_shape_poly(self, add_all_jax, arg_descriptors=args, polymorphic_shapes=['v', 'v'])\n    check_shape_poly(self, add_all_jax, arg_descriptors=args, polymorphic_shapes=[('v', 'v'), 'v'])\n    check_shape_poly(self, add_all_jax, arg_descriptors=args, polymorphic_shapes=[(['(4,)', '(_,)'], ['4,']), dict(a='(_,)', b='(4,)')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        check_shape_poly(self, add_all_jax, arg_descriptors=args, polymorphic_shapes=polymorphic_shapes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[0] * x.shape[1], dtype=x.dtype).reshape(x.shape)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=str(polymorphic_shapes), polymorphic_shapes=polymorphic_shapes) for polymorphic_shapes in ['b1+6,b1+14,b2', '2*b1,4*b2,b1+b2+18', 'b1+2*b2,4*b2,b1*b1+16']])\ndef test_non_trivial_polynomials_spec(self, polymorphic_shapes='2*b1,4*b2,b1+b2+18'):\n    check_shape_poly(self, lambda x: 2 * x.shape[0] + 3 * x.shape[1] + 4 * x.shape[2], arg_descriptors=[RandArg((16, 24, 32), _f32)], polymorphic_shapes=[polymorphic_shapes])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_constraints_basic(self):\n\n    def f(x):\n        assert x.shape[0] >= 6\n    check_shape_poly(self, f, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['a'], symbolic_constraints=['a >= 8'])",
    "assertions": [
      "assert x.shape[0] >= 6"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_constraints_slice_in_dim(self):\n\n    def f(x):\n        return lax.dynamic_slice_in_dim(x, 0, 8, 0)\n    check_shape_poly(self, f, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['a'], symbolic_constraints=['a >= 8'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_constraints_slice_in_dim_eq(self):\n\n    def f(x, y):\n        v1 = x[:y.shape[0]]\n        v2 = y[2:]\n        return v1 + v2\n    check_shape_poly(self, f, arg_descriptors=[RandArg((16,), _i32), RandArg((18,), _i32)], polymorphic_shapes=['a', 'b'], symbolic_constraints=['b >= 2', 'min(a, b) == b - 2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_constraints_eq_threefry(self):\n\n    def f(x):\n        a, = x.shape\n        x_padded = jnp.concatenate([x, jnp.zeros((a % 2,), dtype=x.dtype)])\n        x_reshaped = x_padded.reshape((2, -1))\n        x_reshaped += 2\n        x_padded_1 = x_reshaped.reshape((-1,))\n        x_1 = x_padded_1[:a]\n        return x + x_1\n    check_shape_poly(self, f, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['a'], symbolic_constraints=['mod(a + mod(a, 2), -2) == 0', '-2*floordiv(a + mod(a, 2), -2) == a + mod(a, 2)'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_constraints_eq_mod_0(self):\n\n    def f(x):\n        b = x.shape[0]\n        y1 = jnp.ones((1, 3, 4, b // 4), dtype=x.dtype)\n        y2 = y1.reshape((1, 3, -1))\n        y3 = x.reshape((1, 1, b)) + y2\n        slice0 = lax.slice(x, (0,), (b // 4,))\n        slice1 = lax.slice(x, (0,), (2 * (b // 4),))\n        slice2 = lax.slice(x, (0,), (3 * (b // 4),))\n        slice3 = lax.slice(x, (0,), (4 * (b // 4),))\n        return jnp.sum(y3) + jnp.sum(slice0) + jnp.sum(slice1) + jnp.sum(slice2) + jnp.sum(slice3)\n    check_shape_poly(self, f, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['b'], symbolic_constraints=['mod(b, 4) == 0'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_unused_args(self):\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((2, 3), _f32), RandArg((3,), _f32)], polymorphic_shapes=[None, 'b'])\n    check_shape_poly(self, lambda x_unused, y, z_unused, w: jnp.concatenate([y, w]), arg_descriptors=[RandArg((3,), _f32), RandArg((4,), _f32), RandArg((5,), _f32), RandArg((6,), _f32)], polymorphic_shapes=[None, 'b1', None, 'b2'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((3,), _f32), RandArg((3,), _f32)], polymorphic_shapes=['b', 'b'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((4,), _f32), RandArg((3,), _f32)], polymorphic_shapes=['b1', 'b2'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((3,), _f32), RandArg((9,), _f32)], polymorphic_shapes=['b1', 'b1 * b1'])\n    check_shape_poly(self, lambda x_unused, y: y + x_unused.shape[0], arg_descriptors=[RandArg((3,), _f32), RandArg((9,), _f32)], polymorphic_shapes=['b1', 'b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + jnp.reshape(y, (1, y.shape[0])), lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jax.debug_key_reuse(False)\ndef test_prng(self):\n    with config.enable_custom_prng(True):\n\n        def f_jax(x):\n            key = random.PRNGKey(123)\n            broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n            gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n            slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n            slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n            upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n            _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n            xs = broadcast_keys\n            counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n            def f_vmap_jax(counts, xs):\n\n                def inner(count, x):\n                    return lax.fori_loop(0, count, lambda _, acc: acc, x)\n                return jax.vmap(inner)(counts, xs)\n            _ = f_vmap_jax(counts, xs)\n            return x\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b1, b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_with_hash_collision_vmap(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (2, -1))\n    orig_hash = None\n    try:\n        orig_hash = getattr(shape_poly._DimExpr, '__hash__')\n\n        def collision_hash(obj):\n            return hash(5)\n        setattr(shape_poly._DimExpr, '__hash__', collision_hash)\n        xs = [np.ones((3, 5, 6), dtype=np.float32)]\n        f_toconvert = jax.vmap(pjit.pjit(f_jax))\n        res_1 = check_shape_poly(self, f_toconvert, arg_descriptors=xs, polymorphic_shapes=['...'])\n        res_2 = check_shape_poly(self, f_toconvert, arg_descriptors=xs, polymorphic_shapes=['b1, b2, ...'])\n        self.assertAllClose(res_1, res_2)\n    finally:\n        setattr(shape_poly._DimExpr, '__hash__', orig_hash)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=op_name, op=op) for op, op_name in [(jnp.array, 'array'), (jnp.sin, 'sin'), (lambda x: x, 'id'), (core.dimension_as_value, 'dimension_as_value')]])\ndef test_poly_unary_op(self, *, op=jnp.array):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        return (op(poly), x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), _f32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_{op.__name__}_other={other}:{type(other)}{('_other_jnp_array' if other_jnp_array else '')}{('_swap' if swap else '')}', op=op, other=other, other_jnp_array=other_jnp_array, swap=swap) for op in [op.add, op.mul, op.sub, op.mod, op.floordiv, op.truediv] for other in [2, np.int32(2), 2.0, np.float32(2), np.array(2, dtype=np.int32), np.arange(1, 5, dtype=np.int32), np.array(2.0, dtype=np.float32), np.arange(1.0, 7.0, dtype=np.float32)] for other_jnp_array in ([True, False] if np.shape(other) == (7,) else [False]) for swap in [False, True]])\ndef test_poly_binary_op(self, *, op=op.add, other=np.arange(2, dtype=np.int32), other_jnp_array=False, swap=True):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        other_wrapped = jnp.array(other) if other_jnp_array else other\n        ops = (poly, other_wrapped) if not swap else (other_wrapped, poly)\n        res = op(*ops)\n        try:\n            op.index(other)\n            other_isint = True\n        except Exception:\n            other_isint = False\n        if hasattr(poly, 'dimension_as_value') and other_isint and (op.__name__ != 'truediv'):\n            self.assertTrue(isinstance(res, int) or hasattr(res, 'dimension_as_value'))\n        if config.enable_x64.value:\n            return (lax.convert_element_type(res, np.float32), x)\n        return (res, x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), np.int32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_shape_as_array(self):\n\n    def f_jax(x):\n        return x + jnp.sum(jnp.array(x.shape)).astype(np.int32)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _i32)], polymorphic_shapes=['b, _'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_dim_as_value_weak_type(self):\n\n    def f_jax(x):\n        d0 = jnp.array(x.shape[0])\n        if isinstance(d0, core.Tracer):\n            self.assertTrue(d0.aval.weak_type)\n        d1 = x.shape[0] + jnp.array(4)\n        if isinstance(d1, core.Tracer):\n            self.assertTrue(d1.aval.weak_type)\n        return d0 + np.array(5.0, dtype=np.float32) + d1 + x[0]\n    with config.numpy_dtype_promotion('strict'):\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), _f32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_vmap_while(self):\n\n    def cond_func(x):\n        return jnp.sum(x) >= 0.0\n\n    def body_func(x):\n        return x - 1.0\n\n    def f_jax(x):\n        return lax.while_loop(cond_func, body_func, x)\n    check_shape_poly(self, jax.vmap(f_jax), arg_descriptors=[RandArg((5, 3), _f32)], polymorphic_shapes=['b, ...'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_vmap_error(self):\n    x = y = np.ones((3, 5), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        check_shape_poly(self, jax.vmap(lambda x, y: x + y), arg_descriptors=[x, y], polymorphic_shapes=['b, ...', None])\n    z = x\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        check_shape_poly(self, jax.vmap(lambda x, y, z: x + y + z), arg_descriptors=[x, y, z], polymorphic_shapes=['b, ...', 'c, ...', None])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['_, h'], expected_output_signature=tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_simple_binary(self):\n    \"\"\"Test shape polymorphism for a simple case, binary function.\"\"\"\n\n    def f_jax(x, y):\n        return x + jnp.sin(y)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=[None, None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=['_, h', '_, h'], input_signature=[tf.TensorSpec([2, None]), tf.TensorSpec([2, 3])], expected_output_signature=tf.TensorSpec([2, 3]) if not config.jax2tf_default_native_serialization.value else tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32), RandArg((3, 3), _f32)], polymorphic_shapes=['h, h', 'h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, make_args=make_args, expect_error=expect_error, expect_msg=expect_msg) for name, make_args, expect_error, expect_msg in [('float_start', lambda b: (0.0, b, None), ValueError, 'must be either dimension expressions or integers'), ('float_step', lambda b: (0, b, 0.5), ValueError, 'must be either dimension expressions or integers'), ('step_0', lambda b: (0, b, 0), ValueError, 'has step == 0'), ('inconclusive_step_sign', lambda b: (0, b, b - 2), core.InconclusiveDimensionOperation, 'must be resolved statically if it is > 0 or < 0')]])\ndef test_arange_error(self, make_args=lambda b: (0.0, b, 2), expect_error=ValueError, expect_msg='must be either dimension expressions or integers'):\n\n    def f_jax(x):\n        return x[0] + jnp.arange(*make_args(x.shape[0]))\n    x = np.ones((3,), dtype=np.int32)\n    with self.assertRaisesRegex(expect_error, expect_msg):\n        check_shape_poly(self, f_jax, arg_descriptors=[x], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'expr={name}', expr=expr) for name, expr in [('d + 2', lambda d: d + 2), ('2 - d', lambda d: 2 - d), ('d * 2', lambda d: d * 2), ('d * d', lambda d: d * d), ('(- d) * d', lambda d: -d * d), ('d * d - d', lambda d: d * d - d), ('d // 2', lambda d: d // 2), ('(d + 1) // 2', lambda d: (d + 1) // 2), ('d // -2', lambda d: d // -2), ('(d + 1) // -2', lambda d: (d + 1) // -2), ('(-d) // 2', lambda d: -d // 2), ('(-d - 1) // 2', lambda d: (-d - 1) // 2), ('(-d) // -2', lambda d: -d // -2), ('(-d - 1) // -2', lambda d: (-d - 1) // -2), ('d % 2', lambda d: d % 2), ('(d + 1) % 2', lambda d: (d + 1) % 2), ('d % -2', lambda d: d % -2), ('(d + 1) % -2', lambda d: (d + 1) % -2), ('(-d) % 2', lambda d: -d % 2), ('(-d - 1) % 2', lambda d: (-d - 1) % 2), ('(-d) % -2', lambda d: -d % -2), ('(-d - 1) % -2', lambda d: (-d - 1) % -2)]])\ndef test_non_trivial_dim_expr(self, expr=lambda d: d % -2):\n    check_shape_poly(self, lambda x: x[0] * 0 + expr(x.shape[0]), arg_descriptors=[RandArg((3,), np.int64)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_forgot_polymorphic_shapes_error(self):\n    msg_re = 'syntax error in symbolic shape'\n    with self.assertRaisesRegex(ValueError, msg_re):\n        check_shape_poly(self, jnp.sin, arg_descriptors=[RandArg((1, 3), _f32)], input_signature=[tf.TensorSpec([1, None])], polymorphic_shapes=[None])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_with_constraints(self):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('not supported')\n\n    def f_jax(x):\n        return lax.dynamic_slice_in_dim(x, 0, 8, 0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['a'], polymorphic_constraints=['a >= 8'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_arg_avals_errors(self):\n    \"\"\"Test error reporting for shape polymorphism.\"\"\"\n\n    def conv_and_run(*, arg_shape: core.Shape, polymorphic_shape: str):\n        arg = np.arange(math.prod(arg_shape), dtype=np.float32).reshape(arg_shape)\n        check_shape_poly(self, lambda x: x, arg_descriptors=[arg], polymorphic_shapes=[polymorphic_shape])\n    with self.assertRaisesRegex(ValueError, re.escape('polymorphic shape spec should be')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=5.0)\n    with self.assertRaisesRegex(ValueError, re.escape('pytree structure error: different types')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=['a list'])\n    with self.assertRaisesRegex(ValueError, re.escape('pytree structure error: different types')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=('a tuple',))\n    with self.assertRaisesRegex(ValueError, \"Cannot solve for values of dimension variables {'b'}\"):\n        conv_and_run(arg_shape=(4, 36, 3), polymorphic_shape='b * b, b * d * d, d')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, \"Division had remainder 2 when computing the value of 'b'\"):\n        conv_and_run(arg_shape=(5, 36), polymorphic_shape='3 * b, ...')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, \"Expected value >= 1 for dimension variable 'b'\"):\n        conv_and_run(arg_shape=(10, 3), polymorphic_shape='3 * b + 10, ...')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, \"Expected value >= 1 for dimension variable 'b'\"):\n        conv_and_run(arg_shape=(7, 3), polymorphic_shape='3 * b + 10, ...')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 3) and the specification 'a' (= 2)\")):\n        conv_and_run(arg_shape=(2, 3), polymorphic_shape='(a, a)')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(testcase_name=lambda kw: kw['shape'], kwargs=[dict(shape=(8, 2, 9), poly_spec='(a + 2*b, a, a + b + c)'), dict(shape=(2, 2, 6), poly_spec='(a + 2*b, a, a + b + c)', expect_error=\"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (2*b + a, a, c + b + a). Obtained dimension variables: 'a' = 2 from specification 'a' for dimension args[0].shape[1] (= 2), 'b' = 0 from specification '2*b + a' for dimension args[0].shape[0] (= 2), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\"), dict(shape=(3, 2, 6), poly_spec='(a + 2*b, a, a + b + c)', expect_error=\"Input shapes do not match the polymorphic shapes specification. Division had remainder 1 when computing the value of 'b'. Using the following polymorphic shapes specifications: args[0].shape = (2*b + a, a, c + b + a). Obtained dimension variables: 'a' = 2 from specification 'a' for dimension args[0].shape[1] (= 2), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\"), dict(shape=(8, 2, 6), poly_spec='(a + 2*b, a, a + b)', expect_error=\"Input shapes do not match the polymorphic shapes specification. Found inconsistency between dimension size args[0].shape[0] (= 8) and the specification '2*b + a' (= 10). Using the following polymorphic shapes specifications: args[0].shape = (2*b + a, a, b + a). Obtained dimension variables: 'a' = 2 from specification 'a' for dimension args[0].shape[1] (= 2), 'b' = 4 from specification 'b + a' for dimension args[0].shape[2] (= 6), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\"), dict(shape=(7, 2, 36), poly_spec='(2 * a + b, a, c * c)', expect_error=\"Cannot solve for values of dimension variables {'c'}. We can only solve linear uni-variate constraints. Using the following polymorphic shapes specifications: args[0].shape = (b + 2*a, a, c^2). Unprocessed specifications: 'c^2' for dimension size args[0].shape[2]. Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#dimension-variables-must-be-solvable-from-the-input-shapes for more details.\")])\ndef test_shape_constraints_errors(self, *, shape, poly_spec: str, expect_error: str | None=None):\n\n    def f_jax(x):\n        return 0.0\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    with contextlib.ExitStack() as stack:\n        if expect_error is not None:\n            stack.push(self.assertRaisesRegex(Exception, re.escape(expect_error)))\n        _ = check_shape_poly(self, f_jax, arg_descriptors=[x], polymorphic_shapes=[poly_spec])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_pytree(self):\n    \"\"\"Arguments and polymorphic_shapes are pytrees.\"\"\"\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    input_signature = [([tf.TensorSpec([None]), tf.TensorSpec([None])], [tf.TensorSpec([None])]), dict(a=tf.TensorSpec([None]), b=tf.TensorSpec([None]))]\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[(['v', 'v'], ['v']), dict(a='v', b='v')], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes='v', expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=['v', 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[('v', 'v'), 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=[([tf.TensorSpec([4]), tf.TensorSpec([4])], [tf.TensorSpec([4])]), dict(a=tf.TensorSpec([4]), b=tf.TensorSpec([4]))], polymorphic_shapes=((['(4,)', '(_,)'], ['4,']), dict(a='(_,)', b='(4,)')), expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[1], dtype=x.dtype)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=str(polymorphic_shapes), polymorphic_shapes=polymorphic_shapes) for polymorphic_shapes in ['b1+6,b1+14,b2', '2*b1,4*b2,b1+b2+18', 'b1+2*b2,4*b2,b1*b1+16']])\ndef test_non_trivial_polynomials_spec(self, polymorphic_shapes='2*b1,4*b2,b1+b2+18'):\n    check_shape_poly(self, lambda x: 2 * x.shape[0] + 3 * x.shape[1] + 4 * x.shape[2], arg_descriptors=[RandArg((16, 24, 32), _f32)], polymorphic_shapes=[polymorphic_shapes])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_unused_args(self):\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((2, 3), _f32), RandArg((3,), _f32)], polymorphic_shapes=[None, 'b'])\n    check_shape_poly(self, lambda x_unused, y, z_unused, w: jnp.concatenate([y, w]), arg_descriptors=[RandArg((3,), _f32), RandArg((4,), _f32), RandArg((5,), _f32), RandArg((6,), _f32)], polymorphic_shapes=[None, 'b1', None, 'b2'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((3,), _f32), RandArg((3,), _f32)], polymorphic_shapes=['b', 'b'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((4,), _f32), RandArg((3,), _f32)], polymorphic_shapes=['b1', 'b2'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((3,), _f32), RandArg((9,), _f32)], polymorphic_shapes=['b1', 'b1 * b1'])\n    check_shape_poly(self, lambda x_unused, y: y + x_unused.shape[0], arg_descriptors=[RandArg((3,), _f32), RandArg((9,), _f32)], polymorphic_shapes=['b1', 'b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_gradients_pytree(self):\n    \"\"\"Shape polymorphism with gradients and pytrees for inputs and outputs.\"\"\"\n\n    def f(x):\n        return dict(res=x['x'] * 2.0)\n    check_shape_poly(self, f, skip_jax_run=True, input_signature=[dict(x=tf.TensorSpec([None, 3, 4]))], polymorphic_shapes=[dict(x='b, 3, 4')])\n    f_tf = jax2tf.convert(f, polymorphic_shapes=[dict(x='b, 3, 4')])\n    x = dict(x=np.ones((2, 3, 4), dtype=np.float32))\n    xv = tf.Variable(x['x'], dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(dict(x=xv))\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, dict(grad=res_tf_grad))\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[0]['res']))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[1]['grad']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_prng(self):\n    with config.enable_custom_prng(True):\n\n        def f_jax(x):\n            key = random.PRNGKey(123)\n            broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n            gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n            slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n            slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n            upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n            _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n            xs = broadcast_keys\n            counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n            def f_vmap_jax(counts, xs):\n\n                def inner(count, x):\n                    return lax.fori_loop(0, count, lambda _, acc: acc, x)\n                return jax.vmap(inner)(counts, xs)\n            _ = f_vmap_jax(counts, xs)\n            return x\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b1, b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=op_name, op=op) for op, op_name in [(jnp.array, 'array'), (jnp.sin, 'sin'), (lambda x: x, 'id'), (core.dimension_as_value, 'dimension_as_value')]])\ndef test_poly_unary_op(self, *, op=jnp.array):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        return (op(poly), x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), _f32)], polymorphic_shapes=['b'], expected_output_signature=(tf.TensorSpec([]), tf.TensorSpec((None,), _f32)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_{op.__name__}_other={other}:{type(other)}{('_other_jnp_array' if other_jnp_array else '')}{('_swap' if swap else '')}', op=op, other=other, other_jnp_array=other_jnp_array, swap=swap) for op in [op.add, op.mul, op.sub, op.mod, op.floordiv, op.truediv] for other in [2, np.int32(2), 2.0, np.float32(2), np.array(2, dtype=np.int32), np.arange(1, 5, dtype=np.int32), np.array(2.0, dtype=np.float32), np.arange(1.0, 7.0, dtype=np.float32)] for other_jnp_array in ([True, False] if np.shape(other) == (7,) else [False]) for swap in [False, True]])\ndef test_poly_binary_op(self, *, op=op.add, other=np.arange(2, dtype=np.int32), other_jnp_array=False, swap=True):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        other_wrapped = jnp.array(other) if other_jnp_array else other\n        ops = (poly, other_wrapped) if not swap else (other_wrapped, poly)\n        res = op(*ops)\n        try:\n            op.index(other)\n            other_isint = True\n        except Exception:\n            other_isint = False\n        if hasattr(poly, 'dimension_as_value') and other_isint and (op.__name__ != 'truediv'):\n            self.assertTrue(isinstance(res, int) or hasattr(res, 'dimension_as_value'))\n        if config.enable_x64.value:\n            return (lax.convert_element_type(res, np.float32), x)\n        return (res, x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), np.int32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_mean0(self):\n\n    def f_jax(x):\n        return jnp.sum(x, axis=0) / x.shape[0]\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_shape_as_array(self):\n\n    def f_jax(x):\n        return x + jnp.sum(jnp.array(x.shape)).astype(np.int32)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b, _'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_dim_as_value_weak_type(self):\n\n    def f_jax(x):\n        d0 = jnp.array(x.shape[0])\n        if isinstance(d0, core.Tracer):\n            (self.assertTrue(d0.aval.weak_type), d0)\n        d1 = x.shape[0] + jnp.array(4)\n        if isinstance(d1, core.Tracer):\n            (self.assertTrue(d1.aval.weak_type), d1)\n        return d0 + np.array(5.0, dtype=np.float32) + d1 + x[0]\n    with config.numpy_dtype_promotion('strict'):\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), _f32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_vmap_while(self):\n\n    def cond_func(x):\n        return jnp.sum(x) >= 0.0\n\n    def body_func(x):\n        return x - 1.0\n\n    def f_jax(x):\n        return lax.while_loop(cond_func, body_func, x)\n    check_shape_poly(self, jax.vmap(f_jax), arg_descriptors=[RandArg((5, 3), _f32)], polymorphic_shapes=['b, ...'], expected_output_signature=tf.TensorSpec((None, 3), dtype=tf.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error=None) -> jax.Array | None:\n    h = PolyHarness('', '', jax.jit(f_jax), arg_descriptors=arg_descriptors, polymorphic_shapes=polymorphic_shapes, symbolic_constraints=symbolic_constraints, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@test_harnesses.parameterized(_flatten_harnesses(_POLY_SHAPE_TEST_HARNESSES))\ndef test_harness(self, harness: PolyHarness):\n    if harness.expect_error == expect_error_associative_scan and (not config.jax2tf_default_native_serialization.value or jtu.test_device_matches(['tpu'])):\n        harness.expect_error = (None, None)\n    custom_call_harnesses = {'householder_product:gpu', 'vmap_geqrf:gpu', 'vmap_lu:gpu', 'vmap_custom_linear_solve:gpu', 'vmap_qr:gpu', 'qr:gpu', 'vmap_svd:gpu'}\n    if f'{harness.group_name}:{jtu.device_under_test()}' in custom_call_harnesses:\n        raise unittest.SkipTest('native serialization with shape polymorphism not implemented for custom calls; b/261671778')\n    if harness.group_name == 'schur' and (not jtu.test_device_matches(['cpu'])):\n        raise unittest.SkipTest('schur decomposition is only implemented on CPU.')\n    if 'fft_fft_type' in harness.fullname:\n        if 'nr_fft_lengths_2' in harness.fullname:\n            raise unittest.SkipTest('native serialization with shape polymorphism not implemented for fft with non-constant fft_lengths on GPU and TPU')\n    if harness.group_name == 'vmap_eigh' and jtu.test_device_matches(['gpu']):\n        shape = harness.original_harness.params['shape']\n        if 0 < shape[-1] <= 32:\n            harness.check_result = False\n    if harness.group_name == 'vmap_eigh':\n        raise unittest.SkipTest('Should not compare eigendecompositions for equality directlybecause eigenvalues are sorted.')\n    if harness.group_name == 'vmap_tan':\n        raise unittest.SkipTest('native lowering with shape polymorphism requires additional StableHLO feature support')\n    if jtu.test_device_matches(['cpu', 'gpu']) and harness.fullname in ['cumsum_reduce_axis_poly', 'cumprod_reduce_axis_poly', 'cummin_reduce_axis_poly', 'cummax_reduce_axis_poly', 'cumlogsumexp_reduce_axis_poly', 'jnp_insert_insert_constant', 'jnp_insert_insert_poly', 'jnp_nonzero_size_constant', 'jnp_nonzero_size_poly']:\n        raise unittest.SkipTest('native serialization with shape polymorphism not implemented for window_reductions on CPU and GPU')\n    if harness.group_name == 'vmap_conv_general_dilated':\n        raise unittest.SkipTest('Need more dynamism for DynamicConvOp')\n    if harness.group_name == 'eig' and (not jtu.test_device_matches(['cpu'])):\n        raise unittest.SkipTest('JAX implements eig only on CPU.')\n    with jtu.thread_local_config_context(**harness.override_jax_config_flags):\n        harness.run_test(self)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def run_test(self, tst: jtu.JaxTestCase) -> jax.Array | None:\n\n    def log_message(extra: str):\n        return f'[{tst._testMethodName}]: {extra}'\n    for fname, fvalue in self.override_jax_config_flags.items():\n        tst.assertEqual(getattr(jax.config, fname), fvalue, f'Flag {fname} current value {getattr(jax.config, fname)} != {fvalue}')\n    f_jax = jax.jit(self.dyn_fun)\n    args = self.dyn_args_maker(tst.rng())\n    args = jax.tree.map(jnp.array, args)\n    args_specs = export.symbolic_args_specs(args, self.polymorphic_shapes, constraints=self.symbolic_constraints)\n    if self.expect_error is not None:\n        with tst.assertRaisesRegex(self.expect_error[0], self.expect_error[1]):\n            export.export(f_jax)(*args_specs)\n        return None\n    exp = export.export(f_jax)(*args_specs)\n    if not self.check_result:\n        return None\n    res_jax_native = f_jax(*args)\n    res_jax_exported = exp.call(*args)\n    custom_assert_lims = [l for l in self.limitations if l.custom_assert is not None]\n    assert len(custom_assert_lims) <= 1, custom_assert_lims\n    tol = None\n    if self.tol is not None:\n        tol = self.tol\n    elif self.limitations:\n        max_lim = self.limitations[0].get_max_tolerance_limitation(self.limitations)\n        if max_lim is not None:\n            tol = max_lim.tol\n    if not custom_assert_lims:\n        tst.assertAllClose(res_jax_native, res_jax_exported, atol=tol, rtol=tol)\n    else:\n        logging.info(log_message(f'Running custom_assert with tol={tol} due to {custom_assert_lims[0]}'))\n        custom_assert_lims[0].custom_assert(tst, res_jax_native, res_jax_exported, args=args, tol=tol, err_msg=None)\n    return res_jax_exported"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_prng(self):\n    with config.enable_custom_prng(True):\n\n        def f_jax(x):\n            key = random.PRNGKey(123)\n            broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n            gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n            slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n            slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n            upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n            _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n            xs = broadcast_keys\n            counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n            def f_vmap_jax(counts, xs):\n\n                def inner(count, x):\n                    return lax.fori_loop(0, count, lambda _, acc: acc, x)\n                return jax.vmap(inner)(counts, xs)\n            _ = f_vmap_jax(counts, xs)\n            return x\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b1, b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_vmap_jax(counts, xs):\n\n    def inner(count, x):\n        return lax.fori_loop(0, count, lambda _, acc: acc, x)\n    return jax.vmap(inner)(counts, xs)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['_, h'], expected_output_signature=tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_simple_binary(self):\n    \"\"\"Test shape polymorphism for a simple case, binary function.\"\"\"\n\n    def f_jax(x, y):\n        return x + jnp.sin(y)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=[None, None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=['_, h', '_, h'], input_signature=[tf.TensorSpec([2, None]), tf.TensorSpec([2, 3])], expected_output_signature=tf.TensorSpec([2, 3]) if not config.jax2tf_default_native_serialization.value else tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32), RandArg((3, 3), _f32)], polymorphic_shapes=['h, h', 'h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[1], dtype=x.dtype)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@compute_on('device_host')\n@jax.jit\ndef f1(x):\n    x = x * 3\n    return f0(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@compute_on('device_host')\n@jax.jit\ndef f1(x):\n    x = x * 3\n    return f0(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f2(xs):\n    _, res = jax.lax.scan(body2, 1, xs)\n    return res"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f2(xs):\n    _, res = jax.lax.scan(body2, 1, xs)\n    return res"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f1(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f1(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f2(x):\n    res1 = exp_f1.call(x)\n    res2 = exp_f1.call(res1)\n    return jnp.cos(res2)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f2(x):\n    res1 = exp_f1.call(x)\n    res2 = exp_f1.call(res1)\n    return jnp.cos(res2)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['_, h'], expected_output_signature=tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_simple_binary(self):\n    \"\"\"Test shape polymorphism for a simple case, binary function.\"\"\"\n\n    def f_jax(x, y):\n        return x + jnp.sin(y)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=[None, None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=['_, h', '_, h'], input_signature=[tf.TensorSpec([2, None]), tf.TensorSpec([2, 3])], expected_output_signature=tf.TensorSpec([2, 3]) if not config.jax2tf_default_native_serialization.value else tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32), RandArg((3, 3), _f32)], polymorphic_shapes=['h, h', 'h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[1], dtype=x.dtype)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f2(x):\n    debug_print('hello: {}', x)\n    debug_print('hello: {}', x + 2)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.pmap\ndef f2(x):\n    debug_print('hello: {}', x)\n    debug_print('hello: {}', x + 2)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f1(y, z):\n    v = api.vmap(lambda _y: f2(_y, z))(y)\n    return jnp.sum(v)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f1(y, z):\n    v = api.vmap(lambda _y: f2(_y, z))(y)\n    return jnp.sum(v)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f2(y, z):\n    v1 = z\n    v2 = jnp.sum(y) + z\n    return jnp.logaddexp(v1, v2)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f2(y, z):\n    v1 = z\n    v2 = jnp.sum(y) + z\n    return jnp.logaddexp(v1, v2)"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['_, h'], expected_output_signature=tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_simple_binary(self):\n    \"\"\"Test shape polymorphism for a simple case, binary function.\"\"\"\n\n    def f_jax(x, y):\n        return x + jnp.sin(y)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=[None, None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=['_, h', '_, h'], input_signature=[tf.TensorSpec([2, None]), tf.TensorSpec([2, 3])], expected_output_signature=tf.TensorSpec([2, 3]) if not config.jax2tf_default_native_serialization.value else tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32), RandArg((3, 3), _f32)], polymorphic_shapes=['h, h', 'h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[1], dtype=x.dtype)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_mean0(self):\n\n    def f_jax(x):\n        return jnp.sum(x, axis=0) / x.shape[0]\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_shape_as_array(self):\n\n    def f_jax(x):\n        return x + jnp.sum(jnp.array(x.shape)).astype(np.int32)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b, _'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_vmap_while(self):\n\n    def cond_func(x):\n        return jnp.sum(x) >= 0.0\n\n    def body_func(x):\n        return x - 1.0\n\n    def f_jax(x):\n        return lax.while_loop(cond_func, body_func, x)\n    check_shape_poly(self, jax.vmap(f_jax), arg_descriptors=[RandArg((5, 3), _f32)], polymorphic_shapes=['b, ...'], expected_output_signature=tf.TensorSpec((None, 3), dtype=tf.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f2(x, y):\n    with set_xla_metadata(a='b'):\n        return (x + y, y * 2.0)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f2(x, y):\n    with set_xla_metadata(a='b'):\n        return (x + y, y * 2.0)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pytree(self):\n    \"\"\"Arguments and polymorphic_shapes are pytrees.\"\"\"\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    input_signature = [([tf.TensorSpec([None]), tf.TensorSpec([None])], [tf.TensorSpec([None])]), dict(a=tf.TensorSpec([None]), b=tf.TensorSpec([None]))]\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[(['v', 'v'], ['v']), dict(a='v', b='v')], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes='v', expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=['v', 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[('v', 'v'), 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=[([tf.TensorSpec([4]), tf.TensorSpec([4])], [tf.TensorSpec([4])]), dict(a=tf.TensorSpec([4]), b=tf.TensorSpec([4]))], polymorphic_shapes=((['(4,)', '(_,)'], ['4,']), dict(a='(_,)', b='(4,)')), expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef reduce(x):\n    return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(red_shape, dty), in_specs=[pl.BlockSpec(in_shape)], out_specs=pl.BlockSpec(red_shape), grid=1)(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        jax2tf.convert(add_all_jax, polymorphic_shapes=polymorphic_shapes)(*args)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef reduce(x):\n    return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(red_shape, dty), in_specs=[pl.BlockSpec(in_shape)], out_specs=pl.BlockSpec(red_shape), grid=1)(x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape=(128, 128, 128)):\n    m, l = x.shape\n    l2, n = y.shape\n    assert l2 == l\n    block_m, block_n, block_l = block_shape\n    assert l % block_l == 0, f'l={l!r}, block_l={block_l!r}'\n    assert m % block_m == 0, f'm={m!r}, block_m={block_m!r}'\n    assert n % block_n == 0, f'n={n!r}, block_n={block_n!r}'\n    grid = (m // block_m, n // block_n, l // block_l)\n    fused_matmul = pl.pallas_call(functools.partial(matmul_kernel), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((block_m, block_l), lambda i, j, k: (i, k)), pl.BlockSpec((block_l, block_n), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((block_m, block_n), lambda i, j, k: (i, j)), grid=grid, interpret=jtu.test_device_matches(['cpu']))\n    return fused_matmul(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape=(128, 128, 128)):\n    m, l = x.shape\n    l2, n = y.shape\n    assert l2 == l\n    block_m, block_n, block_l = block_shape\n    assert l % block_l == 0, f'l={l!r}, block_l={block_l!r}'\n    assert m % block_m == 0, f'm={m!r}, block_m={block_m!r}'\n    assert n % block_n == 0, f'n={n!r}, block_n={block_n!r}'\n    grid = (m // block_m, n // block_n, l // block_l)\n    fused_matmul = pl.pallas_call(functools.partial(matmul_kernel), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((block_m, block_l), lambda i, j, k: (i, k)), pl.BlockSpec((block_l, block_n), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((block_m, block_n), lambda i, j, k: (i, j)), grid=grid, interpret=jtu.test_device_matches(['cpu']))\n    return fused_matmul(x, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pytree(self):\n    \"\"\"Arguments and polymorphic_shapes are pytrees.\"\"\"\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    input_signature = [([tf.TensorSpec([None]), tf.TensorSpec([None])], [tf.TensorSpec([None])]), dict(a=tf.TensorSpec([None]), b=tf.TensorSpec([None]))]\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[(['v', 'v'], ['v']), dict(a='v', b='v')], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes='v', expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=['v', 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[('v', 'v'), 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=[([tf.TensorSpec([4]), tf.TensorSpec([4])], [tf.TensorSpec([4])]), dict(a=tf.TensorSpec([4]), b=tf.TensorSpec([4]))], polymorphic_shapes=((['(4,)', '(_,)'], ['4,']), dict(a='(_,)', b='(4,)')), expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef reduce(x_ref, y_ref):\n    x = x_ref[...]\n    y_ref[...] = jnp.cumsum(x, axis=axis)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        jax2tf.convert(add_all_jax, polymorphic_shapes=polymorphic_shapes)(*args)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef reduce(x_ref, y_ref):\n    x = x_ref[...]\n    y_ref[...] = jnp.cumsum(x, axis=axis)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        jax2tf.convert(add_all_jax, polymorphic_shapes=polymorphic_shapes)(*args)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_gradients_pytree(self):\n    \"\"\"Shape polymorphism with gradients and pytrees for inputs and outputs.\"\"\"\n\n    def f(x):\n        return dict(res=x['x'] * 2.0)\n    check_shape_poly(self, f, skip_jax_run=True, input_signature=[dict(x=tf.TensorSpec([None, 3, 4]))], polymorphic_shapes=[dict(x='b, 3, 4')])\n    f_tf = jax2tf.convert(f, polymorphic_shapes=[dict(x='b, 3, 4')])\n    x = dict(x=np.ones((2, 3, 4), dtype=np.float32))\n    xv = tf.Variable(x['x'], dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(dict(x=xv))\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, dict(grad=res_tf_grad))\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[0]['res']))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[1]['grad']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_grad_not_var_output(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    x = np.arange(12, dtype=np.float32).reshape((4, 3))\n    xv = tf.Variable(x)\n    f_tf = jax2tf.convert(f_jax, with_gradient=True, polymorphic_shapes=['b, ...'])\n    with tf.GradientTape() as tape:\n        res_tf = f_tf(xv)\n    grad_tf = tape.gradient(res_tf, xv)\n    self.assertAllClose(np.ones(x.shape, dtype=np.float32), grad_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(with_function=v) for v in [True, False]])\ndef test_grad_int(self, with_function=False):\n    x_shape = (2, 3, 4)\n    xi = np.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape)\n    yf = xi.astype(np.float32)\n    xi_yf = (xi, yf)\n    zb = np.array([True, False], dtype=np.bool_)\n\n    def f_jax(xi_yf, zb):\n        xi, yf = xi_yf\n        return (jnp.zeros(xi.shape, dtype=jnp.float32), (xi, zb, xi.astype(np.float32) * 2.0 * yf))\n    args = (xi_yf, zb)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=[('b1, b2, 4', 'b1, b2, 4'), 'b1'])\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    res_tf, g_tf = tf_test_util.ComputeTfValueAndGrad(f_tf, args)\n    self.assertAllClose(g_tf[0][0], np.zeros_like(xi))\n    self.assertAllClose(g_tf[0][1], (xi * 2).astype(yf.dtype))\n    self.assertAllClose(g_tf[1], np.zeros_like(zb))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_with_hash_collision_vmap(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (2, -1))\n    try:\n        orig_hash = getattr(shape_poly._DimExpr, '__hash__')\n\n        def collision_hash(obj):\n            return hash(5)\n        setattr(shape_poly._DimExpr, '__hash__', collision_hash)\n        xs = np.ones((3, 5, 6), dtype=np.float32)\n        f_toconvert = jax.vmap(pjit.pjit(f_jax))\n        res_1 = jax2tf.convert(f_toconvert)(xs)\n        res_2 = jax2tf.convert(f_toconvert, polymorphic_shapes='b1, b2, ...')(xs)\n        self.assertAllClose(res_1, res_2)\n    finally:\n        setattr(shape_poly._DimExpr, '__hash__', orig_hash)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_vmap_error(self):\n    x = y = np.ones((3, 5), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y: x + y), polymorphic_shapes=['b, ...', None])(x, y)\n    z = x\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y, z: x + y + z), polymorphic_shapes=['b, ...', 'c, ...', None])(x, y, z)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['_, h'], expected_output_signature=tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_simple_binary(self):\n    \"\"\"Test shape polymorphism for a simple case, binary function.\"\"\"\n\n    def f_jax(x, y):\n        return x + jnp.sin(y)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=[None, None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=['_, h', '_, h'], input_signature=[tf.TensorSpec([2, None]), tf.TensorSpec([2, 3])], expected_output_signature=tf.TensorSpec([2, 3]) if not config.jax2tf_default_native_serialization.value else tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32), RandArg((3, 3), _f32)], polymorphic_shapes=['h, h', 'h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[1], dtype=x.dtype)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        jax2tf.convert(add_all_jax, polymorphic_shapes=polymorphic_shapes)(*args)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_gradients_pytree(self):\n    \"\"\"Shape polymorphism with gradients and pytrees for inputs and outputs.\"\"\"\n\n    def f(x):\n        return dict(res=x['x'] * 2.0)\n    check_shape_poly(self, f, skip_jax_run=True, input_signature=[dict(x=tf.TensorSpec([None, 3, 4]))], polymorphic_shapes=[dict(x='b, 3, 4')])\n    f_tf = jax2tf.convert(f, polymorphic_shapes=[dict(x='b, 3, 4')])\n    x = dict(x=np.ones((2, 3, 4), dtype=np.float32))\n    xv = tf.Variable(x['x'], dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(dict(x=xv))\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, dict(grad=res_tf_grad))\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[0]['res']))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[1]['grad']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_grad_not_var_output(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    x = np.arange(12, dtype=np.float32).reshape((4, 3))\n    xv = tf.Variable(x)\n    f_tf = jax2tf.convert(f_jax, with_gradient=True, polymorphic_shapes=['b, ...'])\n    with tf.GradientTape() as tape:\n        res_tf = f_tf(xv)\n    grad_tf = tape.gradient(res_tf, xv)\n    self.assertAllClose(np.ones(x.shape, dtype=np.float32), grad_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(with_function=v) for v in [True, False]])\ndef test_grad_int(self, with_function=False):\n    x_shape = (2, 3, 4)\n    xi = np.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape)\n    yf = xi.astype(np.float32)\n    xi_yf = (xi, yf)\n    zb = np.array([True, False], dtype=np.bool_)\n\n    def f_jax(xi_yf, zb):\n        xi, yf = xi_yf\n        return (jnp.zeros(xi.shape, dtype=jnp.float32), (xi, zb, xi.astype(np.float32) * 2.0 * yf))\n    args = (xi_yf, zb)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=[('b1, b2, 4', 'b1, b2, 4'), 'b1'])\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    res_tf, g_tf = tf_test_util.ComputeTfValueAndGrad(f_tf, args)\n    self.assertAllClose(g_tf[0][0], np.zeros_like(xi))\n    self.assertAllClose(g_tf[0][1], (xi * 2).astype(yf.dtype))\n    self.assertAllClose(g_tf[1], np.zeros_like(zb))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_with_hash_collision_vmap(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (2, -1))\n    try:\n        orig_hash = getattr(shape_poly._DimExpr, '__hash__')\n\n        def collision_hash(obj):\n            return hash(5)\n        setattr(shape_poly._DimExpr, '__hash__', collision_hash)\n        xs = np.ones((3, 5, 6), dtype=np.float32)\n        f_toconvert = jax.vmap(pjit.pjit(f_jax))\n        res_1 = jax2tf.convert(f_toconvert)(xs)\n        res_2 = jax2tf.convert(f_toconvert, polymorphic_shapes='b1, b2, ...')(xs)\n        self.assertAllClose(res_1, res_2)\n    finally:\n        setattr(shape_poly._DimExpr, '__hash__', orig_hash)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_vmap_error(self):\n    x = y = np.ones((3, 5), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y: x + y), polymorphic_shapes=['b, ...', None])(x, y)\n    z = x\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y, z: x + y + z), polymorphic_shapes=['b, ...', 'c, ...', None])(x, y, z)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0).astype(intx)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0).astype(intx)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_{op.__name__}_other={other}:{type(other)}{('_other_jnp_array' if other_jnp_array else '')}{('_swap' if swap else '')}', op=op, other=other, other_jnp_array=other_jnp_array, swap=swap) for op in [op.add, op.mul, op.sub, op.mod, op.floordiv, op.truediv] for other in [2, np.int32(2), 2.0, np.float32(2), np.array(2, dtype=np.int32), np.arange(1, 5, dtype=np.int32), np.array(2.0, dtype=np.float32), np.arange(1.0, 7.0, dtype=np.float32)] for other_jnp_array in ([True, False] if np.shape(other) == (7,) else [False]) for swap in [False, True]])\ndef test_poly_binary_op(self, *, op=op.add, other=np.arange(2, dtype=np.int32), other_jnp_array=False, swap=True):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        other_wrapped = jnp.array(other) if other_jnp_array else other\n        ops = (poly, other_wrapped) if not swap else (other_wrapped, poly)\n        res = op(*ops)\n        try:\n            op.index(other)\n            other_isint = True\n        except Exception:\n            other_isint = False\n        if hasattr(poly, 'dimension_as_value') and other_isint and (op.__name__ != 'truediv'):\n            self.assertTrue(isinstance(res, int) or hasattr(res, 'dimension_as_value'))\n        if config.enable_x64.value:\n            return (lax.convert_element_type(res, np.float32), x)\n        return (res, x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), np.int32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), floatx))\ndef index(x_ref, idx_ref, o_ref):\n    idx = idx_ref[()]\n    o_ref[:] = x_ref[idx]"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape', 'block_k', 'debug', 'out_dtype'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape, block_k: int=256, out_dtype: jnp.dtype | None=None, debug: bool=False) -> jax.Array:\n    if out_dtype is None:\n        if x.dtype != y.dtype:\n            raise TypeError(f'Cannot deduce output dtype for different input dtypes: {x.dtype}, {y.dtype}')\n        out_dtype = x.dtype\n    acc_dtype = jnp.float32\n    if x.dtype in [jnp.int8, jnp.int4, jnp.uint8, jnp.uint4]:\n        acc_dtype = jnp.int32\n    l, r = block_shape\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec((l, block_k), lambda i, _, k: (i, k)), pl.BlockSpec((block_k, r), lambda _, j, k: (k, j))], out_specs=pl.BlockSpec((l, r), lambda i, j, k: (i, j)), grid=(x.shape[0] // l, y.shape[1] // r, x.shape[1] // block_k), scratch_shapes=[pltpu.VMEM((l, r), acc_dtype)]), compiler_params=pltpu.TPUCompilerParams(dimension_semantics=('parallel', 'parallel', 'arbitrary')), debug=debug)(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape', 'block_k', 'debug', 'out_dtype'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape, block_k: int=256, out_dtype: jnp.dtype | None=None, debug: bool=False) -> jax.Array:\n    if out_dtype is None:\n        if x.dtype != y.dtype:\n            raise TypeError(f'Cannot deduce output dtype for different input dtypes: {x.dtype}, {y.dtype}')\n        out_dtype = x.dtype\n    acc_dtype = jnp.float32\n    if x.dtype in [jnp.int8, jnp.int4, jnp.uint8, jnp.uint4]:\n        acc_dtype = jnp.int32\n    l, r = block_shape\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec((l, block_k), lambda i, _, k: (i, k)), pl.BlockSpec((block_k, r), lambda _, j, k: (k, j))], out_specs=pl.BlockSpec((l, r), lambda i, j, k: (i, j)), grid=(x.shape[0] // l, y.shape[1] // r, x.shape[1] // block_k), scratch_shapes=[pltpu.VMEM((l, r), acc_dtype)]), compiler_params=pltpu.TPUCompilerParams(dimension_semantics=('parallel', 'parallel', 'arbitrary')), debug=debug)(x, y)"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, polymorphic_shapes=polymorphic_shapes) for name, polymorphic_shapes in [('1', ('b', 'b', 'b')), ('2', dict(a='b')), ('3', (dict(a='b'), 'b'))]])\ndef test_pytree_errors(self, polymorphic_shapes=('b', 'b', 'b')):\n    \"\"\"Arguments and polymorphic_shapes are not-matching pytrees.\"\"\"\n    x = np.arange(4, dtype=_f32)\n    args = (([x, x], [x]), dict(a=x, b=x))\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    with self.assertRaisesRegex(ValueError, 'pytree structure error'):\n        jax2tf.convert(add_all_jax, polymorphic_shapes=polymorphic_shapes)(*args)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_gradients_pytree(self):\n    \"\"\"Shape polymorphism with gradients and pytrees for inputs and outputs.\"\"\"\n\n    def f(x):\n        return dict(res=x['x'] * 2.0)\n    check_shape_poly(self, f, skip_jax_run=True, input_signature=[dict(x=tf.TensorSpec([None, 3, 4]))], polymorphic_shapes=[dict(x='b, 3, 4')])\n    f_tf = jax2tf.convert(f, polymorphic_shapes=[dict(x='b, 3, 4')])\n    x = dict(x=np.ones((2, 3, 4), dtype=np.float32))\n    xv = tf.Variable(x['x'], dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(dict(x=xv))\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, dict(grad=res_tf_grad))\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[0]['res']))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[1]['grad']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_grad_not_var_output(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    x = np.arange(12, dtype=np.float32).reshape((4, 3))\n    xv = tf.Variable(x)\n    f_tf = jax2tf.convert(f_jax, with_gradient=True, polymorphic_shapes=['b, ...'])\n    with tf.GradientTape() as tape:\n        res_tf = f_tf(xv)\n    grad_tf = tape.gradient(res_tf, xv)\n    self.assertAllClose(np.ones(x.shape, dtype=np.float32), grad_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(with_function=v) for v in [True, False]])\ndef test_grad_int(self, with_function=False):\n    x_shape = (2, 3, 4)\n    xi = np.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape)\n    yf = xi.astype(np.float32)\n    xi_yf = (xi, yf)\n    zb = np.array([True, False], dtype=np.bool_)\n\n    def f_jax(xi_yf, zb):\n        xi, yf = xi_yf\n        return (jnp.zeros(xi.shape, dtype=jnp.float32), (xi, zb, xi.astype(np.float32) * 2.0 * yf))\n    args = (xi_yf, zb)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=[('b1, b2, 4', 'b1, b2, 4'), 'b1'])\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    res_tf, g_tf = tf_test_util.ComputeTfValueAndGrad(f_tf, args)\n    self.assertAllClose(g_tf[0][0], np.zeros_like(xi))\n    self.assertAllClose(g_tf[0][1], (xi * 2).astype(yf.dtype))\n    self.assertAllClose(g_tf[1], np.zeros_like(zb))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_readme_examples(self):\n    \"\"\"Some of the examples from the README.\"\"\"\n    jax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (math.prod(x.shape),)), polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: x + x.shape[0] + jnp.sin(x.shape[0]), polymorphic_shapes=['b'])(np.ones(3))\n    jax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0], polymorphic_shapes=['(v, _)'])(np.ones((3, 4)))\n    with self.assertRaisesRegex(TypeError, 'prod requires ndarray or scalar arguments'):\n        jax2tf.convert(lambda x: jnp.prod(x.shape) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    jax2tf.convert(lambda x: jnp.prod(jnp.array(x.shape)) + x, polymorphic_shapes=['(b, 4)'])(np.ones((3, 4)))\n    four_ones = np.ones((4,))\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('add got incompatible shapes for broadcasting: (v,), (4,)')):\n        jax2tf.convert(lambda x, y: x + y, polymorphic_shapes=['(v,)', '(4,)'])(four_ones, four_ones)\n    with self.assertRaisesRegex(TypeError, re.escape('dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)')):\n        jax2tf.convert(lambda x: jnp.matmul(x, x), polymorphic_shapes=['(v, 4)'])(np.ones((4, 4)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.compile('Cannot divide evenly the sizes of shapes \\\\(b, 5, 7\\\\) and \\\\(2, -1\\\\)', re.DOTALL)):\n        jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 7)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(b, _, _)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])), polymorphic_shapes=['(b1, b2, ...)'])(np.ones((4, 5, 6)))\n    jax2tf.convert(lambda x: jnp.reshape(x, (2, -1)), polymorphic_shapes=['(2*b, ...)'])(np.ones((4, 5, 7)))\n    with self.assertRaisesRegex(core.InconclusiveDimensionOperation, re.escape(\"Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive\")):\n        jax2tf.convert(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1, polymorphic_shapes=['(a, b)'])(np.ones((4, 4)))\n\n    def f1_jax(x):\n        return jnp.concatenate([x, jnp.array([0.0 if x.shape[0] == 0 else 1.0], dtype=np.float32)])\n    x0 = np.array([], np.float32)\n    self.assertEqual(jnp.array([0.0], dtype=np.float32), f1_jax(x0))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (b,). Obtained dimension variables: 'b' = 0\")):\n        _ = jax2tf.convert(f1_jax, polymorphic_shapes=['b'])(x0)\n\n    def f2_jax(x):\n        return jnp.sum(x) + (0.0 if x.shape[0] != x.shape[1] else 1.0)\n    x45 = np.ones((4, 5), dtype=np.float32)\n    self.assertEqual(jnp.sum(x45), f2_jax(x45))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 5) and the specification 'b' (= 4)\")):\n        _ = jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'])(x45)\n    x = np.ones((5,), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'Cannot solve for values of dimension variables'):\n        jax2tf.convert(lambda x: jnp.sum(x), polymorphic_shapes=['a + b'])(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_with_hash_collision_vmap(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (2, -1))\n    try:\n        orig_hash = getattr(shape_poly._DimExpr, '__hash__')\n\n        def collision_hash(obj):\n            return hash(5)\n        setattr(shape_poly._DimExpr, '__hash__', collision_hash)\n        xs = np.ones((3, 5, 6), dtype=np.float32)\n        f_toconvert = jax.vmap(pjit.pjit(f_jax))\n        res_1 = jax2tf.convert(f_toconvert)(xs)\n        res_2 = jax2tf.convert(f_toconvert, polymorphic_shapes='b1, b2, ...')(xs)\n        self.assertAllClose(res_1, res_2)\n    finally:\n        setattr(shape_poly._DimExpr, '__hash__', orig_hash)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_vmap_error(self):\n    x = y = np.ones((3, 5), dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y: x + y), polymorphic_shapes=['b, ...', None])(x, y)\n    z = x\n    with self.assertRaisesRegex(ValueError, 'vmap got inconsistent sizes for array axes to be mapped'):\n        jax2tf.convert(jax.vmap(lambda x, y, z: x + y + z), polymorphic_shapes=['b, ...', 'c, ...', None])(x, y, z)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_eval_poly_shapes(self):\n\n    def f1(x, y):\n        return jnp.concatenate([x, y], axis=1)\n\n    def f2(x, z):\n        return (jnp.concatenate([x, jax.lax.slice_in_dim(z, 0, 5, axis=1)], axis=1),)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = x\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = x_polymorphic_shape\n    z_spec, z_polymorphic_shape = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, z_spec.dtype)\n    self.assertEqual('(a, 10)', z_polymorphic_shape)\n    z = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=[x_polymorphic_shape, z_polymorphic_shape])(x, z)\n    self.assertAllClose(f2(x, f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_eval_polymorphic_shapes')\ndef eval_polymorphic_shape(fun_jax: Callable, *, polymorphic_shapes=None) -> Callable:\n    \"\"\"Evaluates the output shape in presence of shape polymorphism.\n\n  This is done without lowering or executing the function, same as for\n  `jax.eval_shape`.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during shape evaluation. See discussion for `jax2tf.convert`.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n\n  Returns: a function that takes `jax.ShapeDtypeStruct`s (or any values\n    with `.shape` and `.dtype` attributes) corresponding to the inputs for\n    `fun_jax`, and returns a tuple with:\n\n      * the jax.ShapeDtypeStruct corresponding to the result, as for\n       `jax.eval_shape`. The shape may contain symbolic dimension expressions.\n      * the value that can be passed to `polymorphic_shapes` for a subsequent\n        call to `jax2tf.eval_polymorphic_shape`, or `jax2tf.convert`.\n\n  For example:\n\n  >>> import jax\n  >>> from jax.experimental import jax2tf\n  >>> from jax import numpy as jnp\n  >>>\n  >>> f = lambda A, x: jnp.sin(jnp.dot(A, x))\n  >>> A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)\n  >>> x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)\n  >>> out_spec, out_poly_shape = jax2tf.eval_polymorphic_shape(f, polymorphic_shapes=[\"a, b\", \"b, c\"])(A, x)\n  >>> print(out_spec.shape)\n  (\"a\", \"c\")\n  >>> print(out_poly_shape)\n  (a, c)\n  >>> res_spec, res_poly_shape = jax2tf.eval_polymorphic_shape(lambda x: x.T, polymorphic_shapes=[out_poly_shape])(out_spec)\n  >>> print(res_poly_shape)\n  (c, a)\n  \"\"\"\n\n    def do_eval_polymorphic_shape(*args_specs) -> Any:\n        args_poly_specs = export.symbolic_args_specs(args_specs, polymorphic_shapes)\n        res_poly_spec = jax.eval_shape(fun_jax, *args_poly_specs)\n        res_polymorphic_shape = tree_util.tree_map(lambda r: str(r.shape), res_poly_spec)\n        return (res_poly_spec, res_polymorphic_shape)\n    return do_eval_polymorphic_shape"
  },
  {
    "test_code": "def test_eval_poly_shapes_tuple_output(self):\n\n    def f1(x, y):\n        return (x, jnp.concatenate([x, y], axis=0))\n\n    def f2(z, w):\n        return jnp.concatenate([z, w], axis=0)\n    x = np.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))\n    y = np.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))\n    x_polymorphic_shape = 'a, _'\n    y_polymorphic_shape = 'b, _'\n    zw_specs, zw_polymorphic_shapes = jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    self.assertEqual(np.float32, zw_specs[0].dtype)\n    self.assertEqual(np.float32, zw_specs[1].dtype)\n    self.assertEqual(('(a, 5)', '(b + a, 5)'), zw_polymorphic_shapes)\n    z, w = jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)\n    res = jax2tf.convert(f2, polymorphic_shapes=zw_polymorphic_shapes)(z, w)\n    self.assertAllClose(f2(*f1(x, y)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_eval_polymorphic_shapes')\ndef eval_polymorphic_shape(fun_jax: Callable, *, polymorphic_shapes=None) -> Callable:\n    \"\"\"Evaluates the output shape in presence of shape polymorphism.\n\n  This is done without lowering or executing the function, same as for\n  `jax.eval_shape`.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during shape evaluation. See discussion for `jax2tf.convert`.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n\n  Returns: a function that takes `jax.ShapeDtypeStruct`s (or any values\n    with `.shape` and `.dtype` attributes) corresponding to the inputs for\n    `fun_jax`, and returns a tuple with:\n\n      * the jax.ShapeDtypeStruct corresponding to the result, as for\n       `jax.eval_shape`. The shape may contain symbolic dimension expressions.\n      * the value that can be passed to `polymorphic_shapes` for a subsequent\n        call to `jax2tf.eval_polymorphic_shape`, or `jax2tf.convert`.\n\n  For example:\n\n  >>> import jax\n  >>> from jax.experimental import jax2tf\n  >>> from jax import numpy as jnp\n  >>>\n  >>> f = lambda A, x: jnp.sin(jnp.dot(A, x))\n  >>> A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)\n  >>> x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)\n  >>> out_spec, out_poly_shape = jax2tf.eval_polymorphic_shape(f, polymorphic_shapes=[\"a, b\", \"b, c\"])(A, x)\n  >>> print(out_spec.shape)\n  (\"a\", \"c\")\n  >>> print(out_poly_shape)\n  (a, c)\n  >>> res_spec, res_poly_shape = jax2tf.eval_polymorphic_shape(lambda x: x.T, polymorphic_shapes=[out_poly_shape])(out_spec)\n  >>> print(res_poly_shape)\n  (c, a)\n  \"\"\"\n\n    def do_eval_polymorphic_shape(*args_specs) -> Any:\n        args_poly_specs = export.symbolic_args_specs(args_specs, polymorphic_shapes)\n        res_poly_spec = jax.eval_shape(fun_jax, *args_poly_specs)\n        res_polymorphic_shape = tree_util.tree_map(lambda r: str(r.shape), res_poly_spec)\n        return (res_poly_spec, res_polymorphic_shape)\n    return do_eval_polymorphic_shape"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_with_custom_vjp(self):\n    \"\"\"Shape-polymorphic custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return jnp.matmul(x, jnp.transpose(x, axes=(0, 1, 3, 2)))\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (jnp.matmul(ct_b, residual),)\n    f.defvjp(f_fwd, f_bwd)\n    x = np.ones((2, 3, 4, 5), dtype=np.float32)\n    res_jax = f(x)\n    res_jax_grad = jax.grad(lambda x: jnp.sum(f(x)))(x)\n    f_tf = jax2tf.convert(f, polymorphic_shapes=['(batch1, batch2, d1, d2)'])\n    self.assertAllClose(res_jax, f_tf(x))\n    xv = tf.Variable(x, dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(xv)\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, res_tf_grad)\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(res_jax_grad, res_tf_grad)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([3, 4, 8, 9]))\n    if config.jax2tf_default_native_serialization.value:\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((None, None, None, None), tuple(tf_grad.output_shapes[1]))\n    else:\n        self.assertEqual((3, 4, 8, 8), tuple(tf_grad.output_shapes[0]))\n        self.assertEqual((3, 4, 8, 9), tuple(tf_grad.output_shapes[1]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_kwargs(self):\n    \"\"\"Test shape polymorphism for a function with kwargs.\"\"\"\n    x = np.ones(3, dtype=np.float32)\n    y = np.ones(1, dtype=np.float32)\n\n    def f_jax(x, *, y):\n        return x + jnp.sin(y)\n    f_tf: Callable[..., Any] = jax2tf.convert(f_jax, polymorphic_shapes=['b, ...'])\n    self.assertAllClose(f_jax(x, y=y), f_tf(x, y=y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model(self):\n    f_jax = jnp.sin\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))\n    y = np.concatenate([x, x])\n    self.assertAllClose(f_jax(y), restored_f(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model_int_function(self):\n\n    def f_jax(x):\n        return jnp.reshape(x, (-1,))\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    f_tf = tf.function(f_tf, autograph=False)\n    x_shape = (2, 3, 4)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec((None,) + x.shape[1:], x.dtype)])\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    res_jax_rt = f_jax_rt(x)\n    self.assertAllClose(f_jax(x), res_jax_rt)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model_constant_gradient(self):\n\n    def f_jax(x):\n        return x\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertAllClose(f_jax(x), restored_f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_reshape_compiled(self):\n    traced = False\n\n    def f_jax(x):\n        nonlocal traced\n        traced = True\n        y = jnp.sin(x)\n        return y.reshape([x.shape[0], -1])\n    x = self.rng().rand(4, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    f_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=['b, ...']), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))\n    self.assertTrue(traced)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)\n    x = self.rng().rand(6, 2, 3)\n    res_jax = f_jax(x)\n    traced = False\n    self.assertAllClose(res_jax, f_tf(x))\n    self.assertFalse(traced)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_simple_unary(self):\n    \"\"\"Test shape polymorphism for a simple case, unary function.\"\"\"\n\n    def f_jax(x):\n        return x + jnp.sin(x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['_, h'], expected_output_signature=tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32)], polymorphic_shapes=['h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_simple_binary(self):\n    \"\"\"Test shape polymorphism for a simple case, binary function.\"\"\"\n\n    def f_jax(x, y):\n        return x + jnp.sin(y)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=[None, None], expected_output_signature=tf.TensorSpec([2, 3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32), RandArg((2, 3), _f32)], polymorphic_shapes=['_, h', '_, h'], input_signature=[tf.TensorSpec([2, None]), tf.TensorSpec([2, 3])], expected_output_signature=tf.TensorSpec([2, 3]) if not config.jax2tf_default_native_serialization.value else tf.TensorSpec([2, None]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 3), _f32), RandArg((3, 3), _f32)], polymorphic_shapes=['h, h', 'h, h'], expected_output_signature=tf.TensorSpec([None, None]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=name, make_args=make_args, expect_error=expect_error, expect_msg=expect_msg) for name, make_args, expect_error, expect_msg in [('float_start', lambda b: (0.0, b, None), ValueError, 'must be either dimension expressions or integers'), ('float_step', lambda b: (0, b, 0.5), ValueError, 'must be either dimension expressions or integers'), ('step_0', lambda b: (0, b, 0), ValueError, 'has step == 0'), ('inconclusive_step_sign', lambda b: (0, b, b - 2), core.InconclusiveDimensionOperation, 'must be resolved statically if it is > 0 or < 0')]])\ndef test_arange_error(self, make_args=lambda b: (0.0, b, 2), expect_error=ValueError, expect_msg='must be either dimension expressions or integers'):\n\n    def f_jax(x):\n        return x[0] + jnp.arange(*make_args(x.shape[0]))\n    x = np.ones((3,), dtype=np.int32)\n    with self.assertRaisesRegex(expect_error, expect_msg):\n        check_shape_poly(self, f_jax, arg_descriptors=[x], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'expr={name}', expr=expr) for name, expr in [('d + 2', lambda d: d + 2), ('2 - d', lambda d: 2 - d), ('d * 2', lambda d: d * 2), ('d * d', lambda d: d * d), ('(- d) * d', lambda d: -d * d), ('d * d - d', lambda d: d * d - d), ('d // 2', lambda d: d // 2), ('(d + 1) // 2', lambda d: (d + 1) // 2), ('d // -2', lambda d: d // -2), ('(d + 1) // -2', lambda d: (d + 1) // -2), ('(-d) // 2', lambda d: -d // 2), ('(-d - 1) // 2', lambda d: (-d - 1) // 2), ('(-d) // -2', lambda d: -d // -2), ('(-d - 1) // -2', lambda d: (-d - 1) // -2), ('d % 2', lambda d: d % 2), ('(d + 1) % 2', lambda d: (d + 1) % 2), ('d % -2', lambda d: d % -2), ('(d + 1) % -2', lambda d: (d + 1) % -2), ('(-d) % 2', lambda d: -d % 2), ('(-d - 1) % 2', lambda d: (-d - 1) % 2), ('(-d) % -2', lambda d: -d % -2), ('(-d - 1) % -2', lambda d: (-d - 1) % -2)]])\ndef test_non_trivial_dim_expr(self, expr=lambda d: d % -2):\n    check_shape_poly(self, lambda x: x[0] * 0 + expr(x.shape[0]), arg_descriptors=[RandArg((3,), np.int64)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_static_shape_result(self):\n    \"\"\"The result has static shape.\"\"\"\n\n    def f_jax(x):\n        return jnp.sum(x + jnp.sin(x), axis=0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=[None], expected_output_signature=tf.TensorSpec([3]))\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((2, 3), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([3]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_forgot_polymorphic_shapes_error(self):\n    msg_re = 'syntax error in symbolic shape'\n    with self.assertRaisesRegex(ValueError, msg_re):\n        check_shape_poly(self, jnp.sin, arg_descriptors=[RandArg((1, 3), _f32)], input_signature=[tf.TensorSpec([1, None])], polymorphic_shapes=[None])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_with_constraints(self):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('not supported')\n\n    def f_jax(x):\n        return lax.dynamic_slice_in_dim(x, 0, 8, 0)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((16,), _i32)], polymorphic_shapes=['a'], polymorphic_constraints=['a >= 8'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_arg_avals_errors(self):\n    \"\"\"Test error reporting for shape polymorphism.\"\"\"\n\n    def conv_and_run(*, arg_shape: core.Shape, polymorphic_shape: str):\n        arg = np.arange(math.prod(arg_shape), dtype=np.float32).reshape(arg_shape)\n        check_shape_poly(self, lambda x: x, arg_descriptors=[arg], polymorphic_shapes=[polymorphic_shape])\n    with self.assertRaisesRegex(ValueError, re.escape('polymorphic shape spec should be')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=5.0)\n    with self.assertRaisesRegex(ValueError, re.escape('pytree structure error: different types')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=['a list'])\n    with self.assertRaisesRegex(ValueError, re.escape('pytree structure error: different types')):\n        conv_and_run(arg_shape=(2,), polymorphic_shape=('a tuple',))\n    with self.assertRaisesRegex(ValueError, \"Cannot solve for values of dimension variables {'b'}\"):\n        conv_and_run(arg_shape=(4, 36, 3), polymorphic_shape='b * b, b * d * d, d')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, \"Division had remainder 2 when computing the value of 'b'\"):\n        conv_and_run(arg_shape=(5, 36), polymorphic_shape='3 * b, ...')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, \"Expected value >= 1 for dimension variable 'b'\"):\n        conv_and_run(arg_shape=(10, 3), polymorphic_shape='3 * b + 10, ...')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, \"Expected value >= 1 for dimension variable 'b'\"):\n        conv_and_run(arg_shape=(7, 3), polymorphic_shape='3 * b + 10, ...')\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, re.escape(\"Found inconsistency between dimension size args[0].shape[1] (= 3) and the specification 'a' (= 2)\")):\n        conv_and_run(arg_shape=(2, 3), polymorphic_shape='(a, a)')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(testcase_name=lambda kw: kw['shape'], kwargs=[dict(shape=(8, 2, 9), poly_spec='(a + 2*b, a, a + b + c)'), dict(shape=(2, 2, 6), poly_spec='(a + 2*b, a, a + b + c)', expect_error=\"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'b'. Using the following polymorphic shapes specifications: args[0].shape = (2*b + a, a, c + b + a). Obtained dimension variables: 'a' = 2 from specification 'a' for dimension args[0].shape[1] (= 2), 'b' = 0 from specification '2*b + a' for dimension args[0].shape[0] (= 2), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\"), dict(shape=(3, 2, 6), poly_spec='(a + 2*b, a, a + b + c)', expect_error=\"Input shapes do not match the polymorphic shapes specification. Division had remainder 1 when computing the value of 'b'. Using the following polymorphic shapes specifications: args[0].shape = (2*b + a, a, c + b + a). Obtained dimension variables: 'a' = 2 from specification 'a' for dimension args[0].shape[1] (= 2), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\"), dict(shape=(8, 2, 6), poly_spec='(a + 2*b, a, a + b)', expect_error=\"Input shapes do not match the polymorphic shapes specification. Found inconsistency between dimension size args[0].shape[0] (= 8) and the specification '2*b + a' (= 10). Using the following polymorphic shapes specifications: args[0].shape = (2*b + a, a, b + a). Obtained dimension variables: 'a' = 2 from specification 'a' for dimension args[0].shape[1] (= 2), 'b' = 4 from specification 'b + a' for dimension args[0].shape[2] (= 6), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\"), dict(shape=(7, 2, 36), poly_spec='(2 * a + b, a, c * c)', expect_error=\"Cannot solve for values of dimension variables {'c'}. We can only solve linear uni-variate constraints. Using the following polymorphic shapes specifications: args[0].shape = (b + 2*a, a, c^2). Unprocessed specifications: 'c^2' for dimension size args[0].shape[2]. Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#dimension-variables-must-be-solvable-from-the-input-shapes for more details.\")])\ndef test_shape_constraints_errors(self, *, shape, poly_spec: str, expect_error: str | None=None):\n\n    def f_jax(x):\n        return 0.0\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    with contextlib.ExitStack() as stack:\n        if expect_error is not None:\n            stack.push(self.assertRaisesRegex(Exception, re.escape(expect_error)))\n        _ = check_shape_poly(self, f_jax, arg_descriptors=[x], polymorphic_shapes=[poly_spec])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_pytree(self):\n    \"\"\"Arguments and polymorphic_shapes are pytrees.\"\"\"\n\n    def add_all_jax(x_pair_of_list, y_dict):\n        x_list_0, x_list_1 = x_pair_of_list\n        return functools.reduce(op.add, x_list_0 + x_list_1 + [y_dict['a'], y_dict['b']])\n    input_signature = [([tf.TensorSpec([None]), tf.TensorSpec([None])], [tf.TensorSpec([None])]), dict(a=tf.TensorSpec([None]), b=tf.TensorSpec([None]))]\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[(['v', 'v'], ['v']), dict(a='v', b='v')], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes='v', expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=['v', 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=input_signature, polymorphic_shapes=[('v', 'v'), 'v'], expected_output_signature=tf.TensorSpec([None]))\n    check_shape_poly(self, add_all_jax, skip_jax_run=True, input_signature=[([tf.TensorSpec([4]), tf.TensorSpec([4])], [tf.TensorSpec([4])]), dict(a=tf.TensorSpec([4]), b=tf.TensorSpec([4]))], polymorphic_shapes=((['(4,)', '(_,)'], ['4,']), dict(a='(_,)', b='(4,)')), expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_with_nested_jit(self):\n\n    def f_jax(x):\n        return jnp.sin(x) + jnp.arange(x.shape[1], dtype=x.dtype)\n    check_shape_poly(self, lambda x: x + jax.jit(f_jax)(x), arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['a, b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=str(polymorphic_shapes), polymorphic_shapes=polymorphic_shapes) for polymorphic_shapes in ['b1+6,b1+14,b2', '2*b1,4*b2,b1+b2+18', 'b1+2*b2,4*b2,b1*b1+16']])\ndef test_non_trivial_polynomials_spec(self, polymorphic_shapes='2*b1,4*b2,b1+b2+18'):\n    check_shape_poly(self, lambda x: 2 * x.shape[0] + 3 * x.shape[1] + 4 * x.shape[2], arg_descriptors=[RandArg((16, 24, 32), _f32)], polymorphic_shapes=[polymorphic_shapes])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_unused_args(self):\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((2, 3), _f32), RandArg((3,), _f32)], polymorphic_shapes=[None, 'b'])\n    check_shape_poly(self, lambda x_unused, y, z_unused, w: jnp.concatenate([y, w]), arg_descriptors=[RandArg((3,), _f32), RandArg((4,), _f32), RandArg((5,), _f32), RandArg((6,), _f32)], polymorphic_shapes=[None, 'b1', None, 'b2'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((3,), _f32), RandArg((3,), _f32)], polymorphic_shapes=['b', 'b'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((4,), _f32), RandArg((3,), _f32)], polymorphic_shapes=['b1', 'b2'])\n    check_shape_poly(self, lambda x_unused, y: y * 2.0, arg_descriptors=[RandArg((3,), _f32), RandArg((9,), _f32)], polymorphic_shapes=['b1', 'b1 * b1'])\n    check_shape_poly(self, lambda x_unused, y: y + x_unused.shape[0], arg_descriptors=[RandArg((3,), _f32), RandArg((9,), _f32)], polymorphic_shapes=['b1', 'b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_gradients_pytree(self):\n    \"\"\"Shape polymorphism with gradients and pytrees for inputs and outputs.\"\"\"\n\n    def f(x):\n        return dict(res=x['x'] * 2.0)\n    check_shape_poly(self, f, skip_jax_run=True, input_signature=[dict(x=tf.TensorSpec([None, 3, 4]))], polymorphic_shapes=[dict(x='b, 3, 4')])\n    f_tf = jax2tf.convert(f, polymorphic_shapes=[dict(x='b, 3, 4')])\n    x = dict(x=np.ones((2, 3, 4), dtype=np.float32))\n    xv = tf.Variable(x['x'], dtype=np.float32)\n\n    def tf_value_and_grad(xv):\n        with tf.GradientTape() as tape:\n            tape.watch(xv)\n            res_tf = f_tf(dict(x=xv))\n            res_tf_grad = tape.gradient(res_tf, xv)\n            return (res_tf, dict(grad=res_tf_grad))\n    res_tf, res_tf_grad = tf_value_and_grad(xv)\n    tf_grad = tf.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[0]['res']))\n    self.assertEqual((None, 3, 4), tuple(tf_grad.output_shapes[1]['grad']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_cond(self):\n\n    def f(x, y):\n        return lax.cond(jnp.sum(x) > 0.0, lambda _: x + y, lambda _: jnp.zeros_like(x), operand=None)\n    x = np.ones((2, 3))\n    y = np.ones((3,))\n    res_jax = f(x, y)\n    self.assertAllClose(res_jax, check_shape_poly(self, f, arg_descriptors=[x, y], polymorphic_shapes=['(b, h)', 'h']))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_while(self):\n\n    def f(x):\n        return lax.while_loop(lambda x_iter: x_iter[1] < 5, lambda x_iter: (x_iter[0] + jnp.arange(x_iter[0].shape[0], dtype=np.float32), x_iter[1] + 1), (x, 0))\n    x = np.ones((3,), dtype=np.float32)\n    res_tf = check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)'])\n    self.assertAllClose(f(x), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_prng(self):\n    with config.enable_custom_prng(True):\n\n        def f_jax(x):\n            key = random.PRNGKey(123)\n            broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n            gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n            slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n            slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n            upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n            _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n            xs = broadcast_keys\n            counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n            def f_vmap_jax(counts, xs):\n\n                def inner(count, x):\n                    return lax.fori_loop(0, count, lambda _, acc: acc, x)\n                return jax.vmap(inner)(counts, xs)\n            _ = f_vmap_jax(counts, xs)\n            return x\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b1, b2'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_dynamic_shapes(self):\n\n    def f(x):\n        return jnp.sum(x, axis=0) * x.shape[0]\n    x = np.arange(3.0)\n    self.assertAllClose(9.0, check_shape_poly(self, f, arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    self.assertAllClose(9.0, check_shape_poly(self, jax.jit(f), arg_descriptors=[x], polymorphic_shapes=['(b,)']))\n    res_primal, res_tangent = check_shape_poly(self, lambda x, xt: jax.jvp(f, (x,), (xt,)), arg_descriptors=[x, np.array([0.1, 0.2, 0.3])], polymorphic_shapes=['b', 'b'])\n    self.assertAllClose((9.0, 1.8), (res_primal, res_tangent))\n    self.assertAllClose(np.array([3.0, 3.0, 3.0]), check_shape_poly(self, jax.grad(f), arg_descriptors=[x], polymorphic_shapes=['b']))\n    xv = np.arange(24.0).reshape((2, 3, 4))\n    res_vmap = jax.vmap(f, in_axes=1)(xv)\n    res_iter = jnp.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])\n    self.assertAllClose(res_iter, res_vmap)\n    res_vmap_tf = check_shape_poly(self, jax.vmap(f, in_axes=1), arg_descriptors=[xv], polymorphic_shapes=['b1, b2, ...'])\n    self.assertAllClose(res_iter, res_vmap_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=op_name, op=op) for op, op_name in [(jnp.array, 'array'), (jnp.sin, 'sin'), (lambda x: x, 'id'), (core.dimension_as_value, 'dimension_as_value')]])\ndef test_poly_unary_op(self, *, op=jnp.array):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        return (op(poly), x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), _f32)], polymorphic_shapes=['b'], expected_output_signature=(tf.TensorSpec([]), tf.TensorSpec((None,), _f32)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_{op.__name__}_other={other}:{type(other)}{('_other_jnp_array' if other_jnp_array else '')}{('_swap' if swap else '')}', op=op, other=other, other_jnp_array=other_jnp_array, swap=swap) for op in [op.add, op.mul, op.sub, op.mod, op.floordiv, op.truediv] for other in [2, np.int32(2), 2.0, np.float32(2), np.array(2, dtype=np.int32), np.arange(1, 5, dtype=np.int32), np.array(2.0, dtype=np.float32), np.arange(1.0, 7.0, dtype=np.float32)] for other_jnp_array in ([True, False] if np.shape(other) == (7,) else [False]) for swap in [False, True]])\ndef test_poly_binary_op(self, *, op=op.add, other=np.arange(2, dtype=np.int32), other_jnp_array=False, swap=True):\n\n    def f_jax(x):\n        poly = 2 * x.shape[0]\n        other_wrapped = jnp.array(other) if other_jnp_array else other\n        ops = (poly, other_wrapped) if not swap else (other_wrapped, poly)\n        res = op(*ops)\n        try:\n            op.index(other)\n            other_isint = True\n        except Exception:\n            other_isint = False\n        if hasattr(poly, 'dimension_as_value') and other_isint and (op.__name__ != 'truediv'):\n            self.assertTrue(isinstance(res, int) or hasattr(res, 'dimension_as_value'))\n        if config.enable_x64.value:\n            return (lax.convert_element_type(res, np.float32), x)\n        return (res, x)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), np.int32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_mean0(self):\n\n    def f_jax(x):\n        return jnp.sum(x, axis=0) / x.shape[0]\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b, _'], expected_output_signature=tf.TensorSpec([4]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_shape_as_array(self):\n\n    def f_jax(x):\n        return x + jnp.sum(jnp.array(x.shape)).astype(np.int32)\n    check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3, 4), _f32)], polymorphic_shapes=['b, _'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_dim_as_value_weak_type(self):\n\n    def f_jax(x):\n        d0 = jnp.array(x.shape[0])\n        if isinstance(d0, core.Tracer):\n            (self.assertTrue(d0.aval.weak_type), d0)\n        d1 = x.shape[0] + jnp.array(4)\n        if isinstance(d1, core.Tracer):\n            (self.assertTrue(d1.aval.weak_type), d1)\n        return d0 + np.array(5.0, dtype=np.float32) + d1 + x[0]\n    with config.numpy_dtype_promotion('strict'):\n        check_shape_poly(self, f_jax, arg_descriptors=[RandArg((3,), _f32)], polymorphic_shapes=['b'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  },
  {
    "test_code": "def test_vmap_while(self):\n\n    def cond_func(x):\n        return jnp.sum(x) >= 0.0\n\n    def body_func(x):\n        return x - 1.0\n\n    def f_jax(x):\n        return lax.while_loop(cond_func, body_func, x)\n    check_shape_poly(self, jax.vmap(f_jax), arg_descriptors=[RandArg((5, 3), _f32)], polymorphic_shapes=['b, ...'], expected_output_signature=tf.TensorSpec((None, 3), dtype=tf.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/shape_poly_test.py",
    "function": "def check_shape_poly(tst, f_jax: Callable, *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), skip_jax_run: bool=False, polymorphic_shapes: Sequence[str | None]=(), polymorphic_constraints: Sequence[str]=(), input_signature: Sequence[tf.TensorSpec] | None=None, expected_output_signature: tf.TensorSpec | None=None, expect_error=(None, None)) -> jax.Array | None:\n    h = PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, polymorphic_constraints=polymorphic_constraints, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)\n    return h.run_test(tst)"
  }
]