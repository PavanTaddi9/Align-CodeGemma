[
  {
    "test_code": "@parameterized.parameters(['', 'pmap'])\n@jtu.run_on_devices('cuda')\ndef test_cudnn_fusion(self, mode):\n    batch_size = 2\n    if mode == 'pmap' and jax.device_count() < batch_size:\n        raise SkipTest('pmap test requires 2 GPUs')\n\n    @cudnn_fusion\n    def comp1(x, y, z):\n        return jnp.float32(jax.lax.batch_matmul(jnp.bfloat16(x), y)) + z\n    k = jax.random.key(0)\n    s = (batch_size, 16, 16)\n    x = jnp.int8(jax.random.normal(k, shape=s))\n    y = jnp.bfloat16(jax.random.normal(k, shape=s))\n    z = jnp.float32(jax.random.normal(k, shape=s))\n    fn = jax.pmap(comp1) if mode == 'pmap' else comp1\n    jitted = jax.jit(comp1)\n    lowered = jitted.lower(x, y, z)\n    stablehlo = lowered.as_text('stablehlo')\n    self.assertIn('func.func private @comp1', stablehlo)\n    self.assertIn('__cudnn$fusion', stablehlo)\n    hlo = lowered.as_text('hlo')\n    self.assertIn('custom_call_target=\"__cudnn$fusion\"', hlo)\n    self.assertIn('called_computations=', hlo)\n    compiled = lowered.compile({'xla_gpu_cublas_fallback': False})\n    hlo_after_opt = compiled.as_text()\n    self.assertIn('kind=kCustom', hlo_after_opt)\n    self.assertIn('plan_id', hlo_after_opt)\n    self.assertAllClose(compiled(x, y, z), fn(x, y, z))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/cudnn_fusion_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.parameters(['', 'pmap'])\n@jtu.run_on_devices('cuda')\ndef test_cudnn_fusion(self, mode):\n    batch_size = 2\n    if mode == 'pmap' and jax.device_count() < batch_size:\n        raise SkipTest('pmap test requires 2 GPUs')\n\n    @cudnn_fusion\n    def comp1(x, y, z):\n        return jnp.float32(jax.lax.batch_matmul(jnp.bfloat16(x), y)) + z\n    k = jax.random.key(0)\n    s = (batch_size, 16, 16)\n    x = jnp.int8(jax.random.normal(k, shape=s))\n    y = jnp.bfloat16(jax.random.normal(k, shape=s))\n    z = jnp.float32(jax.random.normal(k, shape=s))\n    fn = jax.pmap(comp1) if mode == 'pmap' else comp1\n    jitted = jax.jit(comp1)\n    lowered = jitted.lower(x, y, z)\n    stablehlo = lowered.as_text('stablehlo')\n    self.assertIn('func.func private @comp1', stablehlo)\n    self.assertIn('__cudnn$fusion', stablehlo)\n    hlo = lowered.as_text('hlo')\n    self.assertIn('custom_call_target=\"__cudnn$fusion\"', hlo)\n    self.assertIn('called_computations=', hlo)\n    compiled = lowered.compile({'xla_gpu_cublas_fallback': False})\n    hlo_after_opt = compiled.as_text()\n    self.assertIn('kind=kCustom', hlo_after_opt)\n    self.assertIn('plan_id', hlo_after_opt)\n    self.assertAllClose(compiled(x, y, z), fn(x, y, z))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/cudnn_fusion_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.parameters(['', 'pmap'])\n@jtu.run_on_devices('cuda')\ndef test_cudnn_fusion(self, mode):\n    batch_size = 2\n    if mode == 'pmap' and jax.device_count() < batch_size:\n        raise SkipTest('pmap test requires 2 GPUs')\n\n    @cudnn_fusion\n    def comp1(x, y, z):\n        return jnp.float32(jax.lax.batch_matmul(jnp.bfloat16(x), y)) + z\n    k = jax.random.key(0)\n    s = (batch_size, 16, 16)\n    x = jnp.int8(jax.random.normal(k, shape=s))\n    y = jnp.bfloat16(jax.random.normal(k, shape=s))\n    z = jnp.float32(jax.random.normal(k, shape=s))\n    fn = jax.pmap(comp1) if mode == 'pmap' else comp1\n    jitted = jax.jit(comp1)\n    lowered = jitted.lower(x, y, z)\n    stablehlo = lowered.as_text('stablehlo')\n    self.assertIn('func.func private @comp1', stablehlo)\n    self.assertIn('__cudnn$fusion', stablehlo)\n    hlo = lowered.as_text('hlo')\n    self.assertIn('custom_call_target=\"__cudnn$fusion\"', hlo)\n    self.assertIn('called_computations=', hlo)\n    compiled = lowered.compile({'xla_gpu_cublas_fallback': False})\n    hlo_after_opt = compiled.as_text()\n    self.assertIn('kind=kCustom', hlo_after_opt)\n    self.assertIn('plan_id', hlo_after_opt)\n    self.assertAllClose(compiled(x, y, z), fn(x, y, z))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/cudnn_fusion_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  }
]