[
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_experimental_get_compiler_ir_design_doc(self):\n    x = np.zeros((10,), dtype=np.int32)\n\n    def fun_tf(x):\n        begin = 0\n        return x[begin:5]\n    hlo = tf.function(fun_tf, jit_compile=True, autograph=False).experimental_get_compiler_ir(x)()\n    self.assertIn('(arg0.1: s32[10]) -> s32[5]', hlo)\n    x = np.zeros((10,), dtype=np.int32)\n    x = np.zeros((10,), dtype=np.int32)\n\n    def fun_tf(x):\n        begin = tf.shape(x)[0] - 2\n        return x[begin:]\n    hlo = tf.function(fun_tf, jit_compile=True, autograph=False).experimental_get_compiler_ir(x)()\n    self.assertIn('(arg0.1: s32[10]) -> s32[2]', hlo)\n    outer_var = tf.Variable(np.array([3.0], dtype=np.float32))\n    x = np.array([2.0, 3.0, 4.0], dtype=np.float32)\n\n    def fun_tf(x):\n        return x * tf.broadcast_to(outer_var, x.shape) + 1.0\n    hlo = tf.function(fun_tf, jit_compile=True, autograph=False).experimental_get_compiler_ir(x)()\n    self.assertIn('(arg0.1: f32[3], arg1.2: f32[1]) -> f32[3]', hlo)\n    outer_ct = np.array([3.0], dtype=np.float32)\n    x = np.array([2.0, 3.0, 4.0], dtype=np.float32)\n\n    def fun_tf(x):\n        return x * tf.broadcast_to(outer_ct, x.shape) + 1.0\n    hlo = tf.function(fun_tf, jit_compile=True, autograph=False).experimental_get_compiler_ir(x)()\n    self.assertIn('(arg0.1: f32[3]) -> f32[3]', hlo)\n    x = np.array([2.0, 3.0, 4.0], dtype=np.float32)\n\n    def fun_tf_outer(x):\n        x_const = tf.constant(0, shape=x.shape, dtype=x.dtype)\n        _ = tf.function(tf.math.sin, jit_compile=True, autograph=False).experimental_get_compiler_ir(x_const)()\n\n    def fun_tf_outer_2(x):\n        _ = tf.function(tf.math.sin, jit_compile=True).get_concrete_function(tf.TensorSpec(x.shape, x.dtype))\n        return x\n    _ = tf.function(fun_tf_outer_2)(x)\n    _ = tf.function(fun_tf_outer_2, jit_compile=True)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_output_shape_dtype_none(self):\n    x = jnp.zeros(10, dtype=jnp.float32)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def fun_tf(x):\n        return\n    fun_jax_1 = jax2tf.call_tf(fun_tf, output_shape_dtype=None)\n    fun_jax_2 = jax2tf.call_tf(fun_tf)\n    self.assertIsNone(fun_jax_1(x))\n    self.assertIsNone(fun_jax_2(x))\n    fun_jax_3 = jax2tf.call_tf(fun_tf, output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        _ = fun_jax_3(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_output_shape_dtype_not_none(self):\n    x = jnp.zeros(10, dtype=jnp.float32)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def fun_tf(x):\n        return x\n    fun_jax_1 = jax2tf.call_tf(fun_tf, output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32))\n    fun_jax_2 = jax2tf.call_tf(fun_tf)\n    self.assertAllClose(fun_jax_1(x), fun_jax_2(x))\n    fun_jax_3 = jax2tf.call_tf(fun_tf, output_shape_dtype=None)\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        _ = fun_jax_3(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_type={type_.__name__}', 'type_': type_} for type_ in dlpack.SUPPORTED_DTYPES))\ndef test_avoid_copy_between_gpu_and_cpu(self, type_):\n    try:\n        gpu_devices = jax.devices('gpu')\n    except RuntimeError:\n        gpu_devices = []\n    if not gpu_devices:\n        raise unittest.SkipTest('Test requires a GPU device.')\n\n    def tf_fun(x):\n        if type_ == jnp.bool_:\n            return tf.math.logical_or(x, True)\n        else:\n            return x + 1\n    jax_array_on_gpu = jnp.zeros([1], type_, device=gpu_devices[0])\n\n    @contextlib.contextmanager\n    def _transfer_guard(guard_level):\n        with contextlib.ExitStack() as stack:\n            stack.enter_context(jax.transfer_guard_device_to_device(guard_level))\n            stack.enter_context(jax.transfer_guard_device_to_host(guard_level))\n            if type_ != jnp.int32:\n                stack.enter_context(jax.transfer_guard_host_to_device(guard_level))\n            yield\n    with _transfer_guard('disallow_explicit'):\n        jax2tf.call_tf(tf_fun)(jax_array_on_gpu)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g():\n    return jnp.zeros(n) + x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g():\n    return jax.lax.cond(True, lambda: data[0], lambda: data[1])"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g():\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(a, b):\n    c = jnp.zeros_like(a)\n    _, b, c, _ = for_impl(5, body2, (a, b, c, 0))\n    return (b, c)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g():\n    _, _, attr_tangents = attrs.jvp(f, (), (), [(thing, 'x', 1.0)])\n    (thing_, attr_, tangent_), = attr_tangents\n    self.assertIs(thing, thing_)\n    self.assertEqual(attr_, 'x')\n    return (jax_getattr(thing, 'x'), tangent_)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@pjit\ndef g(y):\n    return jnp.sin(y)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.remat\ndef g(x):\n    jax.jit(lambda: 0 if jnp.add(1, 1) else 0)()\n    return lax.sin(x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    if x > 0.0:\n        return x * 2\n    else:\n        return x + 2"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.pmap\ndef g(z):\n    return f(z, z + 77)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    z = x * 2\n    return shard_alike(x, z)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef g(x):\n    return x"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_grad_int_argument_unused(self):\n    batch_size = 5\n    inputs = np.ones((batch_size, 3), dtype=np.float32)\n    rng = np.array([1, 2], dtype=np.uint32)\n    params = np.float32(0.5)\n\n    def jax_model(params, rng, inputs):\n        return jnp.ones([batch_size, 2], dtype=jnp.float32)\n    tf_model = jax2tf.convert(jax_model, with_gradient=True)\n\n    def _loss_fn(inference_fn, params, rng, inputs):\n        prediction = inference_fn(params, rng, inputs)\n        return jnp.mean(prediction)\n    jax_loss_fn = partial(_loss_fn, jax_model)\n    jax_grad = jax.grad(jax_loss_fn)(params, rng, inputs)\n    paramsv = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        tf_prediction = tf_model(paramsv, rng, inputs)\n        tf_loss = tf.reduce_mean(tf_prediction)\n        tf_grad = tape.gradient(tf_loss, paramsv)\n    self.assertAllClose(jax_grad, tf_grad.numpy())\n    call_tf_loss_fn = partial(_loss_fn, jax2tf.call_tf(tf_model))\n    call_tf_grad = jax.grad(call_tf_loss_fn)(params, rng, inputs)\n    self.assertAllClose(jax_grad, call_tf_grad)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly(self, with_jit=False):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(tf.math.sin, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, x.dtype))(x)\n        z = jnp.cos(y)\n        w = jax2tf.call_tf(lambda z: tf.concat([z, z], axis=0), output_shape_dtype=jax.ShapeDtypeStruct((2 * z.shape[0],), z.dtype))(z)\n        assert w.shape[0] == 2 * x.shape[0]\n        return w\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert w.shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_pytree_result(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(lambda x: (x, tf.concat([x, x], axis=0)), output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct((2 * x.shape[0],), x.dtype)))(x)\n        assert y[0].shape[0] == x.shape[0]\n        assert y[1].shape[0] == 2 * x.shape[0]\n        return y\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert y[0].shape[0] == x.shape[0]",
      "assert y[1].shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_no_output_shape_dtype(self, with_jit=True):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin)(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_mismatch_output_shape_dtype_tree(self, with_jit=False):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin, output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype)))(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(with_jit=with_jit, kind=kind) for with_jit in [True, False] for kind in ['bad_rank', 'bad_dim', 'bad_dtype', 'bad_dtype_x64']))\ndef test_shape_poly_error_mismatch_output_shape_dtype(self, with_jit=False, kind='bad_rank'):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n    if kind == 'bad_rank':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct((), x.dtype))(x)\n    elif kind == 'bad_dim':\n\n        def fun_jax(x):\n            bad_shape = (5 + x.shape[0],)\n            y = jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(bad_shape, x.dtype))(x)\n            return y + jnp.ones(bad_shape, dtype=x.dtype)\n    elif kind == 'bad_dtype':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.int32))(x)\n    elif kind == 'bad_dtype_x64':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x * np.float64(3.0), output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.float64))(x)\n    else:\n        assert False\n    expect_ex = ValueError\n    expect_error = 'The shapes or dtypes returned by the TensorFlow function do not match the declared output_shape_dtype'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)\n    if kind == 'bad_dim' and with_jit:\n        expect_error = 'Dimensions must be equal, but are 4 and 9 for .* AddV2'\n    if kind == 'bad_dim' and jax.config.jax2tf_default_native_serialization:\n        expect_error = 'Error compiling TensorFlow function'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_control_flow(self, with_jit=True):\n\n    def times_5_tf(x):\n        c = lambda i, acc: tf.less(i, 5)\n        b = lambda i, acc: (tf.add(i, 1), tf.add(acc, x))\n        _, acc = tf.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])\n        return acc\n\n    def fun_jax(x):\n\n        def body(_, acc):\n            return jax2tf.call_tf(times_5_tf)(acc)\n        return lax.fori_loop(0, 3, body, x)\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(np.float32(x * 5 * 5 * 5), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "def test_pmap(self):\n    logging.info('Running test_pmap on %s devices', jax.local_device_count())\n\n    def plus_2_tf(x):\n        return tf.math.add(2.0, x)\n\n    def fun_jax(x):\n        return np.float32(3.0) * jax2tf.call_tf(plus_2_tf)(x)\n    x = np.arange(jax.local_device_count(), dtype=np.float32)\n    res = jax.pmap(fun_jax)(x)\n    self.assertAllClose(np.float32(3.0 * (x + 2)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    if x > 0.0:\n        return x * 2\n    else:\n        return x + 2"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('x', 'y'), out_specs=out_spec)\ndef g(x):\n    result = lax.psum(x, axis_name=reduce_along)\n\n    def check_rep(result):\n        self.assertEqual(jax.experimental.shard_map.get_replication(result), set(reduce_along))\n        return result\n    result = check_rep(result)\n    result = jax.vmap(check_rep)(result)\n    return result"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return x"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(x):\n    return jax.pure_callback(lambda x: x, x, x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_arg(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_arg(self, with_jit=True):\n    x = np.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_devicearray_arg(self, with_jit=False):\n    x = jnp.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)\n    x = jnp.array(3.0, dtype=jnp.bfloat16)\n    res = jax2tf.call_tf(lambda x: x)(x)\n    self.assertAllClose(x, res)\n    with self.assertRaises(AssertionError):\n        self.assertTrue(np.shares_memory(x, res))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_input(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 5.0\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = jnp.sin(x)\n    self.assertAllClose(res_call_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_module_documentation(self):\n\n    def cos_tf(x):\n        return tf.math.cos(x)\n\n    def cos_tf_sin_jax(x):\n        return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))\n    x = np.float32(1.0)\n    cos_tf_sin_jax(x)\n    jax.jit(cos_tf_sin_jax)(x)\n    jax.grad(cos_tf_sin_jax)(x)\n    logging.info(jax.make_jaxpr(cos_tf_sin_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(a, b):\n    an, ai = a\n    bn, bi = b\n    which = an >= bn\n    return (jnp.where(which, an, bn), jnp.where(which, ai, bi))"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.profiler.annotate_function, name='aname')\ndef g(x):\n    return x + 2"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, donate_argnums=0, out_shardings=Layout(DLL.AUTO))\ndef g(x):\n    return x * 2"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g():\n    x = x_ref[...] * y_ref[...]\n    y_ref[...] = x * 2\n    x_ref[...] = y_ref[...] + x_ref[...]"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(ys, _):\n    y, _ = ys\n    y = checkpoint_name(jnp.sin(y), 'y')\n    z = checkpoint_name(jnp.sin(y), 'z')\n    z = jax.lax.with_sharding_constraint(z, s)\n    z = z.T\n    w = checkpoint_name(jnp.sin(z), 'w')\n    return ((w.T, jnp.sum(w)), None)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnums=[1])\ndef g(x, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_effectful(self):\n    x = np.ones((3,), dtype=np.float32)\n    lower_effect = jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=True)).lower(x)\n    self.assertNotEmpty(lower_effect._lowering.compile_args['unordered_effects'])\n    lower_no_effect = jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=False)).lower(x)\n    self.assertEmpty(lower_no_effect._lowering.compile_args['unordered_effects'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_call_tf_graph(self):\n    const = tf.Variable(0.0, dtype=tf.float32)\n\n    @tf.function(jit_compile=True)\n    def tf_func_1(x):\n        return x * x + const\n\n    @tf.function\n    def tf_func_2(x, y):\n        return tf_func_1(x) + y\n\n    @tf.function\n    def tf_func_3(x, y, z):\n        return (tf_func_2(x, y) + z, z)\n    x = jnp.array(3.0, dtype=jnp.float32)\n    y = jnp.array(3.0, dtype=jnp.float32)\n    z = jnp.array(5.0, dtype=jnp.float32)\n    f_jax = jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=False))\n    stablehlo_module = f_jax.lower(x, y, z).compiler_ir('stablehlo')\n    self.assertNotIn('stablehlo.custom_call', str(stablehlo_module))\n    f_jax = jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=True))\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = f_jax.lower(x, y, z).compiler_ir('stablehlo')\n        self.assertIn('stablehlo.custom_call', str(stablehlo_module))\n        called_index_list = []\n\n        def _extract_info(op):\n            if op.operation.name != 'stablehlo.custom_call':\n                return\n            tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n            called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n            called_index_list.append(called_index)\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jit\ndef g(z):\n    return self.pmap(lambda x: x[jnp.newaxis] * y)(z)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(y):\n    return y"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(weights, x, h_0, c_0):\n    W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n    y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n    seq_length_mask = jnp.tile(jnp.arange(seq_len, dtype=jnp.int32)[None], [batch_size, 1]) < seq_lengths[:, None]\n    loss = jnp.sum(jnp.where(seq_length_mask[..., None], y_ref, 0.0))\n    return (loss, (y_ref, h_n_ref, c_n_ref))"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_control_flow(self, with_jit=True):\n\n    def times_5_tf(x):\n        c = lambda i, acc: tf.less(i, 5)\n        b = lambda i, acc: (tf.add(i, 1), tf.add(acc, x))\n        _, acc = tf.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])\n        return acc\n\n    def fun_jax(x):\n\n        def body(_, acc):\n            return jax2tf.call_tf(times_5_tf)(acc)\n        return lax.fori_loop(0, 3, body, x)\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(np.float32(x * 5 * 5 * 5), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def add(self, x: jax.Array) -> jax.Array:\n    self.value += np.asarray(x)\n    return jax.device_put(self.value, x.sharding)"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def add(self, x: jax.Array) -> jax.Array:\n    self.value += np.asarray(x)\n    return jax.device_put(self.value, x.sharding)"
  },
  {
    "test_code": "def test_pmap(self):\n    logging.info('Running test_pmap on %s devices', jax.local_device_count())\n\n    def plus_2_tf(x):\n        return tf.math.add(2.0, x)\n\n    def fun_jax(x):\n        return np.float32(3.0) * jax2tf.call_tf(plus_2_tf)(x)\n    x = np.arange(jax.local_device_count(), dtype=np.float32)\n    res = jax.pmap(fun_jax)(x)\n    self.assertAllClose(np.float32(3.0 * (x + 2)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def add(self, x: jax.Array) -> jax.Array:\n    self.value += np.asarray(x)\n    return jax.device_put(self.value, x.sharding)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return x + 4"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return callback_p.bind(x, callback=log_value, effect=log_effect, out_avals=[])"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(x):\n    branch = jax.named_call(lambda x: x)\n    out = jax.lax.cond(True, branch, branch, x)\n    return out"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_arg(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_arg(self, with_jit=True):\n    x = np.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_devicearray_arg(self, with_jit=False):\n    x = jnp.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)\n    x = jnp.array(3.0, dtype=jnp.bfloat16)\n    res = jax2tf.call_tf(lambda x: x)(x)\n    self.assertAllClose(x, res)\n    with self.assertRaises(AssertionError):\n        self.assertTrue(np.shares_memory(x, res))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_input(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 5.0\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = jnp.sin(x)\n    self.assertAllClose(res_call_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_module_documentation(self):\n\n    def cos_tf(x):\n        return tf.math.cos(x)\n\n    def cos_tf_sin_jax(x):\n        return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))\n    x = np.float32(1.0)\n    cos_tf_sin_jax(x)\n    jax.jit(cos_tf_sin_jax)(x)\n    jax.grad(cos_tf_sin_jax)(x)\n    logging.info(jax.make_jaxpr(cos_tf_sin_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(x):\n    y0_arr = jnp.array([[x, 0.1], [x, 0.2]])\n    t = jnp.array([0.0, 5.0])\n    y = jax.vmap(lambda y0: odeint(dx_dt, y0, t))(y0_arr)\n    return y[:, -1].sum()"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef g(x):\n    debug_print('hello: {x}', x=x)\n    return x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(key1, key2):\n    assert_unconsumed(key1)\n    assert_unconsumed(key2)\n    return jax.random.bits(key1)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef g(y):\n    return x * y"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_control_flow(self, with_jit=True):\n\n    def times_5_tf(x):\n        c = lambda i, acc: tf.less(i, 5)\n        b = lambda i, acc: (tf.add(i, 1), tf.add(acc, x))\n        _, acc = tf.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])\n        return acc\n\n    def fun_jax(x):\n\n        def body(_, acc):\n            return jax2tf.call_tf(times_5_tf)(acc)\n        return lax.fori_loop(0, 3, body, x)\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(np.float32(x * 5 * 5 * 5), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef add(x):\n    return x * 2"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef add(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pmap(self):\n    logging.info('Running test_pmap on %s devices', jax.local_device_count())\n\n    def plus_2_tf(x):\n        return tf.math.add(2.0, x)\n\n    def fun_jax(x):\n        return np.float32(3.0) * jax2tf.call_tf(plus_2_tf)(x)\n    x = np.arange(jax.local_device_count(), dtype=np.float32)\n    res = jax.pmap(fun_jax)(x)\n    self.assertAllClose(np.float32(3.0 * (x + 2)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef add(x):\n    return x * 2"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_arg(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_arg(self, with_jit=True):\n    x = np.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_devicearray_arg(self, with_jit=False):\n    x = jnp.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)\n    x = jnp.array(3.0, dtype=jnp.bfloat16)\n    res = jax2tf.call_tf(lambda x: x)(x)\n    self.assertAllClose(x, res)\n    with self.assertRaises(AssertionError):\n        self.assertTrue(np.shares_memory(x, res))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_input(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 5.0\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = jnp.sin(x)\n    self.assertAllClose(res_call_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_module_documentation(self):\n\n    def cos_tf(x):\n        return tf.math.cos(x)\n\n    def cos_tf_sin_jax(x):\n        return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))\n    x = np.float32(1.0)\n    cos_tf_sin_jax(x)\n    jax.jit(cos_tf_sin_jax)(x)\n    jax.grad(cos_tf_sin_jax)(x)\n    logging.info(jax.make_jaxpr(cos_tf_sin_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_grad_with_float0_result(self):\n\n    def f_jax(x, y):\n        return (2 * x, 2 * x + y * y)\n\n    def f_tf(x, y):\n        return (2 * x, tf.cast(2 * x, dtype=y.dtype) + y * y)\n\n    def wrapper(functional, x, y):\n        return jnp.sum(2.0 * functional(3 * x, 4.0 * y)[1])\n    grad_g = jax.grad(partial(wrapper, f_jax), allow_int=True, argnums=(0, 1))\n    grad_g_call_tf = jax.grad(partial(wrapper, jax2tf.call_tf(f_tf)), allow_int=True, argnums=(0, 1))\n    x = np.int32(2)\n    y = np.float32(3.0)\n    g_jax = grad_g(x, y)\n    g_call_tf = grad_g_call_tf(x, y)\n    self.assertEqual(g_jax[0].dtype, dtypes.float0)\n    self.assertEqual(g_call_tf[0].dtype, dtypes.float0)\n    self.assertAllClose(g_jax[1], g_call_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call(f, *args):\n    f = jax.custom_jvp(f)\n    f.defjvp(lambda primals, tangents: (f(*primals), sum(tangents)))\n    return f(*args)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call(f, *args):\n    f = jax.custom_jvp(f)\n    f.defjvp(lambda primals, tangents: (f(*primals), sum(tangents)))\n    return f(*args)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(z):\n    with set_xla_metadata(c='d'):\n        return z ** 2 + 1"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(eps):\n    x = jnp.array(1.0)\n    return jax.grad(f)(x, eps)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return x * y"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(y, Xtree):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    return out"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def g(hiddens):\n    hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n    new_hiddens = hiddens_aug.copy()\n    diff = new_hiddens[:-1] - hiddens\n    diff = new_hiddens[:-1] - hiddens\n    out = jnp.trace(jnp.conj(diff).T @ diff).real\n    return jnp.array(out, dtype=jnp.complex64)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return f(x * 2)"
  },
  {
    "test_code": "def test_effectful(self):\n    x = np.ones((3,), dtype=np.float32)\n    lower_effect = jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=True)).lower(x)\n    self.assertNotEmpty(lower_effect._lowering.compile_args['unordered_effects'])\n    lower_no_effect = jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=False)).lower(x)\n    self.assertEmpty(lower_no_effect._lowering.compile_args['unordered_effects'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_call_tf_graph(self):\n    const = tf.Variable(0.0, dtype=tf.float32)\n\n    @tf.function(jit_compile=True)\n    def tf_func_1(x):\n        return x * x + const\n\n    @tf.function\n    def tf_func_2(x, y):\n        return tf_func_1(x) + y\n\n    @tf.function\n    def tf_func_3(x, y, z):\n        return (tf_func_2(x, y) + z, z)\n    x = jnp.array(3.0, dtype=jnp.float32)\n    y = jnp.array(3.0, dtype=jnp.float32)\n    z = jnp.array(5.0, dtype=jnp.float32)\n    f_jax = jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=False))\n    stablehlo_module = f_jax.lower(x, y, z).compiler_ir('stablehlo')\n    self.assertNotIn('stablehlo.custom_call', str(stablehlo_module))\n    f_jax = jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=True))\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = f_jax.lower(x, y, z).compiler_ir('stablehlo')\n        self.assertIn('stablehlo.custom_call', str(stablehlo_module))\n        called_index_list = []\n\n        def _extract_info(op):\n            if op.operation.name != 'stablehlo.custom_call':\n                return\n            tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n            called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n            called_index_list.append(called_index)\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape=(128, 128, 128)):\n    m, l = x.shape\n    l2, n = y.shape\n    assert l2 == l\n    block_m, block_n, block_l = block_shape\n    assert l % block_l == 0, f'l={l!r}, block_l={block_l!r}'\n    assert m % block_m == 0, f'm={m!r}, block_m={block_m!r}'\n    assert n % block_n == 0, f'n={n!r}, block_n={block_n!r}'\n    grid = (m // block_m, n // block_n, l // block_l)\n    fused_matmul = pl.pallas_call(functools.partial(matmul_kernel), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((block_m, block_l), lambda i, j, k: (i, k)), pl.BlockSpec((block_l, block_n), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((block_m, block_n), lambda i, j, k: (i, j)), grid=grid, interpret=jtu.test_device_matches(['cpu']))\n    return fused_matmul(x, y)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_grad_int_argument_unused(self):\n    batch_size = 5\n    inputs = np.ones((batch_size, 3), dtype=np.float32)\n    rng = np.array([1, 2], dtype=np.uint32)\n    params = np.float32(0.5)\n\n    def jax_model(params, rng, inputs):\n        return jnp.ones([batch_size, 2], dtype=jnp.float32)\n    tf_model = jax2tf.convert(jax_model, with_gradient=True)\n\n    def _loss_fn(inference_fn, params, rng, inputs):\n        prediction = inference_fn(params, rng, inputs)\n        return jnp.mean(prediction)\n    jax_loss_fn = partial(_loss_fn, jax_model)\n    jax_grad = jax.grad(jax_loss_fn)(params, rng, inputs)\n    paramsv = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        tf_prediction = tf_model(paramsv, rng, inputs)\n        tf_loss = tf.reduce_mean(tf_prediction)\n        tf_grad = tape.gradient(tf_loss, paramsv)\n    self.assertAllClose(jax_grad, tf_grad.numpy())\n    call_tf_loss_fn = partial(_loss_fn, jax2tf.call_tf(tf_model))\n    call_tf_grad = jax.grad(call_tf_loss_fn)(params, rng, inputs)\n    self.assertAllClose(jax_grad, call_tf_grad)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly(self, with_jit=False):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(tf.math.sin, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, x.dtype))(x)\n        z = jnp.cos(y)\n        w = jax2tf.call_tf(lambda z: tf.concat([z, z], axis=0), output_shape_dtype=jax.ShapeDtypeStruct((2 * z.shape[0],), z.dtype))(z)\n        assert w.shape[0] == 2 * x.shape[0]\n        return w\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert w.shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_pytree_result(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(lambda x: (x, tf.concat([x, x], axis=0)), output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct((2 * x.shape[0],), x.dtype)))(x)\n        assert y[0].shape[0] == x.shape[0]\n        assert y[1].shape[0] == 2 * x.shape[0]\n        return y\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert y[0].shape[0] == x.shape[0]",
      "assert y[1].shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_no_output_shape_dtype(self, with_jit=True):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin)(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_mismatch_output_shape_dtype_tree(self, with_jit=False):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin, output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype)))(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(with_jit=with_jit, kind=kind) for with_jit in [True, False] for kind in ['bad_rank', 'bad_dim', 'bad_dtype', 'bad_dtype_x64']))\ndef test_shape_poly_error_mismatch_output_shape_dtype(self, with_jit=False, kind='bad_rank'):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n    if kind == 'bad_rank':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct((), x.dtype))(x)\n    elif kind == 'bad_dim':\n\n        def fun_jax(x):\n            bad_shape = (5 + x.shape[0],)\n            y = jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(bad_shape, x.dtype))(x)\n            return y + jnp.ones(bad_shape, dtype=x.dtype)\n    elif kind == 'bad_dtype':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.int32))(x)\n    elif kind == 'bad_dtype_x64':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x * np.float64(3.0), output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.float64))(x)\n    else:\n        assert False\n    expect_ex = ValueError\n    expect_error = 'The shapes or dtypes returned by the TensorFlow function do not match the declared output_shape_dtype'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)\n    if kind == 'bad_dim' and with_jit:\n        expect_error = 'Dimensions must be equal, but are 4 and 9 for .* AddV2'\n    if kind == 'bad_dim' and jax.config.jax2tf_default_native_serialization:\n        expect_error = 'Error compiling TensorFlow function'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_control_flow(self, with_jit=True):\n\n    def times_5_tf(x):\n        c = lambda i, acc: tf.less(i, 5)\n        b = lambda i, acc: (tf.add(i, 1), tf.add(acc, x))\n        _, acc = tf.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])\n        return acc\n\n    def fun_jax(x):\n\n        def body(_, acc):\n            return jax2tf.call_tf(times_5_tf)(acc)\n        return lax.fori_loop(0, 3, body, x)\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(np.float32(x * 5 * 5 * 5), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), intx), input_output_aliases={0: 0}, grid=())\ndef add(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), intx), input_output_aliases={0: 0}, grid=())\ndef add(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_pmap(self):\n    logging.info('Running test_pmap on %s devices', jax.local_device_count())\n\n    def plus_2_tf(x):\n        return tf.math.add(2.0, x)\n\n    def fun_jax(x):\n        return np.float32(3.0) * jax2tf.call_tf(plus_2_tf)(x)\n    x = np.arange(jax.local_device_count(), dtype=np.float32)\n    res = jax.pmap(fun_jax)(x)\n    self.assertAllClose(np.float32(3.0 * (x + 2)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), intx), input_output_aliases={0: 0}, grid=())\ndef add(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_arg(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_arg(self, with_jit=True):\n    x = np.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_devicearray_arg(self, with_jit=False):\n    x = jnp.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)\n    x = jnp.array(3.0, dtype=jnp.bfloat16)\n    res = jax2tf.call_tf(lambda x: x)(x)\n    self.assertAllClose(x, res)\n    with self.assertRaises(AssertionError):\n        self.assertTrue(np.shares_memory(x, res))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_input(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 5.0\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = jnp.sin(x)\n    self.assertAllClose(res_call_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_module_documentation(self):\n\n    def cos_tf(x):\n        return tf.math.cos(x)\n\n    def cos_tf_sin_jax(x):\n        return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))\n    x = np.float32(1.0)\n    cos_tf_sin_jax(x)\n    jax.jit(cos_tf_sin_jax)(x)\n    jax.grad(cos_tf_sin_jax)(x)\n    logging.info(jax.make_jaxpr(cos_tf_sin_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_grad_int_argument_unused(self):\n    batch_size = 5\n    inputs = np.ones((batch_size, 3), dtype=np.float32)\n    rng = np.array([1, 2], dtype=np.uint32)\n    params = np.float32(0.5)\n\n    def jax_model(params, rng, inputs):\n        return jnp.ones([batch_size, 2], dtype=jnp.float32)\n    tf_model = jax2tf.convert(jax_model, with_gradient=True)\n\n    def _loss_fn(inference_fn, params, rng, inputs):\n        prediction = inference_fn(params, rng, inputs)\n        return jnp.mean(prediction)\n    jax_loss_fn = partial(_loss_fn, jax_model)\n    jax_grad = jax.grad(jax_loss_fn)(params, rng, inputs)\n    paramsv = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        tf_prediction = tf_model(paramsv, rng, inputs)\n        tf_loss = tf.reduce_mean(tf_prediction)\n        tf_grad = tape.gradient(tf_loss, paramsv)\n    self.assertAllClose(jax_grad, tf_grad.numpy())\n    call_tf_loss_fn = partial(_loss_fn, jax2tf.call_tf(tf_model))\n    call_tf_grad = jax.grad(call_tf_loss_fn)(params, rng, inputs)\n    self.assertAllClose(jax_grad, call_tf_grad)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly(self, with_jit=False):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(tf.math.sin, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, x.dtype))(x)\n        z = jnp.cos(y)\n        w = jax2tf.call_tf(lambda z: tf.concat([z, z], axis=0), output_shape_dtype=jax.ShapeDtypeStruct((2 * z.shape[0],), z.dtype))(z)\n        assert w.shape[0] == 2 * x.shape[0]\n        return w\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert w.shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_pytree_result(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(lambda x: (x, tf.concat([x, x], axis=0)), output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct((2 * x.shape[0],), x.dtype)))(x)\n        assert y[0].shape[0] == x.shape[0]\n        assert y[1].shape[0] == 2 * x.shape[0]\n        return y\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert y[0].shape[0] == x.shape[0]",
      "assert y[1].shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_no_output_shape_dtype(self, with_jit=True):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin)(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_mismatch_output_shape_dtype_tree(self, with_jit=False):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin, output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype)))(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(with_jit=with_jit, kind=kind) for with_jit in [True, False] for kind in ['bad_rank', 'bad_dim', 'bad_dtype', 'bad_dtype_x64']))\ndef test_shape_poly_error_mismatch_output_shape_dtype(self, with_jit=False, kind='bad_rank'):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n    if kind == 'bad_rank':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct((), x.dtype))(x)\n    elif kind == 'bad_dim':\n\n        def fun_jax(x):\n            bad_shape = (5 + x.shape[0],)\n            y = jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(bad_shape, x.dtype))(x)\n            return y + jnp.ones(bad_shape, dtype=x.dtype)\n    elif kind == 'bad_dtype':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.int32))(x)\n    elif kind == 'bad_dtype_x64':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x * np.float64(3.0), output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.float64))(x)\n    else:\n        assert False\n    expect_ex = ValueError\n    expect_error = 'The shapes or dtypes returned by the TensorFlow function do not match the declared output_shape_dtype'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)\n    if kind == 'bad_dim' and with_jit:\n        expect_error = 'Dimensions must be equal, but are 4 and 9 for .* AddV2'\n    if kind == 'bad_dim' and jax.config.jax2tf_default_native_serialization:\n        expect_error = 'Error compiling TensorFlow function'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0).astype(intx)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_bool(self, with_jit=False):\n\n    def fun_tf(x, y):\n        return tf.math.logical_and(x, y)\n    x = np.array([True, False, True, False], dtype=np.bool_)\n    y = np.array([True, True, False, False], dtype=np.bool_)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x, y)\n    self.assertAllClose(np.array([True, False, False, False], dtype=np.bool_), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.bool_))\ndef logical_and(x_ref, o_ref):\n    o_ref[()] = jnp.logical_and(x_ref[()], True)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.jit\ndef g(x, y):\n    return lax.add(x, y)"
  },
  {
    "test_code": "def test_pmap(self):\n    logging.info('Running test_pmap on %s devices', jax.local_device_count())\n\n    def plus_2_tf(x):\n        return tf.math.add(2.0, x)\n\n    def fun_jax(x):\n        return np.float32(3.0) * jax2tf.call_tf(plus_2_tf)(x)\n    x = np.arange(jax.local_device_count(), dtype=np.float32)\n    res = jax.pmap(fun_jax)(x)\n    self.assertAllClose(np.float32(3.0 * (x + 2)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def pmap(f, axis_name=None, *, in_axes=0, out_axes=0, static_broadcasted_argnums=(), devices=None, backend=None, axis_size=None, donate_argnums=(), global_arg_shapes=None):\n    devices = tuple(devices) if devices is not None else devices\n    axis_name, static_broadcasted_tuple, donate_tuple = _shared_code_pmap(f, axis_name, static_broadcasted_argnums, donate_argnums, in_axes, out_axes)\n\n    def infer_params(*args, **kwargs):\n        p = _prepare_pmap(f, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, devices, backend, axis_size, args, kwargs)\n        for arg in p.flat_args:\n            dispatch.check_arg(arg)\n        mesh = Mesh(_get_devices(p, backend), (axis_name,))\n        _pmapped, in_specs, out_specs = _cached_shard_map(p.flat_fun, mesh, p.in_axes_flat, p.out_axes_thunk, axis_name)\n        flat_global_args = host_local_array_to_global_array(p.flat_args, mesh, list(in_specs))\n        jitted_f = jax.jit(_pmapped, donate_argnums=(i for i, val in enumerate(p.donated_invars) if val))\n        return (jitted_f, flat_global_args, p.out_tree, mesh, out_specs)\n\n    def wrapped(*args, **kwargs):\n        jitted_f, flat_global_args, out_tree, mesh, out_specs = infer_params(*args, **kwargs)\n        outs = jitted_f(*flat_global_args)\n        outs = global_array_to_host_local_array(outs, mesh, out_specs())\n        return tree_unflatten(out_tree(), outs)\n\n    def lower(*args, **kwargs):\n        jitted_f, _, _, _, _ = infer_params(*args, **kwargs)\n        return jitted_f.lower(*args, **kwargs)\n    wrapped.lower = lower\n    return wrapped"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call(*_, **__):\n    raise NotImplementedError('jax.experimental.host_callback has been deprecated since March 2024 and is now no longer supported. See https://github.com/jax-ml/jax/issues/20385')"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call(*_, **__):\n    raise NotImplementedError('jax.experimental.host_callback has been deprecated since March 2024 and is now no longer supported. See https://github.com/jax-ml/jax/issues/20385')"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape', 'block_k', 'debug', 'out_dtype'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape, block_k: int=256, out_dtype: jnp.dtype | None=None, debug: bool=False) -> jax.Array:\n    if out_dtype is None:\n        if x.dtype != y.dtype:\n            raise TypeError(f'Cannot deduce output dtype for different input dtypes: {x.dtype}, {y.dtype}')\n        out_dtype = x.dtype\n    acc_dtype = jnp.float32\n    if x.dtype in [jnp.int8, jnp.int4, jnp.uint8, jnp.uint4]:\n        acc_dtype = jnp.int32\n    l, r = block_shape\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec((l, block_k), lambda i, _, k: (i, k)), pl.BlockSpec((block_k, r), lambda _, j, k: (k, j))], out_specs=pl.BlockSpec((l, r), lambda i, j, k: (i, j)), grid=(x.shape[0] // l, y.shape[1] // r, x.shape[1] // block_k), scratch_shapes=[pltpu.VMEM((l, r), acc_dtype)]), compiler_params=pltpu.TPUCompilerParams(dimension_semantics=('parallel', 'parallel', 'arbitrary')), debug=debug)(x, y)"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_grad_int_argument_unused(self):\n    batch_size = 5\n    inputs = np.ones((batch_size, 3), dtype=np.float32)\n    rng = np.array([1, 2], dtype=np.uint32)\n    params = np.float32(0.5)\n\n    def jax_model(params, rng, inputs):\n        return jnp.ones([batch_size, 2], dtype=jnp.float32)\n    tf_model = jax2tf.convert(jax_model, with_gradient=True)\n\n    def _loss_fn(inference_fn, params, rng, inputs):\n        prediction = inference_fn(params, rng, inputs)\n        return jnp.mean(prediction)\n    jax_loss_fn = partial(_loss_fn, jax_model)\n    jax_grad = jax.grad(jax_loss_fn)(params, rng, inputs)\n    paramsv = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        tf_prediction = tf_model(paramsv, rng, inputs)\n        tf_loss = tf.reduce_mean(tf_prediction)\n        tf_grad = tape.gradient(tf_loss, paramsv)\n    self.assertAllClose(jax_grad, tf_grad.numpy())\n    call_tf_loss_fn = partial(_loss_fn, jax2tf.call_tf(tf_model))\n    call_tf_grad = jax.grad(call_tf_loss_fn)(params, rng, inputs)\n    self.assertAllClose(jax_grad, call_tf_grad)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly(self, with_jit=False):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(tf.math.sin, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, x.dtype))(x)\n        z = jnp.cos(y)\n        w = jax2tf.call_tf(lambda z: tf.concat([z, z], axis=0), output_shape_dtype=jax.ShapeDtypeStruct((2 * z.shape[0],), z.dtype))(z)\n        assert w.shape[0] == 2 * x.shape[0]\n        return w\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert w.shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_pytree_result(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(lambda x: (x, tf.concat([x, x], axis=0)), output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct((2 * x.shape[0],), x.dtype)))(x)\n        assert y[0].shape[0] == x.shape[0]\n        assert y[1].shape[0] == 2 * x.shape[0]\n        return y\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert y[0].shape[0] == x.shape[0]",
      "assert y[1].shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_no_output_shape_dtype(self, with_jit=True):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin)(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_mismatch_output_shape_dtype_tree(self, with_jit=False):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin, output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype)))(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(with_jit=with_jit, kind=kind) for with_jit in [True, False] for kind in ['bad_rank', 'bad_dim', 'bad_dtype', 'bad_dtype_x64']))\ndef test_shape_poly_error_mismatch_output_shape_dtype(self, with_jit=False, kind='bad_rank'):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n    if kind == 'bad_rank':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct((), x.dtype))(x)\n    elif kind == 'bad_dim':\n\n        def fun_jax(x):\n            bad_shape = (5 + x.shape[0],)\n            y = jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(bad_shape, x.dtype))(x)\n            return y + jnp.ones(bad_shape, dtype=x.dtype)\n    elif kind == 'bad_dtype':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.int32))(x)\n    elif kind == 'bad_dtype_x64':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x * np.float64(3.0), output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.float64))(x)\n    else:\n        assert False\n    expect_ex = ValueError\n    expect_error = 'The shapes or dtypes returned by the TensorFlow function do not match the declared output_shape_dtype'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)\n    if kind == 'bad_dim' and with_jit:\n        expect_error = 'Dimensions must be equal, but are 4 and 9 for .* AddV2'\n    if kind == 'bad_dim' and jax.config.jax2tf_default_native_serialization:\n        expect_error = 'Error compiling TensorFlow function'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_arg(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_res(self, with_jit=True):\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(lambda x: 4.0))(x)\n    self.assertAllClose(4.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_arg(self, with_jit=True):\n    x = np.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_res(self, with_jit=False):\n    x = np.ones((2, 3))\n    res = _maybe_jit(with_jit, jax2tf.call_tf(lambda _: x))(x)\n    self.assertAllClose(x, res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_devicearray_arg(self, with_jit=False):\n    x = jnp.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)\n    x = jnp.array(3.0, dtype=jnp.bfloat16)\n    res = jax2tf.call_tf(lambda x: x)(x)\n    self.assertAllClose(x, res)\n    with self.assertRaises(AssertionError):\n        self.assertTrue(np.shares_memory(x, res))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_pytree(self, with_jit=True):\n\n    def fun_tf(x: dict, y: tuple) -> tuple:\n        return (x['first'] * x['second'], y[0] + y[1])\n    x = dict(first=np.float32(3.0), second=np.float32(4.0))\n    y = (np.float64(5.0), np.float64(6.0))\n    fun_jax = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))\n    res = fun_jax(x, y)\n    self.assertAllClose((np.float32(12.0), np.float64(11.0)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_result_tuple(self):\n    x1 = np.ones(3, dtype=np.int32)\n    x2 = np.ones(5, dtype=np.float32)\n\n    def fun_tf():\n        return tf.tuple([x1, x2])\n    fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n    res = fun_jax()\n    self.assertAllClose(res, (x1, x2))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_error_bad_result_tensorarray(self):\n\n    def fun_tf():\n        ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n        ta = ta.unstack([0, 1, 2, 3, 4])\n        return ta\n    with self.assertRaisesRegex(ValueError, 'The called TF function returns a result that is not convertible to JAX'):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_error_bad_result_string(self):\n\n    def fun_tf():\n        return tf.constant('foo')\n    with self.assertRaisesRegex(ValueError, 'The called TF function returns a result that is not convertible to JAX'):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_control_flow(self, with_jit=True):\n\n    def times_5_tf(x):\n        c = lambda i, acc: tf.less(i, 5)\n        b = lambda i, acc: (tf.add(i, 1), tf.add(acc, x))\n        _, acc = tf.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])\n        return acc\n\n    def fun_jax(x):\n\n        def body(_, acc):\n            return jax2tf.call_tf(times_5_tf)(acc)\n        return lax.fori_loop(0, 3, body, x)\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(np.float32(x * 5 * 5 * 5), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}{('_jit' if with_jit else '')}', dtype=dtype, with_jit=with_jit) for dtype in set(jtu.dtypes.all) - {np.bool_} for with_jit in [True, False]))\ndef test_dtypes(self, dtype=np.int32, with_jit=True):\n\n    def fun_tf(x):\n        return tf.raw_ops.AddV2(x=x, y=tf.constant(3, dtype=dtype))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x) + x\n    x = np.ones((3,), dtype=dtype)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(dtype(2 * x + 3), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_bool(self, with_jit=False):\n\n    def fun_tf(x, y):\n        return tf.math.logical_and(x, y)\n    x = np.array([True, False, True, False], dtype=np.bool_)\n    y = np.array([True, True, False, False], dtype=np.bool_)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x, y)\n    self.assertAllClose(np.array([True, False, False, False], dtype=np.bool_), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_input(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 5.0\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = jnp.sin(x)\n    self.assertAllClose(res_call_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_output(self, with_jit=True):\n\n    def f_tf(x):\n        return (tf.constant(3.0, tf.float64), x)\n    x = np.float32(5.0)\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = (3.0, x)\n    self.assertAllClose(res_call_tf, res_jax)\n    res_call_tf_jit = jax.jit(jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(res_call_tf_jit, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_var_read(self, with_jit=True):\n    outer_var_array = np.array([3.0, 4.0], dtype=np.float32)\n    outer_var = tf.Variable(outer_var_array)\n\n    def fun_tf(x):\n        return x * outer_var + 1.0\n    x = np.array([2.0, 5.0], dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * outer_var_array + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_var_read_x64(self, with_jit=True):\n    outer_var_array = np.array([3.0, 4.0], dtype=np.float64)\n    outer_var = tf.Variable(outer_var_array)\n\n    def fun_tf(x):\n        return x * tf.cast(outer_var, x.dtype) + 1.0\n    x = np.array([2.0, 5.0], dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * outer_var_array + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_with_var_different_shape(self):\n    v = tf.Variable((4.0, 2.0), dtype=tf.float32)\n\n    def tf_func(x):\n        return v + x\n    x = np.float32(123.0)\n    tf_out = tf_func(x)\n    jax_func = jax.jit(jax2tf.call_tf(tf_func))\n    jax_out = jax_func(x)\n    self.assertAllClose(tf_out, jax_out, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_var_write_error(self, with_jit=True):\n    if with_jit:\n        raise unittest.SkipTest('variable writes not yet working')\n    outer_var = tf.Variable(3.0, dtype=np.float32)\n\n    def fun_tf(x):\n        outer_var.assign(tf.constant(4.0))\n        return x * outer_var + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 4.0 + 1, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_tensor_capture(self, with_jit=True):\n    outer_tensor = tf.constant(3.0, dtype=np.float32)\n\n    def fun_tf(x):\n        return x * outer_tensor + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 3.0 + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_tensor_capture_x64(self, with_jit=True):\n    outer_tensor = tf.constant(3.0, dtype=np.float64)\n\n    def fun_tf(x):\n        return x * tf.cast(outer_tensor * 3.14, tf.float32) + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 3.0 * 3.14 + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_value_capture(self, with_jit=True):\n    outer_val = np.array(3.0, dtype=np.float32)\n\n    def fun_tf(x):\n        return x * outer_val + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 3.0 + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_multiple_capture(self, with_jit=True):\n    if jtu.test_device_matches(['gpu']):\n        raise unittest.SkipTest('Test fails on GPU')\n    v2 = tf.Variable(2.0, dtype=np.float32)\n    v3 = tf.Variable(3.0, dtype=np.float32)\n    t4 = tf.constant(4.0, dtype=np.float32)\n    t5 = tf.constant(5.0, dtype=np.float32)\n\n    def fun_tf(x):\n        return (x * v3 + t4 + v2) * v3 + t5\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose((x * 3.0 + 4.0 + 2.0) * 3.0 + 5.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_with_capture_then_convert_again(self):\n    captured_by_tf = tf.Variable(np.arange(1024, dtype=np.float32))\n\n    def tf_fn(x):\n        return tf.math.add(x, captured_by_tf)\n    x = np.arange(1024, dtype=np.float32)\n    res = jax2tf.convert(jax2tf.call_tf(tf_fn))(x)\n    self.assertAllClose(res, 2 * x)\n    res = tf.function(jax2tf.convert(jax2tf.call_tf(tf_fn)), autograph=False)(x)\n    self.assertAllClose(res, 2 * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_grad(self, with_jit=False):\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, jax.grad(jax2tf.call_tf(tf.math.sin)))(x)\n    self.assertAllClose(np.cos(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_grad_pytree(self, with_jit=False):\n\n    def fun_tf(x: dict, y: tuple) -> tuple:\n        return x['first'] * x['second'] + 3.0 * y[0] + 4.0 * y[1]\n    x = dict(first=np.float32(3.0), second=np.float32(4.0))\n    y = (np.float32(5.0), np.float32(6.0))\n    grad_x = _maybe_jit(with_jit, jax.grad(jax2tf.call_tf(fun_tf)))(x, y)\n    self.assertAllClose(dict(first=np.float32(4.0), second=np.float32(3.0)), grad_x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_grad_nested(self):\n    b = np.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)\n    c = np.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)\n    x_dict = dict(b=b, c=c)\n\n    def f_tf(x_dict):\n        return dict(r=tf.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    @jax.jit\n    def f_jax(x_dict):\n        return dict(r=jnp.matmul(x_dict['c'], x_dict['b']), s=7.0 * x_dict['c'])\n\n    def loss(functional, x_dict):\n        prediction = functional(x_dict)\n        weights = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n        weighted_pred = jnp.matmul(weights, prediction['r'])\n        return jnp.sum(weighted_pred) + 4.0 * jnp.sum(prediction['s'])\n    g_fun_with_tf = jax.grad(partial(loss, jax2tf.call_tf(f_tf)))\n    g_fun_with_jax = jax.grad(partial(loss, f_jax))\n    g_tf = g_fun_with_tf(x_dict)\n    g_jax = g_fun_with_jax(x_dict)\n    self.assertAllClose(g_jax, g_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_grad_int_argument_unused(self):\n    batch_size = 5\n    inputs = np.ones((batch_size, 3), dtype=np.float32)\n    rng = np.array([1, 2], dtype=np.uint32)\n    params = np.float32(0.5)\n\n    def jax_model(params, rng, inputs):\n        return jnp.ones([batch_size, 2], dtype=jnp.float32)\n    tf_model = jax2tf.convert(jax_model, with_gradient=True)\n\n    def _loss_fn(inference_fn, params, rng, inputs):\n        prediction = inference_fn(params, rng, inputs)\n        return jnp.mean(prediction)\n    jax_loss_fn = partial(_loss_fn, jax_model)\n    jax_grad = jax.grad(jax_loss_fn)(params, rng, inputs)\n    paramsv = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        tf_prediction = tf_model(paramsv, rng, inputs)\n        tf_loss = tf.reduce_mean(tf_prediction)\n        tf_grad = tape.gradient(tf_loss, paramsv)\n    self.assertAllClose(jax_grad, tf_grad.numpy())\n    call_tf_loss_fn = partial(_loss_fn, jax2tf.call_tf(tf_model))\n    call_tf_grad = jax.grad(call_tf_loss_fn)(params, rng, inputs)\n    self.assertAllClose(jax_grad, call_tf_grad)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_grad_with_float0_result(self):\n\n    def f_jax(x, y):\n        return (2 * x, 2 * x + y * y)\n\n    def f_tf(x, y):\n        return (2 * x, tf.cast(2 * x, dtype=y.dtype) + y * y)\n\n    def wrapper(functional, x, y):\n        return jnp.sum(2.0 * functional(3 * x, 4.0 * y)[1])\n    grad_g = jax.grad(partial(wrapper, f_jax), allow_int=True, argnums=(0, 1))\n    grad_g_call_tf = jax.grad(partial(wrapper, jax2tf.call_tf(f_tf)), allow_int=True, argnums=(0, 1))\n    x = np.int32(2)\n    y = np.float32(3.0)\n    g_jax = grad_g(x, y)\n    g_call_tf = grad_g_call_tf(x, y)\n    self.assertEqual(g_jax[0].dtype, dtypes.float0)\n    self.assertEqual(g_call_tf[0].dtype, dtypes.float0)\n    self.assertAllClose(g_jax[1], g_call_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_grad_custom(self, with_jit=False):\n\n    @tf.custom_gradient\n    def func_square_tf(x):\n\n        def grad(dy, variables=None):\n            return (3.0 * x * dy,)\n        return (x * x, grad)\n    x = np.float32(4.0)\n    grad_x = _maybe_jit(with_jit, jax.grad(jax2tf.call_tf(func_square_tf)))(x)\n    self.assertAllClose(np.float32(3.0) * x, grad_x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_degree={degree!r}{('_jit' if with_jit else '')}', degree=degree, with_jit=with_jit) for degree in [1, 2, 3, 4] for with_jit in [True, False]))\ndef test_higher_order_grad(self, degree=2, with_jit=False):\n\n    def fun_tf(x):\n        return 2.0 * x * x * x\n\n    def fun_jax(x):\n        return 3.0 * _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n\n    def fun_jax_pure(x):\n        return 3.0 * fun_tf(x)\n    grad_jax = fun_jax\n    grad_jax_pure = fun_jax_pure\n    for _ in range(degree):\n        grad_jax = jax.grad(grad_jax)\n        grad_jax_pure = jax.grad(grad_jax_pure)\n    res_jax = grad_jax(np.float32(5.0))\n    logging.info('Grad of %s degree is %s', degree, res_jax)\n    self.assertAllClose(res_jax, grad_jax_pure(np.float32(5.0)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_pmap(self):\n    logging.info('Running test_pmap on %s devices', jax.local_device_count())\n\n    def plus_2_tf(x):\n        return tf.math.add(2.0, x)\n\n    def fun_jax(x):\n        return np.float32(3.0) * jax2tf.call_tf(plus_2_tf)(x)\n    x = np.arange(jax.local_device_count(), dtype=np.float32)\n    res = jax.pmap(fun_jax)(x)\n    self.assertAllClose(np.float32(3.0 * (x + 2)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_function_compile_time_constant_inputs(self):\n    x = np.array([1, 2, 3], dtype=np.int32)\n\n    def fun_tf(x):\n        end_idx = x[1]\n        res = x[0:end_idx]\n        return res\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    self.assertAllClose(x[0:x[1]], res1)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_effectful(self):\n    x = np.ones((3,), dtype=np.float32)\n    lower_effect = jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=True)).lower(x)\n    self.assertNotEmpty(lower_effect._lowering.compile_args['unordered_effects'])\n    lower_no_effect = jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=False)).lower(x)\n    self.assertEmpty(lower_no_effect._lowering.compile_args['unordered_effects'])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_module_documentation(self):\n\n    def cos_tf(x):\n        return tf.math.cos(x)\n\n    def cos_tf_sin_jax(x):\n        return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))\n    x = np.float32(1.0)\n    cos_tf_sin_jax(x)\n    jax.jit(cos_tf_sin_jax)(x)\n    jax.grad(cos_tf_sin_jax)(x)\n    logging.info(jax.make_jaxpr(cos_tf_sin_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_tf_gather(self):\n    \"\"\"tf_gather gradient output is tf.IndexSlices.\"\"\"\n    operand = jnp.array(np.random.uniform(size=(100, 128)))\n    indices = jnp.array(np.random.randint(low=0, high=100, size=(4000,)))\n\n    @tf.function(jit_compile=True, autograph=False)\n    def fun_tf(operand, indices):\n        return tf.experimental.numpy.std(tf.gather(operand, indices))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    grad_fun_jax = jax.grad(fun_jax)\n    grad_res = grad_fun_jax(operand, indices)\n    self.assertEqual(grad_res.shape, (100, 128))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_output_shape_dtype_none(self):\n    x = jnp.zeros(10, dtype=jnp.float32)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def fun_tf(x):\n        return\n    fun_jax_1 = jax2tf.call_tf(fun_tf, output_shape_dtype=None)\n    fun_jax_2 = jax2tf.call_tf(fun_tf)\n    self.assertIsNone(fun_jax_1(x))\n    self.assertIsNone(fun_jax_2(x))\n    fun_jax_3 = jax2tf.call_tf(fun_tf, output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        _ = fun_jax_3(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_output_shape_dtype_not_none(self):\n    x = jnp.zeros(10, dtype=jnp.float32)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def fun_tf(x):\n        return x\n    fun_jax_1 = jax2tf.call_tf(fun_tf, output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32))\n    fun_jax_2 = jax2tf.call_tf(fun_tf)\n    self.assertAllClose(fun_jax_1(x), fun_jax_2(x))\n    fun_jax_3 = jax2tf.call_tf(fun_tf, output_shape_dtype=None)\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        _ = fun_jax_3(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_type={type_.__name__}', 'type_': type_} for type_ in dlpack.SUPPORTED_DTYPES))\ndef test_avoid_copy_between_gpu_and_cpu(self, type_):\n    try:\n        gpu_devices = jax.devices('gpu')\n    except RuntimeError:\n        gpu_devices = []\n    if not gpu_devices:\n        raise unittest.SkipTest('Test requires a GPU device.')\n\n    def tf_fun(x):\n        if type_ == jnp.bool_:\n            return tf.math.logical_or(x, True)\n        else:\n            return x + 1\n    jax_array_on_gpu = jnp.zeros([1], type_, device=gpu_devices[0])\n\n    @contextlib.contextmanager\n    def _transfer_guard(guard_level):\n        with contextlib.ExitStack() as stack:\n            stack.enter_context(jax.transfer_guard_device_to_device(guard_level))\n            stack.enter_context(jax.transfer_guard_device_to_host(guard_level))\n            if type_ != jnp.int32:\n                stack.enter_context(jax.transfer_guard_host_to_device(guard_level))\n            yield\n    with _transfer_guard('disallow_explicit'):\n        jax2tf.call_tf(tf_fun)(jax_array_on_gpu)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_call_tf_under_function_context(self):\n\n    def fun_jax(x, y):\n        z = jax2tf.call_tf(tf.math.sin)(x) + jnp.cos(y)\n        return z\n    x = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n    y = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n    converted_fun = tf.function(jax2tf.convert(fun_jax, native_serialization=True))\n    expected = np.sin(x) + np.cos(y)\n    res = tf.function(converted_fun, jit_compile=True, autograph=False)(x, y)\n    self.assertAllClose(expected, res.numpy(), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.all_floating)))\ndef test_all_floating_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    x = jnp.array([5.0, 6.0, 7.0]).astype(dtype)\n\n    def assert_all_close_support_bfloat16(baseline, candidate):\n\n        def conversion(x):\n            if x.shape == tf.TensorShape([]):\n                x = tf.convert_to_tensor([x])\n            if dtype == jnp.float16:\n                x = tf.cast(x, tf.float32)\n            return x\n        baseline = jax.tree_util.tree_map(conversion, baseline)\n        candidate = jax.tree_util.tree_map(conversion, candidate)\n        tol = 0.01 if jtu.test_device_matches(['tpu']) and dtype == np.float16 else None\n        self.assertAllClose(baseline, candidate, atol=tol, rtol=tol)\n    assert_all_close_support_bfloat16(tf_f(x), tf_f_rt(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    assert_all_close_support_bfloat16(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f))\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    assert_all_close_support_bfloat16(grad_fun_jax(x), grad_fun_jax_rt(x))\n    assert_all_close_support_bfloat16(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}', dtype=dtype) for dtype in set(jtu.dtypes.complex)))\ndef test_complex_input_gradient(self, dtype):\n\n    def tf_f(x):\n        res = tf.math.sin(x)\n        return tf.reduce_sum(res)\n    x = jnp.array([5.0 + 4j, 6.0 + 3j, 7.0 + 8j]).astype(dtype)\n    jax_f = jax2tf.call_tf(tf_f)\n    tf_f_rt = jax2tf.convert(jax_f)\n    self.assertAllClose(tf_f(x), tf_f_rt(x))\n    self.assertAllClose(tf.function(tf_f)(x), tf.function(tf_f_rt)(x))\n    self.assertAllClose(tf.function(tf_f, jit_compile=True)(x), tf.function(tf_f_rt, jit_compile=True)(x))\n    grad_fun_jax = jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)\n    grad_fun_jax_rt = jax2tf.call_tf(jax2tf.convert(grad_fun_jax))\n    self.assertAllClose(grad_fun_jax(x), grad_fun_jax_rt(x))\n    self.assertAllClose(jax.jit(grad_fun_jax)(x), jax.jit(grad_fun_jax_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_grad_pytree_arg_with_none_leaf(self):\n\n    def tf_f(x, params):\n        return x * params['y']\n    x = jnp.array(1.0)\n    y = jnp.array(2.0)\n    actual = jax.grad(jax2tf.call_tf(tf_f), argnums=(1,))(x, {'y': y, 'other': None})\n    self.assertDictEqual(actual[0], {'y': x, 'other': None})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_alternate(self):\n    f_tf_inner = tf.math.sin\n\n    def f_jax(x_jax):\n        y_jax = jnp.cos(x_jax)\n        z_jax = jax2tf.call_tf(f_tf_inner)(y_jax)\n        return jnp.cos(z_jax)\n\n    def f_tf_outer(x_tf):\n        y_tf = tf.math.sin(x_tf)\n        z_tf = jax2tf.convert(f_jax)(y_tf)\n        return tf.math.sin(z_tf)\n    x = np.float32(0.7)\n    self.assertAllClose(np.sin(np.cos(np.sin(np.cos(np.sin(x))))), f_tf_outer(x).numpy())\n    xv = tf.Variable(x)\n    with tf.GradientTape() as tape:\n        res = f_tf_outer(xv)\n    g_tf = tape.gradient(res, xv)\n    _, gf = tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))\n    expected_res = np.sin(np.cos(np.sin(np.cos(np.sin(x)))))\n    self.assertAllClose(expected_res, f_tf_outer(x).numpy())\n    expected_grad = np.cos(np.cos(np.sin(np.cos(np.sin(x))))) * np.sin(np.sin(np.cos(np.sin(x)))) * np.cos(np.cos(np.sin(x))) * np.sin(np.sin(x)) * np.cos(x)\n    self.assertAllClose(expected_grad, g_tf.numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False)(x).numpy())\n    self.assertAllClose(expected_res, tf.function(f_tf_outer, autograph=False, jit_compile=True)(x).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_saved_model(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.sin(x)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(np.sin(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(np.sin(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_saved_model_polymorphic_input_static_output(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x)\n    fun_tf_rt = jax2tf.convert(fun_jax)\n    res = fun_tf_rt(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    res = tf.function(fun_tf_rt, jit_compile=True, autograph=False)(x)\n    self.assertAllClose(fun_tf(x), res.numpy())\n    reloaded_f, _ = tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])\n    res = reloaded_f(x)\n    self.assertAllClose(fun_tf(x), res.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_static_output_shape(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def fun_tf(x):\n        return tf.math.reduce_sum(tf.math.sin(x))\n    fun_jax = jax2tf.call_tf(fun_tf)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    self.assertAllClose(fun_tf(x), fun_tf_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly(self, with_jit=False):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(tf.math.sin, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, x.dtype))(x)\n        z = jnp.cos(y)\n        w = jax2tf.call_tf(lambda z: tf.concat([z, z], axis=0), output_shape_dtype=jax.ShapeDtypeStruct((2 * z.shape[0],), z.dtype))(z)\n        assert w.shape[0] == 2 * x.shape[0]\n        return w\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert w.shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_pytree_result(self, with_jit=True):\n    if jax.config.jax2tf_default_native_serialization:\n        raise unittest.SkipTest('TODO(b/268386622): call_tf with shape polymorphism and native serialization.')\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        y = jax2tf.call_tf(lambda x: (x, tf.concat([x, x], axis=0)), output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct((2 * x.shape[0],), x.dtype)))(x)\n        assert y[0].shape[0] == x.shape[0]\n        assert y[1].shape[0] == 2 * x.shape[0]\n        return y\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    res_tf = fun_tf_rt(x)\n    self.assertAllClose(fun_jax(x), res_tf)",
    "assertions": [
      "assert y[0].shape[0] == x.shape[0]",
      "assert y[1].shape[0] == 2 * x.shape[0]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_no_output_shape_dtype(self, with_jit=True):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin)(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_shape_poly_error_mismatch_output_shape_dtype_tree(self, with_jit=False):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n\n    def fun_jax(x):\n        return jax2tf.call_tf(tf.math.sin, output_shape_dtype=(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype)))(x)\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(ValueError, 'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype'):\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(with_jit=with_jit, kind=kind) for with_jit in [True, False] for kind in ['bad_rank', 'bad_dim', 'bad_dtype', 'bad_dtype_x64']))\ndef test_shape_poly_error_mismatch_output_shape_dtype(self, with_jit=False, kind='bad_rank'):\n    x = np.array([7, 8, 9, 10], dtype=np.float32)\n    if kind == 'bad_rank':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct((), x.dtype))(x)\n    elif kind == 'bad_dim':\n\n        def fun_jax(x):\n            bad_shape = (5 + x.shape[0],)\n            y = jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(bad_shape, x.dtype))(x)\n            return y + jnp.ones(bad_shape, dtype=x.dtype)\n    elif kind == 'bad_dtype':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x, output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.int32))(x)\n    elif kind == 'bad_dtype_x64':\n\n        def fun_jax(x):\n            return jax2tf.call_tf(lambda x: x * np.float64(3.0), output_shape_dtype=jax.ShapeDtypeStruct(x.shape, np.float64))(x)\n    else:\n        assert False\n    expect_ex = ValueError\n    expect_error = 'The shapes or dtypes returned by the TensorFlow function do not match the declared output_shape_dtype'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)\n    if kind == 'bad_dim' and with_jit:\n        expect_error = 'Dimensions must be equal, but are 4 and 9 for .* AddV2'\n    if kind == 'bad_dim' and jax.config.jax2tf_default_native_serialization:\n        expect_error = 'Error compiling TensorFlow function'\n    fun_tf_rt = _maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))\n    with self.assertRaisesRegex(expect_ex, expect_error):\n        fun_tf_rt(x)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_inner_native_serialization(self):\n    x = np.ones((3,), dtype=np.float32)\n\n    def f_inner_jax(x):\n        return jnp.sin(x)\n\n    def f_outer_jax(x):\n        f_inner_tf = jax2tf.convert(f_inner_jax, native_serialization=True)\n        return jnp.cos(jax2tf.call_tf(f_inner_tf)(x))\n    f_outer_tf = tf.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)\n    f_outer_graph = str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())\n    self.assertIn('op: \"Cos\"', f_outer_graph)\n    self.assertIn('op: \"XlaCallModule\"', f_outer_graph)\n    self.assertNotIn('op: \"Sin\"', f_outer_graph)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_call_tf_graph(self):\n    const = tf.Variable(0.0, dtype=tf.float32)\n\n    @tf.function(jit_compile=True)\n    def tf_func_1(x):\n        return x * x + const\n\n    @tf.function\n    def tf_func_2(x, y):\n        return tf_func_1(x) + y\n\n    @tf.function\n    def tf_func_3(x, y, z):\n        return (tf_func_2(x, y) + z, z)\n    x = jnp.array(3.0, dtype=jnp.float32)\n    y = jnp.array(3.0, dtype=jnp.float32)\n    z = jnp.array(5.0, dtype=jnp.float32)\n    f_jax = jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=False))\n    stablehlo_module = f_jax.lower(x, y, z).compiler_ir('stablehlo')\n    self.assertNotIn('stablehlo.custom_call', str(stablehlo_module))\n    f_jax = jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=True))\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = f_jax.lower(x, y, z).compiler_ir('stablehlo')\n        self.assertIn('stablehlo.custom_call', str(stablehlo_module))\n        called_index_list = []\n\n        def _extract_info(op):\n            if op.operation.name != 'stablehlo.custom_call':\n                return\n            tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n            called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n            called_index_list.append(called_index)\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@parameterized.named_parameters(dict(testcase_name='multiple_outputs', tf_f=lambda x: tf.py_function(np.sin, [x], tf.float32), output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32)), dict(testcase_name='zero_outputs', tf_f=lambda x: print(tf.strings.length(tf.constant('hello, world'))), output_shape_dtype=None))\ndef test_call_tf_graph_non_compilable(self, tf_f, output_shape_dtype):\n    inputs = jnp.ones([10], dtype=jnp.float32)\n    called_index_list = []\n    xla_call_module_list = []\n\n    def _extract_info(op):\n        if op.operation.name != 'stablehlo.custom_call':\n            return\n        tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n        called_index = ir.IntegerAttr(tf_backend_config['called_index']).value\n        called_index_list.append(called_index)\n    jax_f = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)\n    self.assertAllClose(tf_f(inputs), jax_f(inputs))\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self.assertIn('stablehlo.custom_call @tf.call_tf_function', str(stablehlo_module))\n        self.assertIn('tf.backend_config', str(stablehlo_module))\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n        self.assertLen(called_index_list, 1)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n    func_def = restored_model.f.concrete_functions[0]\n    for node_def in func_def.graph.as_graph_def().node:\n        if node_def.op == 'XlaCallModule':\n            xla_call_module_list.append(node_def)\n    self.assertLen(xla_call_module_list, 1)\n    xla_call_module = xla_call_module_list[0]\n    self.assertGreaterEqual(xla_call_module.attr['version'].i, 5)\n    self.assertIn('function_list', str(xla_call_module.attr))\n    xla_call_module_list.clear()\n    called_index_list.clear()\n\n    def jax_f_2(x):\n        res1 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        res2 = jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)\n        return (res1, res2)\n    stablehlo_module = None\n    with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n        stablehlo_module = jax.jit(jax_f_2).lower(inputs).compiler_ir('stablehlo')\n    if stablehlo_module:\n        self._walk_stablehlo_operations(stablehlo_module, _extract_info)\n    xla_call_module_list.clear()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_b279454591(self):\n    \"\"\"Test case when tensorflow function returns `StatefulPartitionedCall` op.\"\"\"\n    inputs = jnp.ones([10], dtype=jnp.float32)\n\n    def tf_f(x):\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return x\n    jax_f = jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)\n    tf_f_rt = jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt, input_args=[inputs])\n\n    def tf_f_2():\n        y = tf.math.sin(3.0)\n        tf.print(y)\n        return\n    jax_f_2 = jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)\n    tf_f_rt_2 = jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)\n    _, _ = tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(version=version) for version in [9]])\ndef test_call_tf_graph_ordered(self, *, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function\n        def tf_print(x):\n            tf.print(x)\n        call_tf_print = jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)\n        x = jnp.array(1.0, dtype=jnp.float32)\n\n        def body(i, x):\n            call_tf_print(x)\n            return x + 1\n\n        @jax.jit\n        def f_jax(x):\n            return jax.lax.fori_loop(0, 4, body, x)\n        num_custom_calls = 0\n\n        def _check_mlir_ops(op):\n            nonlocal num_custom_calls\n            if op.operation.name == 'stablehlo.custom_call' and ir.StringAttr(op.attributes['call_target_name']).value == 'tf.call_tf_function':\n                num_custom_calls += 1\n                tf_backend_config = ir.DictAttr(op.attributes['tf.backend_config'])\n                self.assertTrue(ir.BoolAttr(tf_backend_config['has_token_input_output']).value)\n                self.assertTrue(hlo.TokenType.isinstance(op.operands[0].type))\n                self.assertTrue(hlo.TokenType.isinstance(op.results[0].type))\n        stablehlo_module = None\n        with self.assertRaisesRegex(ValueError, 'call_tf_graph=True only support exporting by jax2tf.convert currently'):\n            lower = f_jax.lower(x)\n            self.assertNotEmpty(lower._lowering.compile_args['ordered_effects'])\n            stablehlo_module = lower.compiler_ir('stablehlo')\n        if stablehlo_module:\n            self._walk_stablehlo_operations(stablehlo_module, _check_mlir_ops)\n            self.assertEqual(num_custom_calls, 1)\n        f_tf = jax2tf.convert(f_jax, native_serialization=True, with_gradient=False)\n        _, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(ordered=ordered, version=version) for ordered in [True, False] for version in [9]])\ndef test_call_tf_graph_polymorphic(self, ordered: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        @tf.function(jit_compile=True, autograph=False)\n        @partial(jax2tf.convert, with_gradient=False, native_serialization=True, polymorphic_shapes=['(b)'])\n        @jax.jit\n        def tf_f_2(x):\n            tf_f = lambda x: print(tf.strings.length(tf.constant('hello, world')))\n            jax2tf.call_tf(tf_f, call_tf_graph=True, ordered=ordered)(x)\n            return x\n        x = np.arange(3, dtype=np.int32)\n        _ = tf.function(tf_f_2, autograph=False).get_concrete_function(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "@unittest.skip('The root cause here is because the XLACallModule.function_list attribute depends on JAX call_tf lowering. The 2nd time tf.SavedModel TF tracing will not trigger call_tf tracing since it was already cached. The solution is to create the `CallTFContext` to make TF tracing and JAX tracing work together correctly.')\ndef test_call_tf_graph_save_and_load(self):\n\n    def jax_func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data_inputs = (np.array([0.5, 0.7], dtype=np.float32),)\n\n    def tf_func(the_input):\n        res = jax2tf.convert(jax_func, native_serialization=True)(the_input)\n        return tf.identity(res, name='the_result')\n    jit_tf_func = tf.function(tf_func, autograph=False, jit_compile=True)\n    _ = jit_tf_func.get_concrete_function(*data_inputs)\n    module = tf.Module()\n    module.call = jit_tf_func\n    root_dir = self.create_tempdir()\n    saved_model_dir = os.path.join(root_dir, 'saved_model')\n    tf.saved_model.save(module, saved_model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=False))\n    loaded_model = tf.saved_model.load(saved_model_dir)\n    res = loaded_model.call(*data_inputs)\n    self.assertAllClose(jax_func(*data_inputs), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_function_dynamic_shape(self):\n    x = np.array([-1, 0, 1], dtype=np.int32)\n\n    def fun_tf(x):\n        return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n    res1 = jax2tf.call_tf(fun_tf)(x)\n    expected = x[1:]\n    self.assertAllClose(expected, res1, check_dtypes=False)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_jax = jax.jit(jax2tf.call_tf(fun_tf))\n        fun_jax(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        fun_tf_rt = jax2tf.convert(jax2tf.call_tf(fun_tf))\n        fun_tf_rt(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_grad_int_argument(self):\n\n    def f(param, state, x):\n        return (param * x, state)\n    param = np.array([0.7, 0.9], dtype=np.float32)\n    state = dict(array=np.float32(1.0), counter=7, truth=True)\n    x = np.float32(3.0)\n    f_call_tf = jax2tf.call_tf(f)\n    g_call_tf = jax.grad(lambda *args: jnp.sum(f_call_tf(*args)[0]))(param, state, x)\n    g = jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)\n    self.assertAllClose(g_call_tf, g)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_custom_grad(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))\n    x = np.float32(0.7)\n    self.assertAllClose(f(x), f_rt(x))\n    self.assertAllClose(jax.grad(f)(x), jax.grad(f_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_custom_grad_saved_model(self):\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), np.float32(3.0) * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n\n    def g(x):\n        return jnp.sum(f(x))\n    g_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])\n    g_rt = jax2tf.call_tf(g_tf)\n    x = np.array([0.7], dtype=np.float32)\n    self.assertAllClose(g(x), g_rt(x))\n    self.assertAllClose(jax.grad(g)(x), jax.grad(g_rt)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "@parameterized.named_parameters((_named_test(f2_function=f2_function, f2_saved_model=f2_saved_model, f4_function=f4_function, f4_saved_model=f4_saved_model) for f2_function in [True, False] for f2_saved_model in [True, False] for f4_function in [True, False] for f4_saved_model in [True, False]))\ndef test_several_round_trips(self, f2_function=False, f2_saved_model=False, f4_function=False, f4_saved_model=False):\n    if f2_saved_model and f4_saved_model and (not jax.config.jax2tf_default_native_serialization):\n        raise unittest.SkipTest('TODO: error invalid capture when saving custom gradients')\n    x = np.array(0.7, dtype=np.float32)\n\n    def f(n):\n\n        def fn(x):\n            acc = np.array(2.0, dtype=x.dtype)\n            for i in range(n):\n                acc *= x\n            return acc\n        return fn\n    f2_tf = lambda x: x * jax2tf.convert(f(1))(x)\n    if f2_function:\n        f2_tf = tf.function(f2_tf, autograph=False)\n    if f2_saved_model:\n        f2_tf, _ = tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])\n    self.assertAllClose(f(2)(x), f2_tf(x).numpy())\n    _, (g_f2_ft,) = tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])\n    self.assertAllClose(jax.grad(f(2))(x), g_f2_ft.numpy())\n    f3_jax = lambda x: x * jax2tf.call_tf(f2_tf)(x)\n    self.assertAllClose(f(3)(x), f3_jax(x))\n    self.assertAllClose(f(3)(x), jax.jit(f3_jax)(x))\n    self.assertAllClose(jax.grad(f(3))(x), jax.grad(f3_jax)(x))\n    f4_tf = lambda x: x * jax2tf.convert(f3_jax)(x)\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())\n    if f4_function:\n        f4_tf = tf.function(f4_tf, autograph=False)\n    if f4_saved_model:\n        f4_tf, _ = tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])\n    self.assertAllClose(f(4)(x), f4_tf(x).numpy())\n    _, (g_f4_ft,) = tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])\n    self.assertAllClose(jax.grad(f(4))(x), g_f4_ft.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_error_non_compilable_strings(self):\n\n    def f_tf_non_compilable(x):\n        return tf.strings.length(tf.strings.format('Hello {}!', [x]))\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.float32(0.7)\n    self.assertAllClose(f_tf_non_compilable(x).numpy(), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        jax.jit(f_jax)(x)\n    with self.assertRaisesRegex(ValueError, _call_tf_non_compilable_error):\n        lax.cond(True, lambda x: f_jax(x), lambda x: f_jax(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_error_non_compilable_dynamic_shape(self):\n\n    def f_tf_non_compilable(x):\n        return tf.cond(x[0], lambda: x[1:], lambda: x)\n    f_jax = jax2tf.call_tf(f_tf_non_compilable)\n    x = np.array([True, False], dtype=np.bool_)\n    self.assertAllClose(f_tf_non_compilable(x), f_jax(x))\n    with self.assertRaisesRegex(ValueError, _call_tf_dynamic_shape_error):\n        jax.jit(f_jax)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_repro_193754660(self):\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=np.int32)\n\n    def f_jax(x):\n        return x[1]\n    f_tf = jax2tf.convert(f_jax)\n    f_tf_rt, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    f_jax2 = jax2tf.call_tf(f_tf_rt)\n    f_tf2 = jax2tf.convert(f_jax2)\n    res = tf.function(f_tf2, autograph=False)(x)\n    self.assertAllClose(res.numpy(), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_multi_platform(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    jax_platforms = []\n    for backend in ['cpu', 'gpu', 'tpu']:\n        try:\n            devices = jax.devices(backend)\n        except RuntimeError:\n            devices = []\n        if devices:\n            jax_platforms.append(devices[0].platform)\n    jax_and_tf_platforms = set(jax_platforms) & {d.device_type.lower() for d in self.tf_devices}\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    exp = export.export(jax.jit(f_jax), platforms=lowering_platforms)(x)\n    for jax_platform in jax_and_tf_platforms:\n        with self.subTest(jax_platform):\n            jax_device = jax.devices(jax_platform)[0]\n            x_device = jax.device_put(x, jax_device)\n            logging.info('Running harness natively on %s', jax_device)\n            native_res = f_jax(x_device)\n            logging.info('Running exported harness on %s', jax_device)\n            exported_res = exp.call(x_device)\n            self.assertAllClose(native_res, exported_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_multi_platform_call_tf_graph(self):\n\n    def tf_fun(x):\n        return tf.math.sin(x)\n\n    def f_jax(x):\n        return jnp.cos(jax2tf.call_tf(tf_fun, call_tf_graph=True, ordered=True)(jnp.cos(x)))\n    x = np.arange(12, dtype=np.float32).reshape((3, 4))\n    lowering_platforms = ('tpu', 'cpu', 'cuda')\n    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=lowering_platforms))\n    for tf_device in self.tf_devices:\n        with self.subTest(tf_device.device_type):\n            logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n            with tf.device(tf_device):\n                res = f_tf(x)\n            self.assertAllClose(res, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_simple(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    x = np.float32(0.7)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(x):\n        return dict(a=x['a'] + 1.0, b=x)\n    x = dict(a=0.7, b=0.8)\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax))\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_shape_poly(self):\n    f_jax = jnp.sin\n    f_jax_rt = jax2tf.call_tf(jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)']))\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    self.assertAllClose(f_jax(x), f_jax_rt(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_saved_model_simple(self):\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax)\n    restored_tf, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])\n    restored_jax = jax2tf.call_tf(restored_tf)\n    self.assertAllClose(f_jax(x), restored_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_saved_model_variables(self):\n    param = np.array([1.0, 2.0], dtype=np.float32)\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(param, x):\n        return jnp.sin(x) + jnp.cos(param)\n    param_v = tf.Variable(param)\n    f_tf = jax2tf.convert(f_jax)\n    _, restored_model = tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])\n    restored_jax = jax2tf.call_tf(restored_model.f)\n    self.assertAllClose(f_jax(param, x), restored_jax(x))\n    self.assertAllClose(f_jax(param, x), jax.jit(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), jax2tf.convert(restored_jax)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=False)(x))\n    self.assertAllClose(f_jax(param, x), tf.function(jax2tf.convert(restored_jax), autograph=True)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_saved_model_shape_poly(self):\n    tracing_count = 0\n    x = np.array([0.7, 0.8], dtype=np.float32)\n\n    def f_jax(x):\n        nonlocal tracing_count\n        tracing_count += 1\n        return jnp.sin(x)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])\n    res_jax = f_jax(x)\n    self.assertEqual(1, tracing_count)\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])\n    self.assertGreaterEqual(tracing_count, 2)\n    tracing_count = 0\n    f_jax_rt = jax2tf.call_tf(restored_f)\n    self.assertAllClose(res_jax, f_jax_rt(x))\n    y = np.concatenate([x, x])\n    self.assertEqual(0, tracing_count)\n    res_jax_y = f_jax(y)\n    self.assertEqual(1, tracing_count)\n    self.assertAllClose(res_jax_y, f_jax_rt(y))\n    self.assertEqual(1, tracing_count)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_without_gradient_saved_model(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=False), input_args=[x])\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaisesRegex(Exception, 'Gradient explicitly disabled.*jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'):\n        jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_saved_model_no_gradients(self):\n    f_jax = jnp.sum\n    x = np.array([0.7, 0.8], dtype=np.float32)\n    f_tf, _ = tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)\n    f_rt = jax2tf.call_tf(f_tf)\n    self.assertAllClose(f_jax(x), f_rt(x))\n    with self.assertRaises(TypeError):\n        _ = jax.grad(f_rt)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(poly=poly, version=version) for poly in [True, False] for version in [9]])\ndef test_call_tf_ordered_dead_inputs(self, *, poly: bool, version: int):\n    with config.jax_export_calling_convention_version(version):\n        logging.info('Using JAX serialization version %s', jax.config.jax_export_calling_convention_version)\n\n        def f_jax(x1, x_dead, x3):\n            return (x1, jax2tf.call_tf(lambda x: tf.math.sin(x), ordered=True, call_tf_graph=True)(x3))\n        if poly:\n            polymorphic_shapes = ['b', None, None]\n        else:\n            polymorphic_shapes = None\n        f_tf = jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)\n        x1 = np.arange(3, dtype=np.float32)\n        x_dead = np.arange(4, dtype=np.float32)\n        x3 = np.arange(5, dtype=np.float32)\n        self.assertAllClose(f_jax(x1, x_dead, x3), f_tf(x1, x_dead, x3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_arg(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_scalar_res(self, with_jit=True):\n    x = 3.0\n    res = _maybe_jit(with_jit, jax2tf.call_tf(lambda x: 4.0))(x)\n    self.assertAllClose(4.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_arg(self, with_jit=True):\n    x = np.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_numpy_res(self, with_jit=False):\n    x = np.ones((2, 3))\n    res = _maybe_jit(with_jit, jax2tf.call_tf(lambda _: x))(x)\n    self.assertAllClose(x, res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_devicearray_arg(self, with_jit=False):\n    x = jnp.ones((2, 3), dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(tf.math.sin))(x)\n    self.assertAllClose(jnp.sin(x), res)\n    x = jnp.array(3.0, dtype=jnp.bfloat16)\n    res = jax2tf.call_tf(lambda x: x)(x)\n    self.assertAllClose(x, res)\n    with self.assertRaises(AssertionError):\n        self.assertTrue(np.shares_memory(x, res))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_eval_pytree(self, with_jit=True):\n\n    def fun_tf(x: dict, y: tuple) -> tuple:\n        return (x['first'] * x['second'], y[0] + y[1])\n    x = dict(first=np.float32(3.0), second=np.float32(4.0))\n    y = (np.float64(5.0), np.float64(6.0))\n    fun_jax = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))\n    res = fun_jax(x, y)\n    self.assertAllClose((np.float32(12.0), np.float64(11.0)), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_control_flow(self, with_jit=True):\n\n    def times_5_tf(x):\n        c = lambda i, acc: tf.less(i, 5)\n        b = lambda i, acc: (tf.add(i, 1), tf.add(acc, x))\n        _, acc = tf.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])\n        return acc\n\n    def fun_jax(x):\n\n        def body(_, acc):\n            return jax2tf.call_tf(times_5_tf)(acc)\n        return lax.fori_loop(0, 3, body, x)\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(np.float32(x * 5 * 5 * 5), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{dtype.__name__}{('_jit' if with_jit else '')}', dtype=dtype, with_jit=with_jit) for dtype in set(jtu.dtypes.all) - {np.bool_} for with_jit in [True, False]))\ndef test_dtypes(self, dtype=np.int32, with_jit=True):\n\n    def fun_tf(x):\n        return tf.raw_ops.AddV2(x=x, y=tf.constant(3, dtype=dtype))\n\n    def fun_jax(x):\n        return jax2tf.call_tf(fun_tf)(x) + x\n    x = np.ones((3,), dtype=dtype)\n    res = _maybe_jit(with_jit, fun_jax)(x)\n    self.assertAllClose(dtype(2 * x + 3), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_bool(self, with_jit=False):\n\n    def fun_tf(x, y):\n        return tf.math.logical_and(x, y)\n    x = np.array([True, False, True, False], dtype=np.bool_)\n    y = np.array([True, True, False, False], dtype=np.bool_)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x, y)\n    self.assertAllClose(np.array([True, False, False, False], dtype=np.bool_), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_input(self, with_jit=True):\n\n    def f_tf(x):\n        return tf.math.sin(x)\n    x = 5.0\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = jnp.sin(x)\n    self.assertAllClose(res_call_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_x64_output(self, with_jit=True):\n\n    def f_tf(x):\n        return (tf.constant(3.0, tf.float64), x)\n    x = np.float32(5.0)\n    res_call_tf = _maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)\n    res_jax = (3.0, x)\n    self.assertAllClose(res_call_tf, res_jax)\n    res_call_tf_jit = jax.jit(jax2tf.call_tf(f_tf))(x)\n    self.assertAllClose(res_call_tf_jit, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_var_read(self, with_jit=True):\n    outer_var_array = np.array([3.0, 4.0], dtype=np.float32)\n    outer_var = tf.Variable(outer_var_array)\n\n    def fun_tf(x):\n        return x * outer_var + 1.0\n    x = np.array([2.0, 5.0], dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * outer_var_array + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_var_read_x64(self, with_jit=True):\n    outer_var_array = np.array([3.0, 4.0], dtype=np.float64)\n    outer_var = tf.Variable(outer_var_array)\n\n    def fun_tf(x):\n        return x * tf.cast(outer_var, x.dtype) + 1.0\n    x = np.array([2.0, 5.0], dtype=np.float32)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * outer_var_array + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_var_write_error(self, with_jit=True):\n    if with_jit:\n        raise unittest.SkipTest('variable writes not yet working')\n    outer_var = tf.Variable(3.0, dtype=np.float32)\n\n    def fun_tf(x):\n        outer_var.assign(tf.constant(4.0))\n        return x * outer_var + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 4.0 + 1, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_tensor_capture(self, with_jit=True):\n    outer_tensor = tf.constant(3.0, dtype=np.float32)\n\n    def fun_tf(x):\n        return x * outer_tensor + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 3.0 + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_tensor_capture_x64(self, with_jit=True):\n    outer_tensor = tf.constant(3.0, dtype=np.float64)\n\n    def fun_tf(x):\n        return x * tf.cast(outer_tensor * 3.14, tf.float32) + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 3.0 * 3.14 + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_value_capture(self, with_jit=True):\n    outer_val = np.array(3.0, dtype=np.float32)\n\n    def fun_tf(x):\n        return x * outer_val + 1.0\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose(x * 3.0 + 1.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_with_multiple_capture(self, with_jit=True):\n    if jtu.test_device_matches(['gpu']):\n        raise unittest.SkipTest('Test fails on GPU')\n    v2 = tf.Variable(2.0, dtype=np.float32)\n    v3 = tf.Variable(3.0, dtype=np.float32)\n    t4 = tf.constant(4.0, dtype=np.float32)\n    t5 = tf.constant(5.0, dtype=np.float32)\n\n    def fun_tf(x):\n        return (x * v3 + t4 + v2) * v3 + t5\n    x = np.float32(2.0)\n    res = _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n    self.assertAllClose((x * 3.0 + 4.0 + 2.0) * 3.0 + 5.0, res, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_grad(self, with_jit=False):\n    x = np.float32(3.0)\n    res = _maybe_jit(with_jit, jax.grad(jax2tf.call_tf(tf.math.sin)))(x)\n    self.assertAllClose(np.cos(x), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_grad_pytree(self, with_jit=False):\n\n    def fun_tf(x: dict, y: tuple) -> tuple:\n        return x['first'] * x['second'] + 3.0 * y[0] + 4.0 * y[1]\n    x = dict(first=np.float32(3.0), second=np.float32(4.0))\n    y = (np.float32(5.0), np.float32(6.0))\n    grad_x = _maybe_jit(with_jit, jax.grad(jax2tf.call_tf(fun_tf)))(x, y)\n    self.assertAllClose(dict(first=np.float32(4.0), second=np.float32(3.0)), grad_x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@_parameterized_jit\ndef test_grad_custom(self, with_jit=False):\n\n    @tf.custom_gradient\n    def func_square_tf(x):\n\n        def grad(dy, variables=None):\n            return (3.0 * x * dy,)\n        return (x * x, grad)\n    x = np.float32(4.0)\n    grad_x = _maybe_jit(with_jit, jax.grad(jax2tf.call_tf(func_square_tf)))(x)\n    self.assertAllClose(np.float32(3.0) * x, grad_x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_degree={degree!r}{('_jit' if with_jit else '')}', degree=degree, with_jit=with_jit) for degree in [1, 2, 3, 4] for with_jit in [True, False]))\ndef test_higher_order_grad(self, degree=2, with_jit=False):\n\n    def fun_tf(x):\n        return 2.0 * x * x * x\n\n    def fun_jax(x):\n        return 3.0 * _maybe_jit(with_jit, jax2tf.call_tf(fun_tf))(x)\n\n    def fun_jax_pure(x):\n        return 3.0 * fun_tf(x)\n    grad_jax = fun_jax\n    grad_jax_pure = fun_jax_pure\n    for _ in range(degree):\n        grad_jax = jax.grad(grad_jax)\n        grad_jax_pure = jax.grad(grad_jax_pure)\n    res_jax = grad_jax(np.float32(5.0))\n    logging.info('Grad of %s degree is %s', degree, res_jax)\n    self.assertAllClose(res_jax, grad_jax_pure(np.float32(5.0)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/call_tf_test.py",
    "function": "def _maybe_jit(with_jit: bool, func: Callable) -> Callable:\n    if with_jit:\n        return jax.jit(func)\n    else:\n        return func"
  }
]