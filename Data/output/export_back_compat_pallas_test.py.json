[
  {
    "test_code": "@jax.default_matmul_precision('bfloat16')\ndef test_mosaic_matmul(self):\n    if not jtu.if_cloud_tpu_at_least(2024, 9, 30):\n        self.skipTest('Requires libtpu built after 2024-09-30')\n    dtype = jnp.float32\n\n    def func():\n        x_shape = (1024, 512)\n        bias = 1.0\n        scale = 0.001\n        x = bias + scale * jnp.arange(math.prod(x_shape), dtype=dtype).reshape(x_shape)\n        y = x[:512, :256]\n        res = matmul.matmul(x, y, block_shape=(256, 256))\n        return res[::16, ::16]\n    data = self.load_testdata(mosaic_matmul.data_2024_09_24)\n    self.run_one_test(func, data, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/export_back_compat_pallas_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "@jax.default_matmul_precision('bfloat16')\ndef test_mosaic_matmul(self):\n    if not jtu.if_cloud_tpu_at_least(2024, 9, 30):\n        self.skipTest('Requires libtpu built after 2024-09-30')\n    dtype = jnp.float32\n\n    def func():\n        x_shape = (1024, 512)\n        bias = 1.0\n        scale = 0.001\n        x = bias + scale * jnp.arange(math.prod(x_shape), dtype=dtype).reshape(x_shape)\n        y = x[:512, :256]\n        res = matmul.matmul(x, y, block_shape=(256, 256))\n        return res[::16, ::16]\n    data = self.load_testdata(mosaic_matmul.data_2024_09_24)\n    self.run_one_test(func, data, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/export_back_compat_pallas_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "@jax.default_matmul_precision('bfloat16')\ndef test_mosaic_matmul(self):\n    if not jtu.if_cloud_tpu_at_least(2024, 9, 30):\n        self.skipTest('Requires libtpu built after 2024-09-30')\n    dtype = jnp.float32\n\n    def func():\n        x_shape = (1024, 512)\n        bias = 1.0\n        scale = 0.001\n        x = bias + scale * jnp.arange(math.prod(x_shape), dtype=dtype).reshape(x_shape)\n        y = x[:512, :256]\n        res = matmul.matmul(x, y, block_shape=(256, 256))\n        return res[::16, ::16]\n    data = self.load_testdata(mosaic_matmul.data_2024_09_24)\n    self.run_one_test(func, data, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/export_back_compat_pallas_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  }
]