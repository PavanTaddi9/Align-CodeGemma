[
  {
    "test_code": "def test_tf_call_tf_function(self):\n\n    def func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data = self.load_testdata(tf_call_tf_function.data_2023_07_29)\n    self.run_one_test(func, data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/back_compat_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_tf_call_tf_function(self):\n\n    def func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data = self.load_testdata(tf_call_tf_function.data_2023_07_29)\n    self.run_one_test(func, data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/back_compat_tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_tf_call_tf_function(self):\n\n    def func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data = self.load_testdata(tf_call_tf_function.data_2023_07_29)\n    self.run_one_test(func, data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/back_compat_tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_tf_call_tf_function(self):\n\n    def func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data = self.load_testdata(tf_call_tf_function.data_2023_07_29)\n    self.run_one_test(func, data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/back_compat_tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_tf_call_tf_function(self):\n\n    def func(x):\n\n        def func_tf(x):\n            return tf.math.sin(x)\n        return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))\n    data = self.load_testdata(tf_call_tf_function.data_2023_07_29)\n    self.run_one_test(func, data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/back_compat_tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  }
]