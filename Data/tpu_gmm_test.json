[
  {
    "test_code": "@parameterized.parameters(*GROUPED_MATMUL_TESTS)\n@hp.given(hps.data())\ndef test_gmm_sharded_groups(self, m: int, k: int, n: int, data: hps.SearchStrategy[hps.DataObject]):\n    seed = data.draw(seed_strategy())\n    num_groups, group_stride = data.draw(group_strategy())\n    lhs_dtype, rhs_dtype, out_dtype = [data.draw(hps.sampled_from([jnp.float32, jnp.bfloat16])) for _ in range(3)]\n    key = jax.random.key(seed)\n    k1, k2 = jax.random.split(key, 2)\n    lhs = random_dense((m, k), k1, lhs_dtype, limit=1)\n    rhs = random_dense((num_groups, k, n), k2, rhs_dtype, limit=1)\n    group_sizes = data.draw(group_sizes_strategy(m=m, num_groups=num_groups))\n    out, shard_vjpfun = jax.vjp(partial(mblx.gmm, preferred_element_type=out_dtype), lhs, rhs[0:group_stride], group_sizes)\n    vjpfuns = [shard_vjpfun]\n    for group_offset in range(group_stride, num_groups, group_stride):\n        out, shard_vjpfun = jax.vjp(lambda lhs, rhs, group_sizes, out: mblx.gmm(lhs, rhs, group_sizes, out_dtype, group_offset=jnp.array(group_offset, dtype=jnp.int32), existing_out=out), lhs, rhs[group_offset:group_offset + group_stride], group_sizes, out)\n        vjpfuns.append(shard_vjpfun)\n    expected_out, reference_vjpfun = jax.vjp(partial(reference_gmm, preferred_element_type=out_dtype), lhs, rhs, group_sizes)\n    self.assertEqual(out.dtype, out_dtype)\n    self.assertEqual(expected_out.dtype, out_dtype)\n    atol, rtol = tolerances(lhs_dtype, rhs_dtype, out_dtype)\n    self.assert_allclose(out, expected_out, atol=atol, rtol=rtol)\n    cotangent = random_dense((m, n), k1, out_dtype, limit=1)\n    shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[0](cotangent)\n    grad_lhs = shard_grad_lhs\n    grad_rhs = [shard_grad_rhs]\n    for i, group_offset in enumerate(range(group_stride, num_groups, group_stride)):\n        shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[i + 1](cotangent)\n        grad_lhs += shard_grad_lhs\n        grad_rhs.append(shard_grad_rhs)\n    grad_rhs = jnp.concatenate(grad_rhs, axis=0)\n    expected_grad_lhs, expected_grad_rhs, *_ = reference_vjpfun(cotangent)\n    self.assert_allclose(grad_lhs, expected_grad_lhs, atol=atol, rtol=rtol)\n    self.assert_allclose(grad_rhs, expected_grad_rhs, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_gmm_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(*GROUPED_MATMUL_TESTS)\n@hp.given(hps.data())\ndef test_gmm_sharded_groups(self, m: int, k: int, n: int, data: hps.SearchStrategy[hps.DataObject]):\n    seed = data.draw(seed_strategy())\n    num_groups, group_stride = data.draw(group_strategy())\n    lhs_dtype, rhs_dtype, out_dtype = [data.draw(hps.sampled_from([jnp.float32, jnp.bfloat16])) for _ in range(3)]\n    key = jax.random.key(seed)\n    k1, k2 = jax.random.split(key, 2)\n    lhs = random_dense((m, k), k1, lhs_dtype, limit=1)\n    rhs = random_dense((num_groups, k, n), k2, rhs_dtype, limit=1)\n    group_sizes = data.draw(group_sizes_strategy(m=m, num_groups=num_groups))\n    out, shard_vjpfun = jax.vjp(partial(mblx.gmm, preferred_element_type=out_dtype), lhs, rhs[0:group_stride], group_sizes)\n    vjpfuns = [shard_vjpfun]\n    for group_offset in range(group_stride, num_groups, group_stride):\n        out, shard_vjpfun = jax.vjp(lambda lhs, rhs, group_sizes, out: mblx.gmm(lhs, rhs, group_sizes, out_dtype, group_offset=jnp.array(group_offset, dtype=jnp.int32), existing_out=out), lhs, rhs[group_offset:group_offset + group_stride], group_sizes, out)\n        vjpfuns.append(shard_vjpfun)\n    expected_out, reference_vjpfun = jax.vjp(partial(reference_gmm, preferred_element_type=out_dtype), lhs, rhs, group_sizes)\n    self.assertEqual(out.dtype, out_dtype)\n    self.assertEqual(expected_out.dtype, out_dtype)\n    atol, rtol = tolerances(lhs_dtype, rhs_dtype, out_dtype)\n    self.assert_allclose(out, expected_out, atol=atol, rtol=rtol)\n    cotangent = random_dense((m, n), k1, out_dtype, limit=1)\n    shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[0](cotangent)\n    grad_lhs = shard_grad_lhs\n    grad_rhs = [shard_grad_rhs]\n    for i, group_offset in enumerate(range(group_stride, num_groups, group_stride)):\n        shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[i + 1](cotangent)\n        grad_lhs += shard_grad_lhs\n        grad_rhs.append(shard_grad_rhs)\n    grad_rhs = jnp.concatenate(grad_rhs, axis=0)\n    expected_grad_lhs, expected_grad_rhs, *_ = reference_vjpfun(cotangent)\n    self.assert_allclose(grad_lhs, expected_grad_lhs, atol=atol, rtol=rtol)\n    self.assert_allclose(grad_rhs, expected_grad_rhs, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_gmm_test.py",
    "function": "@hps.composite\ndef group_sizes_strategy(draw: hps.DrawFn, m: int, num_groups: int) -> jnp.ndarray:\n    ends_no_final = np.sort(np.array([draw(hps.integers(min_value=0, max_value=m)) for _ in range(num_groups - 1)], dtype=np.int32))\n    ends = np.concatenate([ends_no_final, np.array([m], dtype=np.int32)])\n    starts = np.concatenate([np.zeros(1, dtype=np.int32), ends_no_final])\n    return jnp.array(ends - starts, dtype=jnp.int32)"
  },
  {
    "test_code": "@parameterized.parameters(*GROUPED_MATMUL_TESTS)\n@hp.given(hps.data())\ndef test_gmm_sharded_groups(self, m: int, k: int, n: int, data: hps.SearchStrategy[hps.DataObject]):\n    seed = data.draw(seed_strategy())\n    num_groups, group_stride = data.draw(group_strategy())\n    lhs_dtype, rhs_dtype, out_dtype = [data.draw(hps.sampled_from([jnp.float32, jnp.bfloat16])) for _ in range(3)]\n    key = jax.random.key(seed)\n    k1, k2 = jax.random.split(key, 2)\n    lhs = random_dense((m, k), k1, lhs_dtype, limit=1)\n    rhs = random_dense((num_groups, k, n), k2, rhs_dtype, limit=1)\n    group_sizes = data.draw(group_sizes_strategy(m=m, num_groups=num_groups))\n    out, shard_vjpfun = jax.vjp(partial(mblx.gmm, preferred_element_type=out_dtype), lhs, rhs[0:group_stride], group_sizes)\n    vjpfuns = [shard_vjpfun]\n    for group_offset in range(group_stride, num_groups, group_stride):\n        out, shard_vjpfun = jax.vjp(lambda lhs, rhs, group_sizes, out: mblx.gmm(lhs, rhs, group_sizes, out_dtype, group_offset=jnp.array(group_offset, dtype=jnp.int32), existing_out=out), lhs, rhs[group_offset:group_offset + group_stride], group_sizes, out)\n        vjpfuns.append(shard_vjpfun)\n    expected_out, reference_vjpfun = jax.vjp(partial(reference_gmm, preferred_element_type=out_dtype), lhs, rhs, group_sizes)\n    self.assertEqual(out.dtype, out_dtype)\n    self.assertEqual(expected_out.dtype, out_dtype)\n    atol, rtol = tolerances(lhs_dtype, rhs_dtype, out_dtype)\n    self.assert_allclose(out, expected_out, atol=atol, rtol=rtol)\n    cotangent = random_dense((m, n), k1, out_dtype, limit=1)\n    shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[0](cotangent)\n    grad_lhs = shard_grad_lhs\n    grad_rhs = [shard_grad_rhs]\n    for i, group_offset in enumerate(range(group_stride, num_groups, group_stride)):\n        shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[i + 1](cotangent)\n        grad_lhs += shard_grad_lhs\n        grad_rhs.append(shard_grad_rhs)\n    grad_rhs = jnp.concatenate(grad_rhs, axis=0)\n    expected_grad_lhs, expected_grad_rhs, *_ = reference_vjpfun(cotangent)\n    self.assert_allclose(grad_lhs, expected_grad_lhs, atol=atol, rtol=rtol)\n    self.assert_allclose(grad_rhs, expected_grad_rhs, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_gmm_test.py",
    "function": "def random_dense(shape: tuple[int, ...], key: jax.Array, dtype: jnp.dtype, limit: int | None=None) -> jnp.ndarray:\n    if limit is None:\n        limit = 1 / np.prod(shape)\n    x = jax.random.uniform(key, shape, dtype, minval=-limit, maxval=limit)\n    return x.astype(jnp.bfloat16).astype(dtype)"
  },
  {
    "test_code": "@parameterized.parameters(*GROUPED_MATMUL_TESTS)\n@hp.given(hps.data())\ndef test_gmm_sharded_groups(self, m: int, k: int, n: int, data: hps.SearchStrategy[hps.DataObject]):\n    seed = data.draw(seed_strategy())\n    num_groups, group_stride = data.draw(group_strategy())\n    lhs_dtype, rhs_dtype, out_dtype = [data.draw(hps.sampled_from([jnp.float32, jnp.bfloat16])) for _ in range(3)]\n    key = jax.random.key(seed)\n    k1, k2 = jax.random.split(key, 2)\n    lhs = random_dense((m, k), k1, lhs_dtype, limit=1)\n    rhs = random_dense((num_groups, k, n), k2, rhs_dtype, limit=1)\n    group_sizes = data.draw(group_sizes_strategy(m=m, num_groups=num_groups))\n    out, shard_vjpfun = jax.vjp(partial(mblx.gmm, preferred_element_type=out_dtype), lhs, rhs[0:group_stride], group_sizes)\n    vjpfuns = [shard_vjpfun]\n    for group_offset in range(group_stride, num_groups, group_stride):\n        out, shard_vjpfun = jax.vjp(lambda lhs, rhs, group_sizes, out: mblx.gmm(lhs, rhs, group_sizes, out_dtype, group_offset=jnp.array(group_offset, dtype=jnp.int32), existing_out=out), lhs, rhs[group_offset:group_offset + group_stride], group_sizes, out)\n        vjpfuns.append(shard_vjpfun)\n    expected_out, reference_vjpfun = jax.vjp(partial(reference_gmm, preferred_element_type=out_dtype), lhs, rhs, group_sizes)\n    self.assertEqual(out.dtype, out_dtype)\n    self.assertEqual(expected_out.dtype, out_dtype)\n    atol, rtol = tolerances(lhs_dtype, rhs_dtype, out_dtype)\n    self.assert_allclose(out, expected_out, atol=atol, rtol=rtol)\n    cotangent = random_dense((m, n), k1, out_dtype, limit=1)\n    shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[0](cotangent)\n    grad_lhs = shard_grad_lhs\n    grad_rhs = [shard_grad_rhs]\n    for i, group_offset in enumerate(range(group_stride, num_groups, group_stride)):\n        shard_grad_lhs, shard_grad_rhs, *_ = vjpfuns[i + 1](cotangent)\n        grad_lhs += shard_grad_lhs\n        grad_rhs.append(shard_grad_rhs)\n    grad_rhs = jnp.concatenate(grad_rhs, axis=0)\n    expected_grad_lhs, expected_grad_rhs, *_ = reference_vjpfun(cotangent)\n    self.assert_allclose(grad_lhs, expected_grad_lhs, atol=atol, rtol=rtol)\n    self.assert_allclose(grad_rhs, expected_grad_rhs, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_gmm_test.py",
    "function": "def tolerances(lhs_dtype: jnp.dtype, rhs_dtype: jnp.dtype, out_dtype: jnp.dtype) -> tuple[float, float]:\n    if lhs_dtype == jnp.bfloat16 or rhs_dtype == jnp.bfloat16 or out_dtype == jnp.bfloat16:\n        return (0.001, 0.01)\n    return (0.001, 1e-05)"
  }
]