[
  {
    "test_code": "@jtu.sample_product(configs=[((1, 32), (2, 32), (([1], [1]), ([], [])), False), ((30, 64), (100, 64), (([1], [1]), ([], [])), False), ((192, 96), (160, 96), (([1], [1]), ([], [])), True), ((64, 128, 4), (128, 128), (([1], [0]), ([], [])), True), ((1, 128, 1024), (1, 1024, 128), (([2], [1]), ([0], [0])), True), ((1, 128, 128, 2), (128, 1, 2, 128), (([2], [0]), ([0, 3], [1, 2])), True)], output_type=[jnp.float16, jnp.bfloat16, jnp.float32])\n@jtu.run_on_devices('cuda')\ndef test_dot_general(self, configs, output_type):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape, b_shape, dimension_numbers, is_training = configs\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@jtu.sample_product(in_shardings=sharding_configs)\n@jtu.run_on_devices('cuda')\ndef test_dot_general_sharded(self, in_shardings):\n    if len(jax.local_devices()) < 4:\n        self.skipTest('Require at least 4 devices to run sharding tests.')\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    dimension_numbers = (([2], [2]), ([0], [0]))\n    a_shape = (2, 128, 512)\n    b_shape = (2, 256, 512)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general\n        y = fn(a, b, dimension_numbers)\n        return jnp.sum(jnp.tanh(y))\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        a, b, input_shardings = shard_and_device_put(mesh, in_shardings[0], in_shardings[1], a, b)\n        j_train = jax.jit(jax.value_and_grad(partial(fwd), argnums=[0, 1]), in_shardings=input_shardings)\n        hlo_text = j_train.lower(a, b).compile().as_text()\n        hlo_pattern = re.compile('.*'.join([re.escape(x) for x in ('custom-call', c_name)]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]), in_shardings=input_shardings)\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@jtu.sample_product(configs=[((1, 128, 256), (1, 128, 256), (0, 0, 0)), ((2, 128, 128), (2, 128, 128), (0, 0, 0)), ((2, 128, 128), (128, 2, 128), (0, 1, 2))])\n@jtu.run_on_devices('cuda')\ndef test_dot_general_vmap(self, configs):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape, b_shape, vmap_axes = configs\n    a_axis, b_axis, o_axis = vmap_axes\n    dimension_numbers = (([1], [1]), ([], []))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.vmap(jax.lax.dot_general if is_ref else scaled_dot_general, in_axes=(a_axis, b_axis, None), out_axes=o_axis)\n        y = fn(a, b, dimension_numbers)\n        return jnp.sum(y)\n    j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n    j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n    out, (x_grad, w_grad) = j_train(a, b)\n    out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n    self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=100.0)\n    self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n    self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@jtu.sample_product(configs=[((1, 32), (2, 32), (([1], [1]), ([], [])), False), ((30, 64), (100, 64), (([1], [1]), ([], [])), False), ((192, 96), (160, 96), (([1], [1]), ([], [])), True), ((64, 128, 4), (128, 128), (([1], [0]), ([], [])), True), ((1, 128, 1024), (1, 1024, 128), (([2], [1]), ([0], [0])), True), ((1, 128, 128, 2), (128, 1, 2, 128), (([2], [0]), ([0, 3], [1, 2])), True)], output_type=[jnp.float16, jnp.bfloat16, jnp.float32])\n@jtu.run_on_devices('cuda')\ndef test_dot_general(self, configs, output_type):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape, b_shape, dimension_numbers, is_training = configs\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@jtu.sample_product(in_shardings=sharding_configs)\n@jtu.run_on_devices('cuda')\ndef test_dot_general_sharded(self, in_shardings):\n    if len(jax.local_devices()) < 4:\n        self.skipTest('Require at least 4 devices to run sharding tests.')\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    dimension_numbers = (([2], [2]), ([0], [0]))\n    a_shape = (2, 128, 512)\n    b_shape = (2, 256, 512)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general\n        y = fn(a, b, dimension_numbers)\n        return jnp.sum(jnp.tanh(y))\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        a, b, input_shardings = shard_and_device_put(mesh, in_shardings[0], in_shardings[1], a, b)\n        j_train = jax.jit(jax.value_and_grad(partial(fwd), argnums=[0, 1]), in_shardings=input_shardings)\n        hlo_text = j_train.lower(a, b).compile().as_text()\n        hlo_pattern = re.compile('.*'.join([re.escape(x) for x in ('custom-call', c_name)]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]), in_shardings=input_shardings)\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@jtu.sample_product(configs=[((1, 128, 256), (1, 128, 256), (0, 0, 0)), ((2, 128, 128), (2, 128, 128), (0, 0, 0)), ((2, 128, 128), (128, 2, 128), (0, 1, 2))])\n@jtu.run_on_devices('cuda')\ndef test_dot_general_vmap(self, configs):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape, b_shape, vmap_axes = configs\n    a_axis, b_axis, o_axis = vmap_axes\n    dimension_numbers = (([1], [1]), ([], []))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.vmap(jax.lax.dot_general if is_ref else scaled_dot_general, in_axes=(a_axis, b_axis, None), out_axes=o_axis)\n        y = fn(a, b, dimension_numbers)\n        return jnp.sum(y)\n    j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n    j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n    out, (x_grad, w_grad) = j_train(a, b)\n    out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n    self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=100.0)\n    self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n    self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@jtu.sample_product(contract=[160, 96], lhs_non_contract=[240, 100], dtype=[jnp.float16, jnp.bfloat16, jnp.float32], block_scale_configs=[mxfp8_configs])\n@jtu.run_on_devices('cuda')\ndef test_scaled_matmul(self, contract, lhs_non_contract, dtype, block_scale_configs):\n    batch, rhs_non_contract = (2, 128)\n    a, b, a_q, b_q, a_scales, b_scales = generate_quantized_tensors(batch, lhs_non_contract, contract, rhs_non_contract, block_scale_configs, dtype=dtype)\n\n    def wrapper(lhs, rhs, lhs_scales, rhs_scales, out_type):\n        return scaled_matmul_wrapper(lhs, rhs, lhs_scales, rhs_scales, preferred_element_type=out_type)\n    j_scaled_matmul = jax.jit(partial(wrapper, out_type=dtype))\n    hlo_text = j_scaled_matmul.lower(a_q, b_q, a_scales, b_scales).compile().as_text()\n    hlo_pattern = re.compile('.*'.join([re.escape(x) for x in ('custom-call', c_name)]))\n    self.assertRegex(hlo_text, hlo_pattern)\n    out = j_scaled_matmul(a_q, b_q, a_scales, b_scales)\n    out_ref = np.einsum('BMK,BNK->BMN', a.astype(jnp.float32), b.astype(jnp.float32))\n    self.assertArraysAllClose(out, out_ref.astype(dtype), rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "def generate_quantized_tensors(batch, lhs_non_contract, contract, rhs_non_contract, configs, dtype=jnp.float32):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=dtype)\n    k1, k2 = jax.random.split(jax.random.key(123), 2)\n    a = cast_to_representable(jax.random.uniform(k1, (batch, lhs_non_contract, contract), minval=-1.0, dtype=dtype), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, (batch, rhs_non_contract, contract), minval=-1.0, dtype=dtype), configs[1].data_type)\n    dn = ((2,), (0,))\n    a_3d = shape_normalization(a, dn)\n    b_3d = shape_normalization(b, dn)\n    a_q, a_scales = quantize(a, configs[0])\n    b_q, b_scales = quantize(b, configs[1])\n    return (a, b, a_q, b_q, a_scales, b_scales)"
  },
  {
    "test_code": "@jtu.sample_product(in_shardings=sharding_configs, block_scale_configs=[mxfp8_configs])\n@jtu.run_on_devices('cuda')\ndef test_scaled_matmul_sharded(self, in_shardings, block_scale_configs):\n    if len(jax.local_devices()) < 4:\n        self.skipTest('Require at least 4 devices to run sharding tests.')\n    batch, contract, non_contract = (2, 1024, 256)\n    a, b, a_q, b_q, a_scales, b_scales = generate_quantized_tensors(batch, non_contract, contract, non_contract, block_scale_configs)\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    expected_output_spec = sharding_configs[in_shardings][1]\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        a_q, b_q, a_scales, b_scales, input_shardings = shard_and_device_put(mesh, in_shardings[0], in_shardings[1], a_q, b_q, a_scales, b_scales)\n        args = [a_q, b_q, a_scales, b_scales]\n        j_scaled_matmul = jax.jit(scaled_matmul_wrapper, in_shardings=input_shardings)\n        hlo_compiled = j_scaled_matmul.lower(*args).compile()\n        hlo_pattern = re.compile('.*'.join([re.escape(x) for x in ('custom-call', c_name)]))\n        self.assertRegex(hlo_compiled.as_text(), hlo_pattern)\n        j_ref = jax.jit(partial(jax.lax.dot_general, dimension_numbers=(([2], [2]), ([0], [0]))), in_shardings=input_shardings[:2])\n        out = j_scaled_matmul(*args)\n        out_ref = j_ref(a, b)\n        expected_output_sharding = NamedSharding(mesh=mesh, spec=expected_output_spec)\n        self.assertArraysAllClose(out, out_ref, rtol=0.001, atol=0.001)\n        self.assertTrue(out.sharding.is_equivalent_to(expected_output_sharding, out.ndim))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "def generate_quantized_tensors(batch, lhs_non_contract, contract, rhs_non_contract, configs, dtype=jnp.float32):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=dtype)\n    k1, k2 = jax.random.split(jax.random.key(123), 2)\n    a = cast_to_representable(jax.random.uniform(k1, (batch, lhs_non_contract, contract), minval=-1.0, dtype=dtype), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, (batch, rhs_non_contract, contract), minval=-1.0, dtype=dtype), configs[1].data_type)\n    dn = ((2,), (0,))\n    a_3d = shape_normalization(a, dn)\n    b_3d = shape_normalization(b, dn)\n    a_q, a_scales = quantize(a, configs[0])\n    b_q, b_scales = quantize(b, configs[1])\n    return (a, b, a_q, b_q, a_scales, b_scales)"
  },
  {
    "test_code": "@jtu.sample_product(in_shardings=sharding_configs, block_scale_configs=[mxfp8_configs])\n@jtu.run_on_devices('cuda')\ndef test_scaled_matmul_sharded(self, in_shardings, block_scale_configs):\n    if len(jax.local_devices()) < 4:\n        self.skipTest('Require at least 4 devices to run sharding tests.')\n    batch, contract, non_contract = (2, 1024, 256)\n    a, b, a_q, b_q, a_scales, b_scales = generate_quantized_tensors(batch, non_contract, contract, non_contract, block_scale_configs)\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    expected_output_spec = sharding_configs[in_shardings][1]\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        a_q, b_q, a_scales, b_scales, input_shardings = shard_and_device_put(mesh, in_shardings[0], in_shardings[1], a_q, b_q, a_scales, b_scales)\n        args = [a_q, b_q, a_scales, b_scales]\n        j_scaled_matmul = jax.jit(scaled_matmul_wrapper, in_shardings=input_shardings)\n        hlo_compiled = j_scaled_matmul.lower(*args).compile()\n        hlo_pattern = re.compile('.*'.join([re.escape(x) for x in ('custom-call', c_name)]))\n        self.assertRegex(hlo_compiled.as_text(), hlo_pattern)\n        j_ref = jax.jit(partial(jax.lax.dot_general, dimension_numbers=(([2], [2]), ([0], [0]))), in_shardings=input_shardings[:2])\n        out = j_scaled_matmul(*args)\n        out_ref = j_ref(a, b)\n        expected_output_sharding = NamedSharding(mesh=mesh, spec=expected_output_spec)\n        self.assertArraysAllClose(out, out_ref, rtol=0.001, atol=0.001)\n        self.assertTrue(out.sharding.is_equivalent_to(expected_output_sharding, out.ndim))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "def shard_and_device_put(mesh, a_sharding, b_sharding, a, b, a_scales=None, b_scales=None):\n    a_spec = PartitionSpec(*a_sharding)\n    b_spec = PartitionSpec(*b_sharding)\n    a_named_sharding = NamedSharding(mesh, a_spec)\n    b_named_sharding = NamedSharding(mesh, b_spec)\n    a = jax.device_put(a, a_named_sharding)\n    b = jax.device_put(b, b_named_sharding)\n    if a_scales is not None:\n        a_scales = jax.device_put(a_scales, a_named_sharding)\n    if b_scales is not None:\n        b_scales = jax.device_put(b_scales, b_named_sharding)\n    in_shardings = (a_named_sharding, b_named_sharding)\n    if a_scales is not None and b_scales is not None:\n        in_shardings = (a_named_sharding, b_named_sharding, a_named_sharding, b_named_sharding)\n        return (a, b, a_scales, b_scales, in_shardings)\n    return (a, b, in_shardings)"
  },
  {
    "test_code": "@jtu.sample_product(in_shardings=sharding_configs)\n@jtu.run_on_devices('cuda')\ndef test_dot_general_sharded(self, in_shardings):\n    if len(jax.local_devices()) < 4:\n        self.skipTest('Require at least 4 devices to run sharding tests.')\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    dimension_numbers = (([2], [2]), ([0], [0]))\n    a_shape = (2, 128, 512)\n    b_shape = (2, 256, 512)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0), self.block_scale_configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0), self.block_scale_configs[1].data_type)\n    scaled_dot_general = partial(scaled_dot_general_wrapper, configs=self.block_scale_configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general\n        y = fn(a, b, dimension_numbers)\n        return jnp.sum(jnp.tanh(y))\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        a, b, input_shardings = shard_and_device_put(mesh, in_shardings[0], in_shardings[1], a, b)\n        j_train = jax.jit(jax.value_and_grad(partial(fwd), argnums=[0, 1]), in_shardings=input_shardings)\n        hlo_text = j_train.lower(a, b).compile().as_text()\n        hlo_pattern = re.compile('.*'.join([re.escape(x) for x in ('custom-call', c_name)]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]), in_shardings=input_shardings)\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "def shard_and_device_put(mesh, a_sharding, b_sharding, a, b, a_scales=None, b_scales=None):\n    a_spec = PartitionSpec(*a_sharding)\n    b_spec = PartitionSpec(*b_sharding)\n    a_named_sharding = NamedSharding(mesh, a_spec)\n    b_named_sharding = NamedSharding(mesh, b_spec)\n    a = jax.device_put(a, a_named_sharding)\n    b = jax.device_put(b, b_named_sharding)\n    if a_scales is not None:\n        a_scales = jax.device_put(a_scales, a_named_sharding)\n    if b_scales is not None:\n        b_scales = jax.device_put(b_scales, b_named_sharding)\n    in_shardings = (a_named_sharding, b_named_sharding)\n    if a_scales is not None and b_scales is not None:\n        in_shardings = (a_named_sharding, b_named_sharding, a_named_sharding, b_named_sharding)\n        return (a, b, a_scales, b_scales, in_shardings)\n    return (a, b, in_shardings)"
  },
  {
    "test_code": "@jtu.sample_product(in_shardings=sharding_configs, block_scale_configs=[mxfp8_configs])\n@jtu.run_on_devices('cuda')\ndef test_collectives(self, in_shardings, block_scale_configs):\n    if jtu.device_under_test() != 'gpu' or len(jax.local_devices()) < 4:\n        self.skipTest('Partition Test enabled for at least 4 GPUs')\n    expected_hlo = sharding_configs[in_shardings][0]\n    hlo_text = get_hlo_text(in_shardings, block_scale_configs)\n    hlo_pattern = re.compile('.*'.join([re.escape(x) for x in expected_hlo]), flags=re.DOTALL)\n    self.assertRegex(hlo_text, hlo_pattern, msg=f'Failed to find pattern: {expected_hlo}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/scaled_matmul_stablehlo_test.py",
    "function": "def get_hlo_text(in_shardings, block_scale_configs):\n    mesh_names = ('dp', 'tp')\n    devices = np.array(jax.local_devices()[:4]).reshape((2, 2))\n    mesh = Mesh(devices, mesh_names)\n    _, _, a_q, b_q, a_scales, b_scales = generate_quantized_tensors(2, 512, 1024, 512, block_scale_configs)\n    with mesh:\n        a_q, b_q, a_scales, b_scales, in_shardings = shard_and_device_put(mesh, in_shardings[0], in_shardings[1], a_q, b_q, a_scales, b_scales)\n        pjit_fn = jax.jit(scaled_matmul_wrapper, in_shardings=in_shardings)\n        hlo = pjit_fn.lower(a_q, b_q, a_scales, b_scales).compile()\n    return hlo.as_text()"
  }
]