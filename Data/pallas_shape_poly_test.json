[
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def call(f, *args):\n    f = jax.custom_jvp(f)\n    f.defjvp(lambda primals, tangents: (f(*primals), sum(tangents)))\n    return f(*args)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def call(f, *args):\n    f = jax.custom_jvp(f)\n    f.defjvp(lambda primals, tangents: (f(*primals), sum(tangents)))\n    return f(*args)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "def test_copy(self):\n    block_shape = (8, 128)\n\n    def f(x, *, eager=False):\n\n        def copy_kernel(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (pl.cdiv(x.shape[0], block_shape[0]), (x.shape[1] + 1) // block_shape[1])\n        return pl.pallas_call(copy_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    exp = export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp_1 = exp.call(x1)\n        self.assertAllClose(res_exp_1, x1)\n        shape2 = block_shape\n        x2 = jnp.arange(math.prod(shape2), dtype=np.int32).reshape(shape2)\n        res_exp_2 = exp.call(x2)\n        self.assertAllClose(res_exp_2, x2)\n    with self.assertRaisesRegex(NotImplementedError, 'dynamic grid bounds not supported in the Triton backend'):\n        export.export(jax.jit(f), platforms=['cuda'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static_no_grid(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_block_sizes_must_be_static(self):\n\n    def f(x, *, eager=False):\n\n        def copy_one(x_ref, o_ref):\n            o_ref[...] = x_ref[...]\n        grid = (2, 2)\n        block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n        return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)\n    shape1 = (128, 256)\n    x1 = jnp.arange(math.prod(shape1), dtype=np.int32).reshape(shape1)\n    res = f(x1, eager=True)\n    self.assertAllClose(res, x1)\n    w, h = export.symbolic_shape('w, h')\n    with self.assertRaisesRegex(ValueError, 'shape polymorphism for Pallas does not support dynamically-shaped blocks'):\n        export.export(jax.jit(f), platforms=['tpu'])(jax.ShapeDtypeStruct((w, h), jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  },
  {
    "test_code": "@jtu.run_on_devices('tpu')\ndef test_matmul(self):\n    x_shape = (1024, 256)\n    y_shape = (256, 2048)\n    key = jax.random.key(42)\n    key1, key2 = jax.random.split(key, 2)\n    x = jax.random.normal(key1, x_shape, dtype=np.float32)\n    y = jax.random.normal(key2, y_shape, dtype=np.float32)\n    res = matmul(x, y)\n    self.assertAllClose(res, x @ y, atol=0.0001)\n    m, n, l = export.symbolic_shape('m, n, l', constraints=['mod(m, 128) == 0', 'mod(n, 128) == 0', 'mod(l, 128) == 0'])\n    exp = export.export(matmul, platforms=['tpu'])(jax.ShapeDtypeStruct((m, l), jnp.float32), jax.ShapeDtypeStruct((l, n), jnp.float32))\n    if jtu.test_device_matches(['tpu']):\n        res_exp = exp.call(x, y)\n        self.assertAllClose(res_exp, x @ y, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_shape_poly_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape=(128, 128, 128)):\n    m, l = x.shape\n    l2, n = y.shape\n    assert l2 == l\n    block_m, block_n, block_l = block_shape\n    assert l % block_l == 0, f'l={l!r}, block_l={block_l!r}'\n    assert m % block_m == 0, f'm={m!r}, block_m={block_m!r}'\n    assert n % block_n == 0, f'n={n!r}, block_n={block_n!r}'\n    grid = (m // block_m, n // block_n, l // block_l)\n    fused_matmul = pl.pallas_call(functools.partial(matmul_kernel), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((block_m, block_l), lambda i, j, k: (i, k)), pl.BlockSpec((block_l, block_n), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((block_m, block_n), lambda i, j, k: (i, j)), grid=grid, interpret=jtu.test_device_matches(['cpu']))\n    return fused_matmul(x, y)"
  }
]