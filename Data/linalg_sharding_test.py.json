[
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype, batch_size=8)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    vmap_fun = jax.vmap(fun)\n    vmap_fun_jit = jax.jit(vmap_fun)\n    actual = vmap_fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_non_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    self.assertIn('all-gather', fun_jit.lower(*args_sharded).compile().as_text())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype, batch_size=8)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    vmap_fun = jax.vmap(fun)\n    vmap_fun_jit = jax.jit(vmap_fun)\n    actual = vmap_fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_non_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    self.assertIn('all-gather', fun_jit.lower(*args_sharded).compile().as_text())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype, batch_size=8)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    vmap_fun = jax.vmap(fun)\n    vmap_fun_jit = jax.jit(vmap_fun)\n    actual = vmap_fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_non_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    self.assertIn('all-gather', fun_jit.lower(*args_sharded).compile().as_text())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype, batch_size=8)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    vmap_fun = jax.vmap(fun)\n    vmap_fun_jit = jax.jit(vmap_fun)\n    actual = vmap_fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "def fun(x, t):\n    return jnp.sum(jnp.power(jnp.maximum(x, 0.0), 2)) + t"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_non_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    self.assertIn('all-gather', fun_jit.lower(*args_sharded).compile().as_text())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "def fun(x, t):\n    return jnp.sum(jnp.power(jnp.maximum(x, 0.0), 2)) + t"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype, batch_size=8)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    vmap_fun = jax.vmap(fun)\n    vmap_fun_jit = jax.jit(vmap_fun)\n    actual = vmap_fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "@partial(jax.jit, backend=backend)\ndef fun(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.sample_product(fun_and_shapes=ALL_FUN_AND_SHAPES, dtype=float_types + complex_types)\n@jtu.run_on_devices('gpu', 'cpu')\ndef test_non_batch_axis_sharding(self, fun_and_shapes, dtype):\n    fun, shapes = self.get_fun_and_shapes(fun_and_shapes)\n    args = self.get_args(shapes, dtype)\n    mesh = jtu.create_mesh((2,), ('i',))\n    sharding = jax.NamedSharding(mesh, P('i'))\n    args_sharded = jax.device_put(args, sharding)\n    fun_jit = jax.jit(fun)\n    expected = fun(*args)\n    actual = fun_jit(*args_sharded)\n    self.assertAllClose(actual, expected)\n    self.assertIn('all-gather', fun_jit.lower(*args_sharded).compile().as_text())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_sharding_test.py",
    "function": "@partial(jax.jit, backend=backend)\ndef fun(x, y):\n    return jnp.matmul(x, y)"
  }
]