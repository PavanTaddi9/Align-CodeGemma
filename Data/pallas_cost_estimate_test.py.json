[
  {
    "test_code": "def test_attention(self):\n    qk_dim = 16\n    v_dim = 4\n    kv_len = 128\n    q_len = 64\n\n    def attention(q, k, v):\n        return jax.nn.softmax(q @ k.T, axis=-1) @ v\n    cost = cost_estimate.estimate_cost(attention, jnp.zeros((q_len, qk_dim), dtype=jnp.float32), jnp.zeros((kv_len, qk_dim), dtype=jnp.float32), jnp.zeros((kv_len, v_dim), dtype=jnp.float32))\n    qk_cost = 2 * q_len * kv_len * qk_dim\n    v_cost = 2 * q_len * kv_len * v_dim\n    softmax_flops = kv_len * q_len\n    self.assertEqual(cost.flops, qk_cost + v_cost + 2 * softmax_flops + q_len)\n    self.assertEqual(cost.transcendentals, softmax_flops)\n    input_bytes = q_len * qk_dim + kv_len * qk_dim + kv_len * v_dim\n    output_bytes = q_len * v_dim\n    self.assertEqual(cost.bytes_accessed, 4 * (input_bytes + output_bytes))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_cost_estimate_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_batched_matmul(self):\n\n    def matmul(a, b):\n        return jnp.matmul(a, b)\n    b, m, k, n = (7, 37, 91, 23)\n    cost = cost_estimate.estimate_cost(matmul, jax.ShapeDtypeStruct((b, m, k), jnp.float32), jax.ShapeDtypeStruct((b, k, n), jnp.float32))\n    self.assertEqual(cost.flops, 2 * b * m * k * n)\n    self.assertEqual(cost.transcendentals, 0)\n    self.assertEqual(cost.bytes_accessed, 4 * (b * m * k + b * n * k + b * m * n))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_cost_estimate_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "def test_batched_matmul(self):\n\n    def matmul(a, b):\n        return jnp.matmul(a, b)\n    b, m, k, n = (7, 37, 91, 23)\n    cost = cost_estimate.estimate_cost(matmul, jax.ShapeDtypeStruct((b, m, k), jnp.float32), jax.ShapeDtypeStruct((b, k, n), jnp.float32))\n    self.assertEqual(cost.flops, 2 * b * m * k * n)\n    self.assertEqual(cost.transcendentals, 0)\n    self.assertEqual(cost.bytes_accessed, 4 * (b * m * k + b * n * k + b * m * n))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_cost_estimate_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "def test_batched_matmul(self):\n\n    def matmul(a, b):\n        return jnp.matmul(a, b)\n    b, m, k, n = (7, 37, 91, 23)\n    cost = cost_estimate.estimate_cost(matmul, jax.ShapeDtypeStruct((b, m, k), jnp.float32), jax.ShapeDtypeStruct((b, k, n), jnp.float32))\n    self.assertEqual(cost.flops, 2 * b * m * k * n)\n    self.assertEqual(cost.transcendentals, 0)\n    self.assertEqual(cost.bytes_accessed, 4 * (b * m * k + b * n * k + b * m * n))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_cost_estimate_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  }
]